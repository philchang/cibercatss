{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Clustering\n",
        "\n",
        "So fa, we have primarily focused on supervised machine learning, where we have access to both the features and the corresponding target values. However, in real-world scenarios, it is common to encounter situations where we only have information about the features and lack the target labels.\n",
        "\n",
        "For instance, consider a scenario where we have sales records from a grocery store and we want to analyze the sales based on whether the customers are members of a discount club or not. Since we don't have the target labels to train and evaluate our models in a supervised learning fashion, we need an alternative approach. This is where unsupervised learning comes into play.\n",
        "\n",
        "Unsupervised learning allows us to explore the underlying patterns and groupings in the data without relying on predefined target values. By leveraging clustering algorithms, we can identify latent clusters or groups of observations that exhibit similar behavior or characteristics. This enables us to predict the class or category of observations, even in the absence of a target vector.\n",
        "\n",
        "Clustering algorithms come in various forms and employ diverse methodologies to identify these clusters in the data. In this chapter, we will delve into a selection of clustering algorithms using scikit-learn and explore their practical applications."
      ],
      "metadata": {
        "id": "OU00VXRhjGZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Clustering Using K-Means\n",
        "\n",
        "###Problem\n",
        "\n",
        "You want to group observations into k groups.\n",
        "\n",
        "Solution\n",
        "\n",
        "# Use k-means clustering:"
      ],
      "metadata": {
        "id": "bJNdgxlYkBCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "9ItOrMT3kZPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load iris dataset\n",
        "#TODO\n",
        "\n",
        "#cfreate feature set\n",
        "#TODO\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "features_std = scaler.fit_transform(features)\n",
        "\n",
        "# Create k-mean object\n",
        "cluster = KMeans(n_clusters=3, random_state=0, n_init='auto')\n",
        "\n",
        "# Train model\n",
        "model = cluster.fit(features_std)"
      ],
      "metadata": {
        "id": "zdwad5lvkJSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion**\n",
        "\n",
        "k-means clustering is one of the most common clustering techniques. In k-means clustering, the algorithm attempts to group observations into k groups, with each group having roughly equal variance. The number of groups, k, is specified by the user as a hyperparameter. Specifically, in k-means:\n",
        "\n",
        "1. k cluster “center” points are created at random locations.\n",
        "\n",
        "2. For each observation:\n",
        "\n",
        "  a. The distance between each observation and the k center points is calculated.\n",
        "\n",
        "  b. The observation is assigned to the cluster of the nearest center point.\n",
        "\n",
        "3. The center points are moved to the means (i.e., centers) of their respective clusters.\n",
        "\n",
        "4. Steps 2 and 3 are repeated until no observation changes in cluster membership.\n",
        "\n",
        "At this point the algorithm is considered converged and stops.\n",
        "It is important to note three things about k-means. First, k-means clustering assumes the clusters are convex shaped (e.g., a circle, a sphere). Second, all features are equally scaled. In our solution, we standardized the features to meet this assumption. Third, the groups are balanced (i.e., have roughly the same number of observations). If we suspect that we cannot meet these assumptions, we might try other clustering approaches.\n",
        "\n",
        "In scikit-learn, k-means clustering is implemented in the KMeans class. The most important parameter is n_clusters, which sets the number of clusters k. In some situations, the nature of the data will determine the value for k (e.g., data on a school’s students will have one cluster per grade), but often we don’t know the number of clusters. In these cases, we will want to select k based on using some criteria."
      ],
      "metadata": {
        "id": "Y5KQZ3K5lh62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our solution, we cheated a little and used the Iris flower data, in which we know there are three classes. Therefore, we set k = 3. We can use labels_ to see the predicted classes of each observation:"
      ],
      "metadata": {
        "id": "AtOB-0BfmBfg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VC6qfBvgjCbE"
      },
      "outputs": [],
      "source": [
        "# View predict class\n",
        "model.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we compare this to the observation’s true class we can see that despite the difference in class labels (i.e., 1, 2, and 3), k-means did reasonably well:"
      ],
      "metadata": {
        "id": "XxpAVDaQmrgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View true class\n",
        "iris.target"
      ],
      "metadata": {
        "id": "wAqU-ftemswB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, as you might imagine, the performance of k-means drops considerably, even critically, if we select the wrong number of clusters.\n",
        "\n",
        "Finally, as with other scikit-learns we can use the trained cluster to predict the value of new observations:"
      ],
      "metadata": {
        "id": "oiwYuBzVm3E2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new observation\n",
        "new_observation = [[0.8, 0.8, 0.8, 0.8]]\n",
        "\n",
        "# Predict observation's cluster\n",
        "model.predict(new_observation)"
      ],
      "metadata": {
        "id": "Q7SBqetHm7Ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See Also\n",
        "\n",
        "Introduction to K-means Clustering (https://www.datascience.com/blog/k-means-clustering)"
      ],
      "metadata": {
        "id": "R3kH3qrinJ-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Speeding Up K-Means Clustering\n",
        "\n",
        "Problem\n",
        "\n",
        "You want to group observations into k groups, but k-means takes too long.\n",
        "\n",
        "Solution\n",
        "\n",
        "Use mini-batch k-means:"
      ],
      "metadata": {
        "id": "O8YvEex2nLXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "# Load data\n",
        "#TODO\n",
        "\n",
        "# Standardize features\n",
        "#TODO\n",
        "\n",
        "# Create k-mean object\n",
        "cluster = MiniBatchKMeans(n_clusters=3, random_state=0, batch_size=100)\n",
        "\n",
        "# Train model\n",
        "#TODO\n",
        "\n",
        "# View predict class\n",
        "#TODO"
      ],
      "metadata": {
        "id": "jRlVy_MOnqN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discussion**\n",
        "\n",
        "The mini-batch k-means algorithm operates in a similar manner to the standard k-means algorithm. However, there is one key distinction. In mini-batch k-means, the most computationally intensive step is performed on a random subset of observations instead of the entire dataset. This approach offers a notable advantage by reducing the time required for the algorithm to converge and fit the data, albeit with a slight trade-off in quality.\n",
        "\n",
        "The MiniBatchKMeans algorithm follows a similar structure to KMeans, but with a notable parameter called batch_size. The batch_size parameter determines the number of randomly selected observations included in each batch. It's important to note that a larger batch size increases the computational cost of the training process. Therefore, there is a trade-off between the batch size and the efficiency of the algorithm."
      ],
      "metadata": {
        "id": "hcxwqvpxodup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Clustering Using Meanshift\n",
        "\n"
      ],
      "metadata": {
        "id": "6cp2hWXIouH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem\n",
        "\n",
        "You want to group observations without assuming the number of clusters or their shape.\n",
        "\n",
        "Solution\n",
        "\n",
        "Use meanshift clustering."
      ],
      "metadata": {
        "id": "9bD_6o6BoxKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "\n",
        "# Standardize features\n",
        "#TODO\n",
        "\n",
        "# Create meanshift object\n",
        "cluster = MeanShift(n_jobs=-1)\n",
        "\n",
        "# Train model\n",
        "#TODO\n",
        "\n",
        "# View predict class\n",
        "#TODO"
      ],
      "metadata": {
        "id": "oQHl3r6lo4Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One drawback of the k-means clustering method we previously discussed is the need to specify the number of clusters, k, beforehand, as well as the assumption about the shape of the clusters. However, there is a clustering algorithm called meanshift that overcomes these limitations.\n",
        "\n",
        "Meanshift operates on a simple principle but can be challenging to explain concisely. An analogy might be the most effective way to understand it. Let's imagine a football field covered in dense fog, representing a two-dimensional feature space. On this field, we have 100 individuals standing (representing our observations). Due to the fog, each person's visibility is limited to a short distance. Every minute, each person looks around and takes a step in the direction where they see the highest number of people. Over time, people start to gather and form groups as they continuously move towards larger crowds. Eventually, distinct clusters of people emerge across the field, and individuals are assigned to the clusters they end up in."
      ],
      "metadata": {
        "id": "VF_GN-xOozan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Clustering Using DBSCAN\n",
        "\n",
        "Problem\n",
        "\n",
        "You want to group observations into clusters of high density.\n",
        "\n",
        "Solution\n",
        "\n",
        "Use DBSCAN clustering (Density-based spatial clustering of applications with noise).\n",
        "\n",
        "DBSCAN is motivated by the idea that clusters will be areas where many observations are densely packed together and makes no assumptions of cluster shape. Unlike traditional clustering algorithms, DBSCAN does not require the specification of the number of clusters beforehand. Instead, it identifies dense regions in the data space and groups the data points based on their proximity."
      ],
      "metadata": {
        "id": "4YSfDCZEqA5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "\n",
        "# Standardize features\n",
        "#TODO\n",
        "\n",
        "# Create meanshift object\n",
        "cluster = DBSCAN(n_jobs=-1)\n",
        "\n",
        "# Train model\n",
        "#TODO\n",
        "\n",
        "# View predict class\n",
        "#TODO"
      ],
      "metadata": {
        "id": "Wq4t7nJ1qXrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Using Hierarchical Merging\n",
        "\n",
        "Problem\n",
        "\n",
        "You want to group observations using a hierarchy of clusters.\n",
        "\n",
        "Solution\n",
        "\n",
        "Use agglomerative clustering.\n",
        "\n",
        "Agglomerative clustering is a powerful, flexible hierarchical clustering algorithm. In agglomerative clustering, all observations start as their own clusters. Next, clusters meeting some criteria are merged together. This process is repeated, growing clusters until some end point is reached.\n",
        "\n"
      ],
      "metadata": {
        "id": "qM85dJ-SrVIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "features = iris.data\n",
        "\n",
        "# Standardize features\n",
        "#TODO\n",
        "\n",
        "# Create meanshift object\n",
        "cluster = AgglomerativeClustering(n_clusters=3)\n",
        "\n",
        "# Train model\n",
        "#TODO\n",
        "\n",
        "# View predict class\n",
        "#TODO"
      ],
      "metadata": {
        "id": "qfw-S12nrZVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
