{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbvW1aRMyHXZ"
   },
   "source": [
    "## **Handling Images**\n",
    "Image classification is a fascinating field in machine learning that enables computers to identify patterns and objects in images. Before applying machine learning algorithms to images, we typically need to preprocess the raw images and convert them into usable features. The widely used Open Source Computer Vision Library (OpenCV) is a powerful tool for handling images, offering extensive documentation and popularity among developers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTQDcUD4ym_m"
   },
   "source": [
    "The book will use a set of images as examples, which are available to download on [GitHub](https://github.com/chrisalbon/simulated_datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHDxJPqTzWF-"
   },
   "source": [
    "\n",
    "\n",
    "## Loading Images\n",
    "Let's load an image for preprocessing. We use use OpenCV’s `imread`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2mppcWPhy3B7"
   },
   "outputs": [],
   "source": [
    "# Load cv2 library\n",
    "# TODO\n",
    "\n",
    "#load numpy\n",
    "# TODO\n",
    "\n",
    "#import pyplot from matplotlib\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UkI8A6GCWgW"
   },
   "outputs": [],
   "source": [
    "# Load image as grayscale\n",
    "image = cv2.imread('image file pathway', cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-iVNZ1BCbYa"
   },
   "source": [
    "If we want to view the image, we can use the Python plotting library Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbABrS6rCcR-"
   },
   "outputs": [],
   "source": [
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxRewrzdDyhl"
   },
   "source": [
    "Fundamentally, images are data and when we use imread we convert that data into a data type we are very familiar with—a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6A78H7_kD0jo"
   },
   "outputs": [],
   "source": [
    "# Show data type\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5z5NjBtFK-F"
   },
   "source": [
    "We have transformed the image into a matrix whose elements correspond to individ‐ ual pixels. We can even take a look at the actual values of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3_t5HQqFMxo"
   },
   "outputs": [],
   "source": [
    "# Show image data\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-62oA_RjFWi4"
   },
   "source": [
    "The resolution of our image was 3600 × 2270, the exact dimensions of our matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJWupMX_FYmQ"
   },
   "outputs": [],
   "source": [
    "# Show dimensions\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMAO8AhZGj3J"
   },
   "source": [
    "What does each element in the matrix actually represent? In grayscale images, the value of an individual element is the pixel intensity. Intensity values range from black (0) to white (255). For example, the intensity of the top-rightmost pixel in our image has a value of 140:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRxKUFq9GjZC"
   },
   "outputs": [],
   "source": [
    "# Show first pixel\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sfRepGbHZTa"
   },
   "source": [
    "In the matrix, each element contains three values corresponding to blue, green, red values (BGR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miozbum2HaKA"
   },
   "outputs": [],
   "source": [
    "# Load image in color\n",
    "# TODO\n",
    "\n",
    "# Show first pixel\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLtd_t77HlWl"
   },
   "source": [
    "One small caveat: by default OpenCV uses BGR, but many image applications— including Matplotlib—use red, green, blue (RGB), meaning the red and the blue values are swapped. To properly display OpenCV color images in Matplotlib, we need to first convert the color to [RGB](http://bit.ly/2FxZjKZ) (apologies to hardcopy readers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRncCE6hHrif"
   },
   "outputs": [],
   "source": [
    "# TODO Convert to RGB\n",
    "\n",
    "\n",
    "#TODO Show image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8PWuccLIDPe"
   },
   "source": [
    "## Saving Images\n",
    "In this part, your task is to save an image for preprocessing. You can use OpenCV’s imwrite.\n",
    "\n",
    "Using OpenCV's `imwrite` function allows us to save images to a specified file path. The image format is determined by the extension of the filename (e.g., .*jpg*, .*png*). It is important to note that `imwrite` will overwrite existing files without displaying an error message or requesting confirmation, so caution should be exercised when using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBi7x08nIZWf"
   },
   "outputs": [],
   "source": [
    "# Load image as grayscale\n",
    "# TODO\n",
    "\n",
    "# Save image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hD-aBMsgI35H"
   },
   "source": [
    "## Resizing Images\n",
    "Resizing images is a common preprocessing task in machine learning to ensure consistent dimensions and reduce memory usage. However, it can result in information loss as the image matrix is reduced in size. Common image sizes for machine learning include 32x32, 64x64, 96x96, and 256x256.\n",
    "\n",
    "In this task we want to resize an image for further preprocessing, we can use `resize` to change the size of an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4tMqyWoKFrT"
   },
   "outputs": [],
   "source": [
    "# Resize image to 50 pixels by 50 pixels\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHhXvSNLKL8q"
   },
   "outputs": [],
   "source": [
    "# View image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ezpnoHXKxc6"
   },
   "source": [
    "## Cropping Images\n",
    "\n",
    "In OpenCV, image cropping is performed by selecting specific rows and columns from the image matrix. This allows us to keep only the desired portion of the image. Cropping is beneficial when we want to focus on a specific area of interest in every image, such as in the case of stationary security camera footage.\n",
    "\n",
    "For this task, we want to remove the outer portion of the image to change its dimensions.\n",
    "\n",
    "Solution: The image is encoded as a two-dimensional NumPy array, so we can crop the image easily by slicing the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Drw58oa1MMCx"
   },
   "outputs": [],
   "source": [
    "# Select first half of the columns and all rows [:,:128]\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evVVDJykMS2-"
   },
   "outputs": [],
   "source": [
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymVjE632Mp1D"
   },
   "source": [
    "## Blurring Images\n",
    "In this section your task is to smooth out an image.\n",
    "\n",
    "To achieve image blurring, the average value of neighboring pixels is calculated and assigned to each pixel. This process involves using a mathematical representation called a kernel, which defines the neighboring pixels and the specific operation performed. The size of the kernel determines the extent of blurring, with larger kernels resulting in smoother images. In this example, we apply blurring by averaging the values within a 5 × 5 kernel surrounding each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV1qnk7MNINc"
   },
   "outputs": [],
   "source": [
    "# Blur image 5 × 5 kernel surrounding each pixel\n",
    "# TODO\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99kgVDXSObfR"
   },
   "source": [
    "To highlight the effect of kernel size, here is the same blurring with a 100 × 100 kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e4Mnvy8OcnB"
   },
   "outputs": [],
   "source": [
    "# Blur image 100 × 100 kernel\n",
    "# TODO\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcRejcovOmsJ"
   },
   "source": [
    "## Sharpening Images\n",
    "Sharpening operates in a similar manner to blurring, but with the intention of enhancing image details instead of reducing them. Instead of using a kernel to average neighboring values, a specialized kernel is created to emphasize the central pixel. This process enhances the contrast of edges, making them more pronounced in the image.\n",
    "\n",
    "To perform the task of sharpening an image, create a kernel that highlights the target pixel. Then apply it to the image using fil ter2D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmJqkZ2lPELw"
   },
   "outputs": [],
   "source": [
    "# Load image as grayscale\n",
    "# TODO\n",
    "\n",
    "# Create kernel\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF210jsOPMLo"
   },
   "outputs": [],
   "source": [
    "# sharpen image\n",
    "# TODO\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toyl7bUYPXIU"
   },
   "source": [
    "## Enhancing Contrast\n",
    "\n",
    "Your task is to increase the contrast between pixels in an image.\n",
    "\n",
    "Histogram equalization is an image processing technique that enhances the visibility of objects and shapes. When working with grayscale images, we can directly apply OpenCV's \"equalizeHist\" function to the image to achieve this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIJ3eIHJPxck"
   },
   "outputs": [],
   "source": [
    "# Enhance image\n",
    "# TODO\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3pOY3KOQC59"
   },
   "source": [
    "In the case of a color image, it is necessary to convert the image to the YUV color format. The Y component represents the brightness or luma, while the U and V components represent the color information. Once the conversion is done, we can apply the equalizeHist function to the image and then convert it back to the BGR or RGB color format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RekLZTuQEyC"
   },
   "outputs": [],
   "source": [
    "# Load image\n",
    "# TODO\n",
    "\n",
    "# Convert to YUV\n",
    "image_yuv = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "# Apply histogram equalization\n",
    "image_yuv[:, :, 0] = cv2.equalizeHist(image_yuv[:, :, 0])\n",
    "\n",
    "# Convert to RGB\n",
    "image_rgb = cv2.cvtColor(image_yuv, cv2.COLOR_YUV2RGB)\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_cVABXlQQNm"
   },
   "source": [
    "## Isolating Colors\n",
    "\n",
    "To isolate a color in an image, define a range of colors and then apply a mask to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrsXMx6zQdga"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load image\n",
    "# TODO\n",
    "\n",
    "# Convert BGR to HSV\n",
    "# TODO\n",
    "\n",
    "# Define range of blue values in HSV\n",
    "lower_blue = np.array([50,100,50])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "# Create mask\n",
    "mask = cv2.inRange(image_hsv, lower_blue, upper_blue)\n",
    "\n",
    "# Mask image\n",
    "image_bgr_masked = cv2.bitwise_and(image_bgr, image_bgr, mask=mask)\n",
    "\n",
    "# Convert BGR to RGB\n",
    "# TODO\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OI_9d0WuQzuv"
   },
   "outputs": [],
   "source": [
    "# we create a mask for the image (we will only keep the white areas):\n",
    "# Show image\n",
    "plt.imshow(mask, cmap='gray'), plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YCsdAVMRFBG"
   },
   "source": [
    "## Binarizing Images\n",
    "\n",
    "\n",
    "Binarizing images in machine learning refers to the process of converting grayscale or color images into binary images, where each pixel is assigned either a black or white value. This is typically done by applying a thresholding technique.\n",
    "\n",
    "Thresholding is a technique used to convert pixel intensities above a certain value to white and those below the value to black. Adaptive thresholding is a more advanced method where the threshold value for a pixel is determined based on the intensities of its neighboring pixels. This approach is particularly useful in situations where lighting conditions vary across different areas of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMKGLHIZReJ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load image as grayscale\n",
    "# TODO\n",
    "\n",
    "# Apply adaptive thresholding\n",
    "max_output_value = 255\n",
    "neighborhood_size = 99\n",
    "subtract_from_mean = 10\n",
    "image_binarized = cv2.adaptiveThreshold(image_grey,\n",
    "                                            max_output_value,\n",
    "                                            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                            cv2.THRESH_BINARY,\n",
    "\n",
    "                                            neighborhood_size,\n",
    "                                            subtract_from_mean)\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDWyYZNpR0Lk"
   },
   "source": [
    "## Removing Backgrounds\n",
    "\n",
    "In this part of the image handling problem your task is to isolate the foreground of an image.\n",
    "To perfom this,  you need to mark a rectangle around the desired foreground, then run the GrabCut algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk1Zsa-xSOsi"
   },
   "outputs": [],
   "source": [
    "# Load image and convert to RGB\n",
    "# TODO\n",
    "# TODO\n",
    "\n",
    "# Rectangle values: start x, start y, width, height\n",
    "# TODO\n",
    "\n",
    "# Create initial mask\n",
    "mask = np.zeros(image_rgb.shape[:2], np.uint8)\n",
    "\n",
    "# Create temporary arrays used by grabCut\n",
    "bgdModel = np.zeros((1, 65), np.float64)\n",
    "fgdModel = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Run grabCut\n",
    "cv2.grabCut(image_rgb,              # Our image\n",
    "            mask,                   # The Mask\n",
    "            rectangle,              # Our rectangle\n",
    "            bgdModel,               # Temporary array for background\n",
    "            fgdModel,               # Temporary array for background\n",
    "            5,                      # Number of iterations\n",
    "            cv2.GC_INIT_WITH_RECT)  # Initiative using our rectangle\n",
    "\n",
    "# Create mask where sure and likely backgrounds set to 0, otherwise 1\n",
    "mask_2 = np.where((mask==2) | (mask==0), 0, 1).astype('uint8')\n",
    "\n",
    "# Multiply image with new mask to subtract background\n",
    "image_rgb_nobg = image_rgb * mask_2[:, :, np.newaxis]\n",
    "\n",
    "# Show image\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSLYfZ_IS4Jh"
   },
   "source": [
    "## Detecting Edges\n",
    "\n",
    "Edge detection is a prominent area of focus in computer vision, aiming to identify boundaries between objects in an image. These boundaries, or edges, carry significant information as they represent areas of high contrast or changes in intensity. By detecting edges, we can distinguish important features from less informative regions, such as homogeneous backgrounds. Various techniques exist for edge detection, including Sobel filters and the Laplacian edge detector, each offering different approaches to highlight these informative boundaries.\n",
    "\n",
    "In this part of the problem, we use an edge detection technique like the Canny edge detector to find the edges in an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D86kprx3TEMt"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load image as grayscale\n",
    "# TODO\n",
    "\n",
    "# Calculate median intensity\n",
    "# TODO\n",
    "\n",
    "# Set thresholds to be one standard deviation above and below median intensity\n",
    "# TODO\n",
    "# TODO\n",
    "\n",
    "# Apply canny edge detector\n",
    "# TODO\n",
    "\n",
    "# Show image\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYTop5SRTzeh"
   },
   "source": [
    "## Detecting Corners\n",
    "\n",
    "To detect the corners of an image, we use OpenCV’s implementation of the Harris corner detector, cornerHarris.\n",
    "\n",
    "The Harris corner detector is a widely used technique for identifying corner points, which are regions of high information in an image. It operates by examining windows or patches of pixels and detecting significant changes in their content when the window is slightly moved. The cornerHarris function, used for this purpose, has three important parameters: `block_size` determines the size of the neighboring region for corner detection, `aperture` refers to the size of the Sobel kernel used, and a free parameter controls the sensitivity for detecting corners, with larger values indicating softer corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d78L5X8PUNMs"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load image as grayscale\n",
    "# TODO\n",
    "# TODO\n",
    "# TODO\n",
    "\n",
    "# Set corner detector parameters\n",
    "# TODO\n",
    "# TODO\n",
    "# TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qx58s1Y6lcmU"
   },
   "outputs": [],
   "source": [
    "# Detect corners\n",
    "detector_responses = cv2.cornerHarris(image_gray,\n",
    "                                          block_size,\n",
    "                                          aperture,\n",
    "                                          free_parameter)\n",
    "# Large corner markers\n",
    "detector_responses = cv2.dilate(detector_responses, None)\n",
    "\n",
    "# Only keep detector responses greater than threshold, mark as white\n",
    "threshold = 0.002\n",
    "image_bgr[detector_responses >\n",
    "              threshold *\n",
    "              detector_responses.max()] = [255,255,255]\n",
    "\n",
    "# Convert to grayscale\n",
    "#TODO\n",
    "\n",
    "# Show image\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGTtDYiiU8rR"
   },
   "source": [
    "## Creating Features for Machine Learning\n",
    "\n",
    "You want to convert an image into an observation for machine learning. To do so, use NumPy’s flatten to convert the multidimensional array containing an image’s data into a vector containing the observation’s values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnC-dQa3VVLl"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load image as grayscale\n",
    "# TODO\n",
    "\n",
    "# Resize image to 10 pixels by 10 pixels\n",
    "# TODO\n",
    "\n",
    "# Convert image data to one-dimensional vector\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NciC_IFCVdfa"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "Images are presented as a grid of pixels. If an image is in grayscale, each pixel is pre‐ sented by one value (i.e., pixel intensity: 1 if white, 0 if black). For example, imagine we have a 10 × 10–pixel image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gU--3P1NVepR"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pll61hOgVnex"
   },
   "source": [
    "In this case the dimensions of the images data will be 10 × 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyh7iaDHVoO0"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5No9OE_VqHW"
   },
   "source": [
    "And if we flatten the array, we get a vector of length 100 (10 multiplied by 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcpe5YAwVt4j"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht34z4RcVz3P"
   },
   "source": [
    "This is the feature data for our image that can be joined with the vectors from other images to create the data we will feed to our machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwE_bsThV-Wt"
   },
   "source": [
    "If the image is in color, instead of each pixel being represented by one value, it is rep‐ resented by multiple values (most often three) representing the channels (red, green, blue, etc.) that blend to make the final color of that pixel. For this reason, if our 10 × 10 image is in color, we will have 300 feature values for each observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oENmYS6hWB1P"
   },
   "outputs": [],
   "source": [
    "# Load image in color\n",
    "#TODO\n",
    "\n",
    "# Resize image to 10 pixels by 10 pixels\n",
    "#TODO\n",
    "\n",
    "# Convert image data to one-dimensional vector, show dimensions\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTCzaUf4WMch"
   },
   "source": [
    "One of the major challenges of image processing and computer vision is that since every pixel location in a collection of images is a feature, as the images get larger, the number of features explodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwYQWeRDWMs0"
   },
   "outputs": [],
   "source": [
    "# Load image in grayscale\n",
    "#TODO\n",
    "\n",
    "# Convert image data to one-dimensional vector, show dimensions\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Kr9BFDdWUjq"
   },
   "source": [
    "And the number of features only intensifies when the image is in color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUDxSIWFWYIj"
   },
   "outputs": [],
   "source": [
    "# Load image in color\n",
    "#TODO\n",
    "\n",
    "# Convert image data to one-dimensional vector, show dimensions\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LIFIX3-WhfM"
   },
   "source": [
    "As demonstrated in the output, a compact color image already encompasses approximately 200,000 features. This can pose challenges during model training since the number of features may greatly surpass the number of observations available, leading to potential issues."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
