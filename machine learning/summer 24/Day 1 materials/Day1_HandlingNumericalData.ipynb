{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Handling Numerical Data\n",
        "\n",
        "Quantitative data is the measurement of something—whether class size, monthly sales, or student scores. The natural way to represent these quantities is numerically (e.g., 29 students, $529,392 in sales). In this section, we will cover numerous strategies for transforming raw numerical data into features purpose-built for machine learning algorithms.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dDzDWCHP2rlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Rescaling a Feature\n",
        "\n",
        "\n",
        "Rescaling is a common preprocessing task in machine learning. Many of the algorithms described later the book will assume all features are on the same scale, typically 0 to 1 or –1 to 1. There are a number of rescaling techniques, but one of the simplest is called min-max scaling.\n",
        "\n",
        "##MinMaxScaler\n",
        "\n",
        "**class   sklearn.preprocessing.MinMaxScaler**(**feature_range**=(0, 1), *, copy=True, clip=False)\n",
        "\n",
        "[source](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n",
        "\n",
        "Transform features by scaling each feature to a given range.\n",
        "\n",
        "This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."
      ],
      "metadata": {
        "id": "osbqPuC-607Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem\n",
        "You need to rescale the values of a numerical feature to be between two values.\n",
        "\n",
        "##Solution\n",
        "Use scikit-learn’s MinMaxScaler to rescale a feature array:"
      ],
      "metadata": {
        "id": "fso7MoTY897k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Create feature\n",
        "feature = np.array([[-500.5],\n",
        "                    [-100.1],\n",
        "                    [0],\n",
        "                    [100.1],\n",
        "                    [900.9]])\n",
        "\n",
        "# Create min-max scaler\n",
        "# TODO\n",
        "\n",
        "# Scale feature\n",
        "# TODO\n",
        "\n",
        "# Show feature\n",
        "# TODO"
      ],
      "metadata": {
        "id": "hPlXpncN56KB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.2 Standardizing a Feature\n",
        "A common alternative to MinMax scaling discussed in previous recipe is rescaling of features to be approximately standard normally distributed. Standardize features by removing the mean and scaling to unit variance.\n",
        "\n",
        "Standardization is a common go-to scaling method for machine learning preprocessing and in my experience is used more than min- max scaling. However, it depends on the learning algorithm. For example, principal component analysis often works better using standardization, while min-max scaling is often recommended for neural networks (both algorithms are discussed later in this book). As a general rule, the writer of the book recommends defaulting to standardization unless you have a specific reason to use an alternative.\n",
        "\n",
        "## StandardScaler\n",
        "**class   sklearn.preprocessing.StandardScaler**(*, copy=True, with_mean=True, with_std=True)\n",
        "\n",
        "[source](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
        "\n",
        "## Problem\n",
        "You want to transform a feature to have a mean of 0 and a standard deviation of 1.\n",
        "## Solution\n",
        "scikit-learn’s StandardScaler performs both transformations:"
      ],
      "metadata": {
        "id": "dzAfbBvT807W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Create feature\n",
        "x = np.array([[-1000.1],\n",
        "              [-200.2],\n",
        "              [500.5],\n",
        "              [600.6],\n",
        "              [9000.9]])\n",
        "\n",
        "# Create standarizer scaler\n",
        "# TODO\n",
        "\n",
        "# Transform the feature\n",
        "# TODO\n",
        "\n",
        "# Show feature\n",
        "# TODO"
      ],
      "metadata": {
        "id": "ypwWmFfz-2U-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the effect of standardization by looking at the mean and standard deviation of our solution’s output:"
      ],
      "metadata": {
        "id": "G0Ontp3q_ETu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print mean and standard deviation\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "1cQzADfi_FeR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If our data has significant outliers, it can negatively impact our standardization by affecting the feature’s mean and variance. In this scenario, it is often helpful to instead rescale the feature using the median and quartile range. In scikit-learn, we do this using the **RobustScaler** method:"
      ],
      "metadata": {
        "id": "rE52wuCN_SlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create robust scaler\n",
        "# TODO\n",
        "\n",
        "# Transform feature\n",
        "# TODO"
      ],
      "metadata": {
        "id": "sPx4QwbQ_Tmb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.3 Normalizing Observations\n",
        "Many rescaling methods (e.g., MinMax scaling and standardization) operate on features; however, we can also rescale across individual observations. Normalizer rescales the values on individual observations to have unit norm (the sum of their lengths is 1). This type of rescaling is often used when we have many equivalent features (e.g., text classification when every word or n-word group is a feature).\n",
        "\n",
        "## Normalizer\n",
        "**class    sklearn.preprocessing.Normalizer**(norm='l2', *, copy=True)\n",
        "\n",
        "[source](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html)\n",
        "\n",
        "## Problem\n",
        "You want to rescale the feature values of observations to have unit norm (a total length of 1).\n",
        "## Solution\n",
        "Use Normalizer with a norm argument:"
      ],
      "metadata": {
        "id": "v7EGArK0_g8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create feature matrix\n",
        "features = np.array([[0.5, 0.5],\n",
        "                     [1.1, 3.4],\n",
        "                     [1.5, 20.2],\n",
        "                     [1.63, 20.2],\n",
        "                     [10.9, 3.3]])\n",
        "\n",
        "# Create normalizer\n",
        "# TODO\n",
        "\n",
        "# Transform feature matrix\n",
        "# TODO"
      ],
      "metadata": {
        "id": "bJXE4pdKAjNn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Transforming Features\n",
        "It is common to want to make some custom transformations to one or more features. For example, we might want to create a feature that is the natural log of the values of the different feature. We can do this by creating a function and then mapping it to features using either scikit- learn’s FunctionTransformer or pandas’ apply. In the next solution we created a very simple function, add_ten, which added 10 to each input, but there is no reason we could not define a much more complex function.\n",
        "\n",
        "## FunctionTransformer\n",
        "**class   sklearn.preprocessing.FunctionTransformer**(func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, feature_names_out=None, kw_args=None, inv_kw_args=None)\n",
        "[source](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)\n",
        "\n",
        "## Problem\n",
        "You want to make a custom transformation to one or more features.\n",
        "## Solution\n",
        "In scikit-learn, use FunctionTransformer to apply a function to a set of features:"
      ],
      "metadata": {
        "id": "iV1kaLSjBHaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Create feature matrix\n",
        "features = np.array([[2, 3],\n",
        "                     [2, 3],\n",
        "[2, 3]])\n",
        "\n",
        "# Define a simple function\n",
        "def add_ten(x): return x + 10\n",
        "\n",
        "# Create transformer\n",
        "# TODO\n",
        "\n",
        "# Transform feature matrix\n",
        "# TODO"
      ],
      "metadata": {
        "id": "pKdxBPTtB5JD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Detecting Outliers\n",
        "There is no single best technique for detecting outliers. Instead, we have a collection of techniques all with their own advantages and disadvantages. Our best strategy is often trying multiple techniques and looking at the results as a whole.\n",
        "If at all possible, we should take a look at observations we detect as outliers and try to understand them. For example, if we have a dataset of houses and one feature is number of rooms, is an outlier with 100 rooms really a house or is it actually a hotel that has been misclassified?\n",
        "\n",
        "## Problem\n",
        "You want to identify extreme observations.\n",
        "## Solution\n",
        "Detecting outliers is unfortunately more of an art than a science. However, a common method is to assume the data is normally distributed and based on that assumption “draw” an ellipse around the data, classifying any observation inside the ellipse as an inlier (labeled as 1) and any observation outside the ellipse as an outlier (labeled as -1):\n"
      ],
      "metadata": {
        "id": "spCYElPXCILY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Create simulated data\n",
        "features, _ = make_blobs(n_samples = 10,\n",
        "                         n_features = 2,\n",
        "                         centers = 1,\n",
        "                         random_state = 1)\n",
        "\n",
        "# Replace the first observation's values with extreme value\n",
        "features[0,0] = 10000\n",
        "features[0,1] = 10000\n",
        "\n",
        "# Create detector\n",
        "# TODO\n",
        "\n",
        "# Fit detector\n",
        "# TODO\n",
        "\n",
        "# Predict outliers\n",
        "# TODO"
      ],
      "metadata": {
        "id": "y0k4qcpECvF3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Handling Outliers\n",
        "Similar to detecting outliers, there is no hard-and-fast rule for handling them. How we handle them should be based on two aspects. First, we should consider what makes them an outlier. If we believe they are errors in the data such as from a broken sensor or a miscoded value, then we might drop the observation or replace outlier values with NaN since we can’t believe those values. However, if we believe the outliers are genuine extreme values (e.g., a house (mansion) with 200 bathrooms), then marking them as outliers or transforming their values is more appropriate.\n",
        "\n",
        "Second, how we handle outliers should be based on our goal for machine learning. For example, if we want to predict house prices based on features of the house, we might reasonably assume the price for mansions with over 100 bathrooms is driven by a different dynamic than regular family homes. Furthermore, if we are training a model to use as part of an online home loan web application, we might assume that our potential users will not include billionaires looking to buy a mansion.\n",
        "\n",
        "So what should we do if we have outliers? Think about why they are outliers, have an end goal in mind for the data, and, most importantly, remember that not making a decision to address outliers is itself a decision with implications.\n",
        "\n",
        "One additional point: if you do have outliers standardization might not be appropriate because the mean and variance might be highly influenced by the outliers. In this case, use a rescaling method more robust against outliers like RobustScaler.\n",
        "\n"
      ],
      "metadata": {
        "id": "iM_y4cBBDVaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem\n",
        "You have outliers.\n",
        "## Solution\n",
        "Typically we have three strategies we can use to handle outliers. First, we can drop them:"
      ],
      "metadata": {
        "id": "K5JVN0N3Gneg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame\n",
        "houses = pd.DataFrame()\n",
        "houses['Price'] = [534433, 392333, 293222, 4322032]\n",
        "houses['Bathrooms'] = [2, 3.5, 2, 116]\n",
        "houses['Square_Feet'] = [1500, 2500, 1500, 48000]\n",
        "\n",
        "# Filter observations less than 20\n",
        "# TODO"
      ],
      "metadata": {
        "id": "I_sQjz5JGq5j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we can mark them as outliers and include it as a feature:"
      ],
      "metadata": {
        "id": "clJidvsDG20F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "import numpy as np\n",
        "\n",
        "# Create feature based on boolean condition (if less than 20 retyrn 0, otherwise 1)\n",
        "# TODO\n",
        "\n",
        "# show data\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "JfWh5msvG6LT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can transform the feature to dampen the effect of the outlier:"
      ],
      "metadata": {
        "id": "YWrFTki8HF23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment:\n",
        "# # Log feature\n",
        "# houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\" ]]\n",
        "\n",
        "# # Show data\n",
        "# houses"
      ],
      "metadata": {
        "id": "VCZPojuQHKxF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Discretizating Features\n",
        "Discretization can be a fruitful strategy when we have reason to believe that a numerical feature should behave more like a categorical feature. For example, we might believe there is very little difference in the spending habits of 19- and 20-year-olds, but a significant difference between 20- and 21-year-olds (the age in the United States when young adults can consume alcohol). In that example, it could be useful to break up individuals in our data into those who can drink alcohol and those who cannot. Similarly, in other cases it might be useful to discretize our data into three or more bins.\n",
        "\n",
        "## Binarizer\n",
        "**class   sklearn.preprocessing.Binarizer**(*, threshold=0.0, copy=True)\n",
        "[source](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Binarizer.html)\n",
        "\n",
        "Binarize data (set feature values to 0 or 1) according to a threshold.\n",
        "\n",
        "Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.\n",
        "\n",
        "Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.\n",
        "\n",
        "It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).\n",
        "\n",
        "## Problem\n",
        "You have a numerical feature and want to break it up into discrete bins.\n",
        "## Solution\n",
        "Depending on how we want to break up the data, there are two techniques we can use. First, we can binarize the feature according to some threshold:"
      ],
      "metadata": {
        "id": "j7BGESE7I7Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# Create feature\n",
        "age = np.array([[6],[12],[20],[36],[65]])\n",
        "\n",
        "# Create binarizer\n",
        "# TODO\n",
        "\n",
        "# Transform feature\n",
        "# TODO"
      ],
      "metadata": {
        "id": "LlyaqyiSJUPl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Deleting Observations with Missing Values\n",
        "\n",
        "Most machine learning algorithms cannot handle any missing values in the target and feature arrays. For this reason, we cannot ignore missing values in our data and must address the issue during preprocessing.\n",
        "\n",
        "The simplest solution is to delete every observation that contains one or more missing values, a task quickly and easily accomplished using NumPy or pandas. Keep in mind that based on the nature of the data we might be very reluctant to delete observations with missing values. Deleting them is the nuclear option, since our algorithm loses access to the information contained in the observation’s non-missing values.\n",
        "Just as important, depending on the cause of the missing values, deleting observations can introduce bias into our data.\n",
        "\n",
        "## pandas.DataFrame.**dropna**\n",
        "**DataFrame.dropna**(*, axis=0, how=_NoDefault.no_default, thresh=_NoDefault.no_default, subset=None, inplace=False, ignore_index=False)\n",
        "[source](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)\n",
        "\n",
        "## Problem\n",
        "You need to delete observations containing missing values.\n",
        "## Solution\n",
        "Deleting observations with missing values is easy using pandas:"
      ],
      "metadata": {
        "id": "798kATXaLNkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "dataframe = pd.DataFrame(features, columns=[\"feature_1\", \"feature_2\"])\n",
        "\n",
        "# Remove observations with missing values\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "xbgy8e1jNbIO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can drop missing observations using NumPy:"
      ],
      "metadata": {
        "id": "r0YZNkx0NkHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "import numpy as np\n",
        "\n",
        "# Create feature matrix\n",
        "features = np.array([[1.1, 11.1],\n",
        "                     [2.2, 22.2],\n",
        "                     [3.3, 33.3],\n",
        "                     [4.4, 44.4],\n",
        "                     [np.nan, 55]])\n",
        "\n",
        "# Keep only observations that are not (denoted by ~) missing\n",
        "# TODO"
      ],
      "metadata": {
        "id": "bLkkiq3AM-P0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.9 Imputing Missing Values\n",
        "\n",
        "\n",
        "There are two main strategies for replacing missing data with substitute values, each of which has strengths and weaknesses. First, we can use machine learning to predict the values of the missing data. To do this we treat the feature with missing values as a target vector and use the remaining subset of features to predict missing values. While we can use a wide range of machine learning algorithms to impute values, a popular choice is KNN.  KNN is addressed in depth later, but the short explanation is that the algorithm uses the k nearest observations (according to some distance metric) to predict the missing value. In our solution we predicted the missing value using the five closest observations.\n",
        "\n",
        "The downside to KNN is that in order to know which observations are the closest to the missing value, it needs to calculate the distance between the missing value and every single observation. This is reasonable in smaller datasets, but quickly becomes problematic if a dataset has millions of observations.\n",
        "\n",
        "An alternative and more scalable strategy is to fill in all missing values with some average value. For example, in our solution we used scikit-learn to fill in missing values with a feature’s mean value. The imputed value is often not as close to the true value as when we used KNN, but we can scale mean-filling to data containing millions of observations easily.\n",
        "\n",
        "If we use imputation, it is a good idea to create a binary feature indicating whether or not the observation contains an imputed value.\n",
        "\n",
        "## Problem\n",
        "You have missing values in your data and want to fill in or predict their values.\n",
        "## Solution\n",
        "If you have a small amount of data, predict the missing values using k-nearest neighbors (KNN):"
      ],
      "metadata": {
        "id": "0hdJEsWrNxz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fancyimpute"
      ],
      "metadata": {
        "id": "Zx7h0wAcQClS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from fancyimpute import KNN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Make a simulated feature matrix\n",
        "features, _ = make_blobs(n_samples = 1000,\n",
        "                         n_features = 2,\n",
        "                         random_state = 1)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "standardized_features = scaler.fit_transform(features)\n",
        "\n",
        "# Replace the first feature's first value with a missing value\n",
        "# TODO\n",
        "\n",
        "# uncomment\n",
        "# # Predict the missing values in the feature matrix\n",
        "# features_knn_imputed = KNN(k=5, verbose=0).fit_transform ( standardized_features )\n",
        "\n",
        "# Compare true and imputed values\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "xchEK-GpOvFe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatively, we can use scikit-learn’s *SimpleImputer* module to fill in missing values with the feature’s mean, median, or most frequent value. However, we will typically get worse results than KNN.\n",
        "\n",
        "## SimpleImputer\n",
        "**class   sklearn.impute.SimpleImputer**(*, missing_values=nan, strategy='mean', fill_value=None, copy=True, add_indicator=False, keep_empty_features=False)\n",
        "[source](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)"
      ],
      "metadata": {
        "id": "9zwGbnxuQR4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load library\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create imputer\n",
        "# TODO\n",
        "\n",
        "# Impute values\n",
        "# TODO\n",
        "\n",
        "# Compare true and imputed values\n",
        "# TODO"
      ],
      "metadata": {
        "id": "Dr2EApqWQS0u"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}