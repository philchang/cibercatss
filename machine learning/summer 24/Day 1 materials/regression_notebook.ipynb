{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4RA9hDUj-CY"
      },
      "source": [
        "# Regression Model\n",
        "\n",
        "Informally, a model that generates a numerical prediction. (In contrast, a classification model generates a class prediction.) For example, the following are all regression models:\n",
        "\n",
        "\n",
        "\n",
        "*   A model that predicts a certain house's value, such as 423,000 Euros.\n",
        "*   A model that predicts a certain tree's life expectancy, such as 23.2 years.\n",
        "*   A model that predicts the amount of rain that will fall in a certain city over the next six hours, such as 0.18 inches.\n",
        "\n",
        "Two common types of regression models are:\n",
        "\n",
        "\n",
        "*   Linear regression, which finds the line that best fits label values to features.\n",
        "*   Logistic regression, which generates a probability between 0.0 and 1.0 that a system typically then maps to a class prediction.\n",
        "\n",
        "Not every model that outputs numerical predictions is a regression model. In some cases, a numeric prediction is really just a classification model that happens to have numeric class names. For example, a model that predicts a numeric postal code is a classification model, not a regression model.\n",
        "[source](https://developers.google.com/machine-learning/glossary#regression_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULmpHSvLCnNB"
      },
      "source": [
        "\n",
        "##Problem\n",
        "You want to train a model that represents a linear relationship between the feature and target vector.\n",
        "\n",
        "Dataset: **The California housing dataset**. This dataset can be fetched from internet using scikit-learn.\n",
        "\n",
        "Target variable: The **median house value** for California districts,\n",
        "expressed in hundreds of thousands of dollars ($100,000).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution\n",
        "Using regression algorithms some commonly used regression algorithms:\n",
        "\n",
        "1.   Linear regression\n",
        "2.   Lasso Regression (L1 Regularization)\n",
        "3.   Ridge Regression (L2 Regularization)\n",
        "\n",
        "First, let's start with loading the dataet, data visualization and data prepration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rhsSLfq0n4CQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a09bmWf_-VOn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#load california housing dataset from scikit\n",
        "#TODO\n",
        "\n",
        "#load dataset\n",
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can have a first look at the available description"
      ],
      "metadata": {
        "id": "xCI2h9ku4-B0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# california housing dataset description using DESCR function\n",
        "#TODO"
      ],
      "metadata": {
        "id": "mRFHotNd4_fa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJIHix4BHD0W"
      },
      "source": [
        "### First, let's get familiarized with the data that we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f6yKBWLDFjF6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#let's take a look at the data frame head\n",
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset, we have information regarding the demography (income, population, house occupancy) in the districts, the location of the districts (latitude, longitude), and general information regarding the house in the districts (number of rooms, number of bedrooms, age of the house)."
      ],
      "metadata": {
        "id": "UkUfHyHh6izX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eA1s1N7WGFXM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#show description of data\n",
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let’s have a look to the target to be predicted.\n",
        "\n",
        "The target contains the median of the house value for each district. Therefore, this problem is a regression problem."
      ],
      "metadata": {
        "id": "Q9TkYAEy5ziC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "StRouBIz51WW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud1EkQzuGpma"
      },
      "source": [
        "We can see that:\n",
        "\n",
        "the dataset contains 20,640 samples and 8 features\n",
        "\n",
        "all features are numerical features encoded as floating number\n",
        "\n",
        "there is no missing values:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing data\n",
        "#TODO\n"
      ],
      "metadata": {
        "id": "disUOK8X2J1Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data frame info\n",
        "#TODO\n"
      ],
      "metadata": {
        "id": "q-djVrgP2cdW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s have a quick look at the distribution of these features by plotting their histograms."
      ],
      "metadata": {
        "id": "oDYPEDEn8jaR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ei3BxrnuHC2K",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# distribution of features by plotting their histograms\n",
        "# TODO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKWJMcSsIgaa"
      },
      "source": [
        "Now it is time for exploring more. First of all, we want to visualize the geographical data with latitude and longitude. A good way to do this is to create a scatterplot of all the districts. It is important that you set alpha equal to 0.2, because then the scatterplot has a high density and therefore it is much easier to visualize. Try differnet alpha values to see the changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ufVdvbhDJCGF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# plot the housing value with respect to longitude and latitude\n",
        "# TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3RLkTjwJMy3"
      },
      "source": [
        "Yes! Map of California! Please note that California's big cities: San Diego, Los Angeles, San Jose, or San Francisco, are located in the east coast!\n",
        "\n",
        "The above color map shows the house value and the radius of the circles corresponding to the population of the areas.\n",
        "\n",
        "Based on this plot, we can conclude that:\n",
        "1. Houses near ocean value more, such as San Diego, Los Angeles, San Jose, and San Francisco.\n",
        "2. House in high population density area also value more but the effect decreases as we move further away from the ocean.\n",
        "3. And there are some outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Searching for Correlations:\n",
        "The housing dataset isn't that large and therefore we can easily compute the correlations between every attribute using the \"corr()\" method. We will start by looking how much each attribute is correlated to the median house value."
      ],
      "metadata": {
        "id": "cOMDpQiTluFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrolations between attributes\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "YHlyFLLn07ld"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coefficient of the correlation ranges from 1 to -1. The closer it is to 1 the more correlated it is and vice versa. Correlations that are close to 0, means that there is no correlation, neither negative or positive. You can see that the median_income is correlated the most with the median house value. Because of that, we will generate a more detailed scatterplot below:"
      ],
      "metadata": {
        "id": "wz1OHjM6mGY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation between MedInc and MedHouseVal\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "dH8YjWrN1YCM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAW1AzVmJnqX"
      },
      "source": [
        "## Random Sampling\n",
        "We can perform random subsampling to reduce the number of data points for plotting, while still capturing the relevant characteristics.\n",
        "\n",
        "**DataFrame.sample**(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None, ignore_index=False) [source](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)\n",
        "\n",
        "Return a random sample of items from an axis of object. You can use random_state for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "caCS1RYyJo5N",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make a final analysis by making a pair plot of all features and the target but dropping the longitude and latitude. We will quantize the target such that we can create proper histogram."
      ],
      "metadata": {
        "id": "ey-CaRcW_PQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Scaling\n",
        "Feature scaling is one of the most important transformations you need to apply, since nearly all machine learning algorithms perform bad when the input numerical attributes have widely varying scales, which is the case at our current dataset. For example, the median incomes range from o to 15, but the total number of rooms from 6 to 39,320. Note that scaling the target values is not required.\n",
        "\n",
        "There are two common ways:\n",
        "\n",
        "\n",
        "*   min-max scaling\n",
        "*   standardization\n",
        "\n",
        "We use standardization here, feel free to try min-max scaling too.\n",
        "\n"
      ],
      "metadata": {
        "id": "dVXgsUd7pLxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization\n",
        "# TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "8iw8cAbyprug"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNKyUyt2KbDA"
      },
      "source": [
        "## Split the dataset for testing and training\n",
        "\n",
        "Here, we are randomly splitting the data into training and testing set using train_test_split() method. 80% is kept for training and 20% for testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset for testing and training\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "lPnS_LfSBJ1S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPSXD33r-XrG"
      },
      "source": [
        "##1. Linear Regression\n",
        "\n",
        "Linear regression is a basic supervised learning algorithm that is widely used for making predictions. It is often taught in introductory statistics courses and is considered a fundamental technique in data analysis. Although it is straightforward and relatively simple compared to other machine learning algorithms, **linear regression remains valuable for predicting quantitative values such as home prices or ages**. Despite its simplicity, linear regression and its variations remain relevant and effective in practical applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting Linear Regression model"
      ],
      "metadata": {
        "id": "uvtFiAc7BdUw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vhXLQ7UoF6J4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Instantiate a linear regression object\n",
        "#TODO\n",
        "\n",
        "\n",
        "# fitting model or training model:\n",
        "#TODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you know linear regression assumes that the relationship between the features and the target vector is approximately linear. That is, the effect (also called coefficient, weight, or parameter) of the features on the target vector is constant. In our solution, we have trained our model using  8 features."
      ],
      "metadata": {
        "id": "nWSh0CD0w5te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show features\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "NNerOi472Kmg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we have fit our model, we can view the value of each parameter. Bias or intercept, can be viewed using intercept_:"
      ],
      "metadata": {
        "id": "E28tEP6X2hRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the intercept\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "g0156p9Cw6DF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And coefficients are shown using coef_:"
      ],
      "metadata": {
        "id": "VGf0aN1kxB9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the feature coefficients\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "SXUMzgXwxCLD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our dataset, the target variable is the median house value for California houses, expressed in hundreds of thousands of dollars ($100,000). Therefore the price of the first home in the dataset is:"
      ],
      "metadata": {
        "id": "Nw2_KqHqxJk0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First value in the target vector (y) multiplied by 100,000\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "3m7322NRxJvg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the target value of the first observation, multiplied by 100,000\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "HEfGOafIxT_6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Difference between actual and predicted values\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "30O1z-49xk7t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It means our model was off by $33,668! How can we be sure about this model?"
      ],
      "metadata": {
        "id": "KxQlY4pmx-Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold Cross-Validation\n",
        "As we know we don't want to use the test set until we are confident about our model. But how can we test how our model performs if we can't use the test data ? One way to do this is using **K-Fold Cross-Validation**, which uses part of the training set for training and a part for validation. The following code randomly splits the training set into 10 subset called folds. Then it trains and evaluates 10 times, using every fold once for either training or validation:"
      ],
      "metadata": {
        "id": "Hr2_IDrjuUN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "# TODO\n",
        "\n",
        "\n",
        "# Define cross-validation strategy\n",
        "# TODO\n",
        "\n",
        "\n",
        "# uncomment:\n",
        "# cv_scores = cross_val_score(regressor, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
        "\n",
        "# cv_predictions = cross_val_predict(regressor, X, y, cv=kf)\n",
        "\n",
        "# test set\n",
        "# uncomment\n",
        "# cv_scores_test = cross_val_score(regressor, X_test, y_test, cv=kf, scoring='neg_mean_squared_error')\n",
        "# y_pred_test = regressor.predict(X_test)\n"
      ],
      "metadata": {
        "id": "FAzMe8QkDnNp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note**: The term neg_mean_squared_error refers to a scoring method used in cross-validation and model evaluation in scikit-learn, where the goal is to minimize the Mean Squared Error (MSE). In scikit-learn, some metrics are defined as being maximized (higher is better), so for metrics like MSE, which are minimized (lower is better), the negative value is used to allow the cross-validation function to maximize a score by minimizing the error.\n",
        "\n",
        "[more information on Metrics and scoring](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
        "\n",
        "The *random_state* parameter in KFold cross-validation is used to control the randomness involved in the shuffling process of the data before splitting it into folds. Setting a random_state ensures that the same data split is used every time the code is run, which helps in achieving reproducibility of the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "YNZmZbBu70j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model"
      ],
      "metadata": {
        "id": "dJqPIsky9N8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics for training set\n",
        "\n",
        "# uncomment and complete\n",
        "# TODO\n",
        "\n",
        "# mae =\n",
        "# mse =\n",
        "# rmse =\n",
        "# r2 =\n",
        "\n",
        "# # Print evaluation metrics\n",
        "# print(f\"Mean Absolute Error (MAE): {mae}\")\n",
        "# print(f\"Cross-Validated Mean Squared Error (MSE): {-np.mean(cv_scores)}\")\n",
        "# print(f\"R-squared: {r2}\")\n"
      ],
      "metadata": {
        "id": "azqumoBE-gwS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scores explanation (Standardized data):\n",
        "Since we scaled (using standardization) the features (independent variables) and target (dependent variable), this can affect the range and magnitude of the evaluation metrics. So, the scores are between 0 and 1.\n",
        "\n",
        " **MAE**: 0.46106034365834575, same as MSE, the model's predicted house prices are about 0.46 units away from the actual values in standardized units. This indicates that the model's predictions are fairly close to the actual values.\n",
        "\n",
        "  **MSE**: 0.3999298125600988, indicates nn average, the squared differences between the predicted and actual house prices across the cross-validation folds are approximately 0.40 in standardized units. This suggests that the model has a reasonably good fit, though not perfect.\n",
        "\n",
        "**R-squared**: 0.6000701874399013, indicates approximately 60% of the variability in the house prices can be explained by the model's predictors. This indicates a moderate level of explanatory power, with the remaining 40% of the variability due to other factors or noise."
      ],
      "metadata": {
        "id": "-khXUzIQA5TG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion to Original Scale\n",
        "\n",
        "Let's see the evaluation metrics with original house prices (no scaling) to better sense the scores."
      ],
      "metadata": {
        "id": "d7McOouFEw27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean (original data)\n",
        "# TODO\n",
        "# original_mean =\n",
        "# print(\"Mean of riginal house prices = $\", original_mean)\n",
        "\n",
        "# Standard Deviation (original data)\n",
        "# TODO\n",
        "# original_std =\n",
        "# print(\"Standard deviation of riginal house prices = $\", original_std)"
      ],
      "metadata": {
        "id": "h-Z33JU6E0Ho"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions and actuals back to the original scale\n",
        "# uncomment:\n",
        "# predictions_original_scale = cv_predictions * original_std + original_mean\n",
        "# actuals_original_scale = y * original_std + original_mean\n",
        "\n",
        "# Calculate evaluation metrics on original scale\n",
        "# TODO\n",
        "\n",
        "# Print evaluation metrics\n",
        "# TODO"
      ],
      "metadata": {
        "id": "f3WR8_RpH182"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scatter plot of actual vs predicted values\n",
        "# TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "RZfwy5YuEd7s"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. LASSO Regression (L1 Regularization)\n",
        "\n",
        "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), is a technique used in regression models to prevent overfitting and to perform feature selection.\n",
        "\n",
        "**Regularization**: Regularization adds a penalty to the cost (loss) function (the function the model tries to minimize). This penalty discourages the model from fitting the training data too closely, which helps to generalize better to new data\n",
        "\n",
        "**L1 Penalty**: In L1 regularization, the penalty is proportional to the sum of the absolute values of the coefficients in cost function.\n",
        "\n",
        "**Feature Selection**: One of the key features of L1 regularization is that it can shrink some coefficients to exactly zero. This means that the model effectively ignores those features, performing automatic feature selection. This is particularly useful when you have many features, some of which may be irrelevant.\n",
        "\n",
        "**Controlling Overfitting**: By adding this penalty, L1 regularization prevents the model from becoming too complex and fitting the noise in the training data, thus reducing the risk of overfitting (n undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for unseen data).\n",
        "\n",
        "\n",
        "*   Example: Predicting medical expenses and identifying the most significant factors affecting costs.\n",
        "\n",
        "\n",
        "**Note**: In the context of L1 and L2 regularization, \"L1\" and \"L2\" refer to different types of norm-based penalties applied to the regression coefficients to prevent overfitting and improve generalization."
      ],
      "metadata": {
        "id": "K-YBzFahf0s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Lasso Regression Model"
      ],
      "metadata": {
        "id": "oAcGBDY7gKMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Train the Lasso model\n",
        "# You can adjust alpha for regularization strength\n",
        "\n",
        "# instantiate a lasso object\n",
        "# TODO\n",
        "\n",
        "# fit the model on train set\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "D2batVbjf4Fg"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Predictions"
      ],
      "metadata": {
        "id": "K-kLHS84gLiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions by the trained lasso model\n",
        "# TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "p0WZVSIAgN_a"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model"
      ],
      "metadata": {
        "id": "PGfryElVgbwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics (get help from previous model exaluation)\n",
        "# Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared\n",
        "# TODO\n",
        "\n",
        "# Print evaluation metrics\n",
        "# Calculate evaluation metrics\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "3ko9Vr5dgcsO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation"
      ],
      "metadata": {
        "id": "sureSLU4g6if"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation (get help from previous model)\n",
        "# TODO\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HNmRaFlsg8Id"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid Search for optimization\n",
        "Grid search is a method used for hyperparameter tuning in machine learning. It systematically works through multiple combinations of parameter values, cross-validates each combination, and determines the set of parameters that gives the best performance. The main goal of grid search is to find the optimal hyperparameters for a given model to improve its accuracy or other performance metrics.\n",
        "\n",
        "Here we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) from sklearn.\n",
        "\n"
      ],
      "metadata": {
        "id": "D2ybU_8xiFcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the Alpha Hyperparameter\n",
        "You can tune the alpha parameter to find the optimal value for the dataset. The alpha hyperparameter determines the strength of regularization applied to the model.\n",
        "\n",
        "### Effect of Alpha:\n",
        "\n",
        "High Alpha: A high alpha value increases the penalty on the coefficients, leading to more coefficients being shrunk to zero. This results in a simpler model with potentially fewer features (automatic feature selection).\n",
        "Low Alpha: A low alpha value reduces the penalty on the coefficients, making the model more similar to ordinary least squares regression with little to no regularization."
      ],
      "metadata": {
        "id": "cc8c0IUEBPdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "# param_grid = {'alpha': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 1, 10, 100]}\n",
        "\n",
        "# Perform grid search with GridSearchCV\n",
        "# TODO\n",
        "\n",
        "# fit grid search\n",
        "# TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "-6raZ8MIiIbn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best alpha\n",
        "# TODO\n",
        "\n",
        "# Train the Lasso model with the best alpha\n",
        "# TODO\n",
        "\n",
        "# fit the best lasso model\n",
        "# TODO\n",
        "\n",
        "# Make predictions and evaluate\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "R-WlSDuXiQgL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best LASSO model evaluation"
      ],
      "metadata": {
        "id": "ozsRTcYlDDsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared\n",
        "# TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "wpcI15cOiSlo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Ridge Regression (L2 Regularization)\n",
        "L2 regularization, also known as Ridge regression, is a technique used in regression models to prevent overfitting and improve the model's generalization by adding a penalty term to the loss function. This penalty term is proportional to the square of the magnitude of the coefficients.\n",
        "\n",
        "*   **L2 Penalty**: In L2 regularization, the penalty is proportional to the sum of the squares of the coefficients of the cost function.\n",
        "\n",
        "*   **Shrinkage**: The L2 penalty causes the coefficients to be \"shrunk\" towards zero, but not exactly zero. This means that all features are kept in the model, but their impact is reduced.\n",
        "\n",
        "*   **Controlling Overfitting**: By adding this penalty, L2 regularization prevents the model from becoming too complex and fitting the noise in the training data, thus reducing the risk of overfitting.\n",
        "\n",
        "\n",
        "Example: Predicting housing prices with regularization to avoid overfitting on training data."
      ],
      "metadata": {
        "id": "HyL-HxRTj8xR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Ridge Regression Model"
      ],
      "metadata": {
        "id": "CRkMwMIaj903"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# instantiate a Ridge object\n",
        "# You can adjust alpha for regularization strength\n",
        "# TODO\n",
        "\n",
        "# Train the Ridge model\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "Aja6SopNkAeH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Predictions with Ridge model"
      ],
      "metadata": {
        "id": "3jgBLP7qkL5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "8DStQfMokM8X"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate Ridge Model"
      ],
      "metadata": {
        "id": "shw4yTuflndU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared\n",
        "# Calculate evaluation metrics\n",
        "# TODO\n"
      ],
      "metadata": {
        "id": "bv8IzrGLlp2C"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning the Alpha Parameter using grid search"
      ],
      "metadata": {
        "id": "4KObF5ERmB3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usee parameter grid from. earlier\n",
        "\n",
        "# Perform grid search\n",
        "# TODO\n",
        "\n",
        "# Best alpha\n",
        "# TODO\n",
        "\n",
        "# Train the Ridge model with the best alpha\n",
        "# TODO\n",
        "\n",
        "# Make predictions and evaluate\n",
        "# TODO\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kKq5Lvf4mCvQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Ridge model evaluation"
      ],
      "metadata": {
        "id": "PMZAzJyuEENC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared\n",
        "# TODO"
      ],
      "metadata": {
        "id": "bW4OgbDREB9W"
      },
      "execution_count": 45,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PyCamber",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}