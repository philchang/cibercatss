{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74MGv0HjzPM-"
   },
   "source": [
    "#Day 1: Introduction to Machine Learning\n",
    "In this session, we'll start with an overview of machine learning and its applications, followed by an introduction to the three main types of machine learning and examples: supervised, unsupervised, and reinforcement learning.\n",
    "\n",
    "We will then cover evaluation metrics: accuracy, precision, recall, and F1 score. Finally, we will work on a hands-on exercise by implementing a basic supervised model in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcKqx5lvzSDG"
   },
   "source": [
    "#An overview of machine learning and its applications:\n",
    "Machine learning is a subfield of artificial intelligence that involves building models that can learn from data, and then use that learning to make predictions or decisions. In machine learning, we typically start with a dataset that includes some inputs and corresponding outputs. The goal is to use this dataset to train a model that can take new inputs and predict the corresponding outputs.\n",
    "\n",
    "There are several types of machine learning algorithms, including:\n",
    "\n",
    "- Supervised learning: This involves learning from labeled data, where the inputs and corresponding outputs are provided. Examples of supervised learning algorithms include linear regression, logistic regression, decision trees, and neural networks.\n",
    "\n",
    "- Unsupervised learning: This involves learning from unlabeled data, where only the inputs are provided. Examples of unsupervised learning algorithms include k-means clustering, hierarchical clustering, and principal component analysis (PCA).\n",
    "\n",
    "- Reinforcement learning: This involves learning from trial and error, where the model takes actions and receives feedback in the form of rewards or penalties. Examples of reinforcement learning algorithms include Q-learning and policy gradient methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNlTcNhl9AGg"
   },
   "source": [
    "Machine learning has a wide range of applications across many different fields, including:\n",
    "\n",
    "- Natural language processing: Machine learning can be used to build models that can understand and generate natural language. Applications include language translation, sentiment analysis, and chatbots.\n",
    "\n",
    "- Computer vision: Machine learning can be used to build models that can recognize and interpret images and video. Applications include facial recognition, object detection, and self-driving cars.\n",
    "\n",
    "- Healthcare: Machine learning can be used to build models that can predict disease progression, identify risk factors for certain conditions, and recommend treatment plans.\n",
    "\n",
    "- Finance: Machine learning can be used to build models that can predict stock prices, detect fraud, and assess credit risk.\n",
    "\n",
    "- Marketing: Machine learning can be used to build models that can personalize recommendations, predict customer behavior, and optimize advertising campaigns.\n",
    "\n",
    "Overall, machine learning has the potential to revolutionize many different industries by enabling more accurate predictions and more efficient decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwtkW33p1ATN"
   },
   "source": [
    "#Supervised Learning Techniques\n",
    "\n",
    "Supervised learning is a type of machine learning that involves learning from labeled data. In supervised learning, we start with a dataset that includes both inputs and corresponding outputs. The goal is to use this data to train a model that can take new inputs and predict the corresponding outputs.\n",
    "\n",
    "For example, let's say we want to build a model that can predict housing prices based on features such as the number of bedrooms, the square footage, and the location. We might start by collecting a dataset that includes the features of many different houses, as well as their corresponding sale prices. We would then use this data to train a supervised learning model, such as linear regression or a decision tree, that can take the features of a new house and predict its sale price.\n",
    "\n",
    "Supervised learning can be divided into two main categories:\n",
    "\n",
    "1.   Regression: This involves predicting a continuous output, such as the sale price of a house.\n",
    "2.   Classification: This involves predicting a categorical output, such as whether a customer will buy a product or not.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Q8RFOUvVQA"
   },
   "source": [
    "******************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX7PzqeFqHQv"
   },
   "source": [
    "#*Supervised Learning - Regression*\n",
    "\n",
    "\n",
    "Supervised learning regression is a machine learning technique used to predict **continuous numerical values** based on input features, such as predicting the price of a house based on its features. As mentioned earlier, in this type of learning, the algorithm learns from a labeled dataset, where each data point consists of input features and corresponding target values.\n",
    "\n",
    "The objective of supervised learning regression is to build a model that can generalize patterns in the data and make accurate predictions for unseen examples. The model learns the relationship between the input features and the target variable by fitting a mathematical function to the training data.\n",
    "\n",
    "Key concepts in supervised learning regression:\n",
    "\n",
    "1. Input Features: These are the variables or attributes that are used to predict the target variable. \n",
    "\n",
    "2. Target Variable: Also known as the dependent variable, it represents the variable that we want to predict using the input features. In regression, the target variable is continuous.\n",
    "\n",
    "3. Training Data: This is the labeled dataset used to train the regression model.\n",
    "\n",
    "4. Model Training: During the training phase, the algorithm learns the relationship between the input features and the target variable. It optimizes the model's parameters to minimize the difference between the predicted values and the actual target values.\n",
    "\n",
    "5. Prediction: After the model is trained, it can make predictions on new, unseen examples. It takes the input features and generates a continuous output value as the prediction.\n",
    "\n",
    "6. Evaluation: The performance of a regression model is evaluated using various metrics such as mean squared error (MSE), mean absolute error (MAE), or R-squared. These metrics measure the accuracy and goodness of fit of the model's predictions.\n",
    "\n",
    "Common algorithms used in supervised learning regression include linear regression, polynomial regression, support vector regression, decision tree regression, random forest regression, and neural network regression.\n",
    "\n",
    "Supervised learning regression is widely applied in various domains such as finance, economics, healthcare, and weather prediction. It can be used to predict housing prices, stock market trends, sales forecasts, patient outcomes, and many other continuous numerical variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhd7XoWXyfLI"
   },
   "source": [
    "# *Supervised Learning - Classification*\n",
    "\n",
    "Supervised learning classification is a machine learning technique used to predict categorical or discrete class labels based on input features. Similar to regression, the classification algorithm learns from a labeled dataset.\n",
    "\n",
    "The objective of supervised learning classification is to build a model that can classify or categorize new instances into predefined classes based on the patterns it has learned from the training data.\n",
    "\n",
    "Key concepts in supervised learning classification:\n",
    "\n",
    "1. Input Features: These are the variables or attributes that are used to predict the class labels. \n",
    "\n",
    "2. Class Labels: Also known as the target variable or the dependent variable, they represent the categories or classes that we want to predict using the input features. Class labels can be binary (two classes) or multi-class (more than two classes).\n",
    "\n",
    "3. Training Data: This is the labeled dataset used to train the classification model. It consists of input features and their corresponding class labels.\n",
    "\n",
    "4. Model Training: During the training phase, the algorithm learns the relationship between the input features and the class labels. It optimizes the model's parameters to minimize the classification errors and maximize the accuracy of predictions.\n",
    "\n",
    "5. Prediction: After the model is trained, it can make predictions on new, unseen examples. Given the input features, the model assigns a class label to each instance based on the patterns it has learned from the training data.\n",
    "\n",
    "6. Evaluation: The performance of a classification model is evaluated using various metrics such as accuracy, precision, recall, F1 score, and confusion matrix. These metrics measure the model's ability to correctly classify instances and assess the quality of its predictions.\n",
    "\n",
    "Common algorithms used in supervised learning classification include Logistic Regression, Decision Trees, K-Nearest Neighbors, Support Vector Machines (SVM), and Naive Bayes.\n",
    "\n",
    "Supervised learning classification finds applications in various domains such as spam email detection, sentiment analysis, image recognition, fraud detection, medical diagnosis, and many other tasks where categorical predictions are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4xLcy6109iV"
   },
   "source": [
    "##Evaluating Metrics\n",
    "\n",
    "In machine learning, performance measurements are used to evaluate the performance of a model or algorithm on a dataset. These measurements help to assess the accuracy, precision, recall, and other metrics of the model and determine how well it is performing.\n",
    "\n",
    "There are several common performance measurements used in machine learning, including:\n",
    "\n",
    "1. Accuracy: The proportion of correct predictions out of all the predictions made by the model. However, in some cases, accuracy may not be the most appropriate metric, especially when the dataset is imbalanced, i.e., when one class has much fewer observations than the others. In such cases, precision, recall, and F1 score may provide more informative evaluation metrics.\n",
    "\n",
    "2. Precision: The proportion of true positives out of all the positive predictions made by the model. Precision measures how precise the model is when predicting the positive class.\n",
    "\n",
    "3. Recall: The proportion of true positives out of all the actual positive cases in the dataset. Recall measures how well the model can identify all the positive cases in the dataset.\n",
    "\n",
    "4. F1 score: The harmonic mean of precision and recall, which combines both metrics into a single value.\n",
    "\n",
    "5. ROC curve: A graph that shows the trade-off between the true positive rate and false positive rate of the model at different classification thresholds.\n",
    "\n",
    "6. Confusion matrix: A table that summarizes the predicted and actual classes for a model, which can be used to calculate metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "These performance measurements are important for evaluating the effectiveness of machine learning models and choosing the best model for a given task. They can also be used to identify areas for improvement in the model and guide the development of new models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H01MZFTLHOI5"
   },
   "source": [
    "#Hands-On Exercises\n",
    "\n",
    "The exercises are from the textbook: [Python Machine Learning Codebook](https://www.oreilly.com/library/view/machine-learning-with/9781491989371/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diyeGVWrNOpc"
   },
   "source": [
    "##Problem 1.\n",
    "##1.1 Loading sample dataset\n",
    "We will use `scikit-learn` an open-source `scikit-learn` an open source machine learning library in Python to generate simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmY9xdqWMUi9"
   },
   "outputs": [],
   "source": [
    "# TODO: Load scikit-learn's datasets\n",
    "\n",
    "\n",
    "# TODO: Load digits dataset\n",
    "\n",
    "\n",
    "# TODO: Create features matrix\n",
    "\n",
    "\n",
    "# TODO: Create target vector\n",
    " \n",
    "\n",
    "# TODO: View first observation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7OM_nK2PjJq"
   },
   "source": [
    "Feel free to explore and load some other datasets available in the scikit-learn library.These datasets are commonly referred to as \"toy\" datasets due to their significantly smaller size and cleaner nature compared to real-world datasets. Some popular sample datasets in scikit- learn are:\n",
    "\n",
    "**load_boston**\n",
    "\n",
    "  Contains 503 observations on Boston housing prices. It is a good dataset for exploring regression algorithms.\n",
    "\n",
    "**load_iris**\n",
    "\n",
    "  Contains 150 observations on the measurements of Iris flowers. It is a good data‐ set for exploring classification algorithms.\n",
    "\n",
    "**load_digits**\n",
    "\n",
    "  Contains 1,797 observations from images of handwritten digits. It is a good data‐ set for teaching image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f__bVHSkR_8I"
   },
   "source": [
    "##1.2 Creating a simulated dataset\n",
    "\n",
    "Suppose you need to create a dataset of simulated data. `scikit-learn` offers many methods for creating simulated data. Of those, we discuss three methods that are particularly useful: `make_regression`, `make_classification`, and `make_blobs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KULrvYUCSAJh"
   },
   "source": [
    "If you are looking for a dataset specifically designed for linear regression, the `make_regression` function is a suitable option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUkokOZCSAFr"
   },
   "source": [
    "\n",
    "\n",
    "The `make_regression` function in scikit-learn has several input parameters that allow you to customize the generated dataset. Here are the main parameters:\n",
    "\n",
    "- `n_samples`: The number of samples in the dataset (default is 100).\n",
    "- `n_features`: The number of features (independent variables) in the dataset (default is 100).\n",
    "- `n_targets`: The number of target variables (dependent variables) in the dataset (default is 1).\n",
    "- `bias`: The bias term in the underlying linear model (default is 0.0).\n",
    "- `noise`: The standard deviation of the Gaussian noise added to the output variables (default is 0.0).  Higher values of noise will introduce more randomness and variability while lower values will result in a more deterministic relationship between the features and the target.\n",
    "- `coef`: If set, specifies the coefficient values of the underlying linear model. It can be either a scalar or an array-like object (default is None).\n",
    "- `random_state`: The seed used by the random number generator for reproducibility (default is None).\n",
    "- `n_informative` parameter specifies the number of informative features in the generated dataset.\n",
    "\n",
    "These parameters allow you to control the characteristics of the generated dataset, such as its size, noise level, and coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3JALUslp6uP"
   },
   "outputs": [],
   "source": [
    "# TODO: load make_regression function from sklearn.datasets library\n",
    "\n",
    "\n",
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target, coefficients = make_regression(n_samples = 100,\n",
    "                                                 n_features = 3,\n",
    "                                                 n_informative = 3,\n",
    "                                                 n_targets = 1,\n",
    "                                                 noise = 0.0,\n",
    "                                                 coef = True,\n",
    "                                                 random_state = 1)\n",
    "\n",
    "# TODO: View feature matrix and target vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyB6rG7nSAMN"
   },
   "source": [
    "If your goal is to generate a synthetic dataset for classification purposes, you can use the `make_classification` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TNxrKrxvHXX"
   },
   "outputs": [],
   "source": [
    "# TODO: Load make_classification function from sklearn.datasets library\n",
    "\n",
    "\n",
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 100,\n",
    "                                       n_features = 3,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,  #determines the number of redundant features that are generated and added to the dataset.\n",
    "                                       \n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.25, .75], #allows you to specify the class weights for the generated dataset. \n",
    "                                                              #It is used to control the balance of samples across different classes in the dataset.\n",
    "                                       \n",
    "                                       random_state = 1) #function is used to set the seed value for random number generation. \n",
    "                                                          #By providing a specific value to this parameter, you can ensure reproducibility of the generated dataset. \n",
    "                                                          #It allows you to obtain the same dataset every time you run the function with the same random_state value.\n",
    "\n",
    "# TODO: View feature matrix and target vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XB4d2ozbSAPB"
   },
   "source": [
    "And, finally make_blobs function which is useful for unsupervised clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WjPT01hy6r3"
   },
   "outputs": [],
   "source": [
    "# TODO: Load make_blob from sklearn.datasets library\n",
    "\n",
    "\n",
    "# Generate feature matrix and target vector\n",
    "features, target = make_blobs(n_samples = 100,\n",
    "                              n_features = 2,\n",
    "                              centers = 3,\n",
    "                              cluster_std = 0.5, #determines the standard deviation of each cluster. It controls the spread or dispersion of the generated blobs. \n",
    "                                                  #A higher value of cluster_std results in clusters with greater spread, while a lower value creates more compact clusters.\n",
    "                              shuffle = True,\n",
    "                              random_state = 1)\n",
    "\n",
    "# TODO: View feature matrix and target vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdHijsPE0X2K"
   },
   "source": [
    "For make_blobs, the centers parameter determines the number of clusters generated. Using the matplotlib visualization library, we can visualize the clusters generated by make_blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B95dMDzg0aEC"
   },
   "outputs": [],
   "source": [
    "# TODO: Load matpllotlib.pyplot library\n",
    "\n",
    "\n",
    "# View scatterplot\n",
    "plt.scatter(features[:,0], features[:,1], c=target)\n",
    "\n",
    "plt.xlabel('X-axis')\n",
    "plt.ylabel('Y-axis')\n",
    "plt.title('Scatter Plot')\n",
    "\n",
    "# TODO: show plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COnuA7_66eaL"
   },
   "source": [
    "See Also\n",
    "\n",
    "• [make_regression documentation](http://bit.ly/2FtIBwo)\n",
    "\n",
    "• [make_classification documentation ](http://bit.ly/2FtIKzW)\n",
    "\n",
    "• [make_blobs documentation](http://bit.ly/2FqKMAZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJR8T-NpSAR2"
   },
   "source": [
    "##1.3 Loading a `CSV` file\n",
    "\n",
    "Suppose you need to read from a `CSV` (Comma-Seperated Values) file from your local machine or hosted CSV file. We use the `read_csv` library from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrIeYP8L2zNr"
   },
   "outputs": [],
   "source": [
    "# TODO: Load pandas library\n",
    "\n",
    "\n",
    "# TODO: Load dataset\n",
    " \n",
    "\n",
    "# TODO: View first two rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjvyeY3TKnzX"
   },
   "outputs": [],
   "source": [
    "# Create URL\n",
    "url = 'https://bit.ly/3oZmLJZ-titanic-csv' \n",
    "\n",
    "# TODO: Load dataset\n",
    "\n",
    "\n",
    "# TODO: View first two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9XxDBZdSAUT"
   },
   "source": [
    "##1.4 Loading an Excel file\n",
    "\n",
    "Use read_excel library from pandas to load an Excel spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7pEkg_X13V4i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Load a csv format data \n",
    "\n",
    "dataframe = pd.read_csv(\"./data/Iris.csv\")\n",
    "\n",
    "# TODO: View the first two rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC-mq71c3rpi"
   },
   "source": [
    "Working with Excel files is similar to our solution for reading CSV files. The main difference is the additional parameter, sheetname, that specifies which sheet in the Excel file we wish to load. sheetname can accept both strings containing the name of the sheet and integers pointing to sheet positions (zero-indexed). If we need to load multiple sheets, include them as a list. For example, sheetname=[0,1,2, \"Monthly Sales\"] will return a dictionary of pandas DataFrames containing the first, second, and third sheets and the sheet named Monthly Sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWysSNogSAYf"
   },
   "source": [
    "##1.5 Loading a JSON file\n",
    "\n",
    "If you need to load a JSON file for data preprocessing, the read_json function from the pandas library helps you to convert a JSON file into a pandas object.\n",
    "See Also\n",
    "\n",
    "• [json_normalize documentation](http://bit.ly/2HQqwaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2zkl8RNe4UU-",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Load data from your local machine\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/sammie/Downloads/Iris.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TODO: View the first two rows\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Load data from your local machine\n",
    "dataframe = pd.read_json('file path', orient='columns')\n",
    "    \n",
    "# TODO: View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrmcC6AG4m-j"
   },
   "source": [
    "Importing JSON files into pandas is similar to the last few recipes we have seen. The key difference is the orient parameter, which indicates to pandas how the JSON file is structured. However, it might take some experimenting to figure out which argu‐ ment (split, records, index, columns, and values) is the right one. Another helpful tool pandas offers is json_normalize, which can help convert semistructured JSON data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOosteRD4ow2"
   },
   "source": [
    "##Problem 2\n",
    "##Data Wrangling\n",
    "Data wrangling is a broad concept that refers to the process of transforming raw data into a structured and organized format suitable for analysis. In our case, data wrangling is just one step in the data preprocessing phase, but it holds significant importance.\n",
    "\n",
    "The primary tool commonly used for data wrangling is the data frame, which is a versatile and intuitive data structure. Data frames are tabular in nature, resembling rows and columns similar to a spreadsheet. They provide a convenient way to organize and manipulate data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdbkiaFS-2yd"
   },
   "source": [
    "Let's create a dataframe from titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5TFTxXtAP_z"
   },
   "outputs": [],
   "source": [
    "# TODO: Load pandas library\n",
    "\n",
    "\n",
    "# TODO: Create a URL for titanic dataset\n",
    "\n",
    "\n",
    "# TODO: Load data as a dataframe\n",
    "\n",
    "\n",
    "# TODO: Show first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avWIsLeOBgLD"
   },
   "source": [
    "##2.1 Creating a Data Frame form scratch\n",
    "Pandas offers several methods for creating a new DataFrame object. One straightforward approach is to create an empty DataFrame using the DataFrame constructor and then define each column individually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs_welXjBwPO"
   },
   "outputs": [],
   "source": [
    "# TODO: Create an empty Dataframe\n",
    "\n",
    "\n",
    "# TODO: Add three columns: Name, Age, and Driver. \n",
    "# The names to be added are: Jack Jackson, Steven Stevenson\n",
    "# coresponding age: 38 and 25\n",
    "# driver status: True and False\n",
    "\n",
    "\n",
    "# TODO: Show dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa95hKewDEZQ"
   },
   "source": [
    "Alternatively, after creating a DataFrame object, we have the option to add new rows to the bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYTCPu12DHW-"
   },
   "outputs": [],
   "source": [
    "# Create row\n",
    "new_person = pd.Series(['Molly Mooney', 40, True], index=['Name','Age','Driver'])\n",
    "\n",
    "# TODO: Append row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "994NHOuIDnHf"
   },
   "source": [
    "##2.2 Describing the Data\n",
    "\n",
    "View the first rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75u8nTmEDxhu"
   },
   "outputs": [],
   "source": [
    "# TODO: Load data\n",
    "\n",
    "\n",
    "# TODO: shows fisrt two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoZ9vJHaD_tN"
   },
   "source": [
    "View number of the rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFypAfM3EHQI"
   },
   "outputs": [],
   "source": [
    "# TODO: show dimensionss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CivqpKiErup"
   },
   "outputs": [],
   "source": [
    "# TODO: Show statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLgcN091FfF5"
   },
   "source": [
    "##2.3 Navigating Dataframes\n",
    "\n",
    "Suppose you need to select individual data or slices of the Dataframe. \n",
    "\n",
    "In this case, use `loc` or `iloc` to select one or more rows or values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "my_S142hGDpM"
   },
   "outputs": [],
   "source": [
    "# TODO: Select first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9PmhtXjGUnm"
   },
   "outputs": [],
   "source": [
    "# TODO: select three rows: 2nd to 4th\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66PAaUFBJm2S"
   },
   "source": [
    "##2.4 Selecting Rows Based on Conditionals\n",
    "\n",
    "Selecting and filtering data based on specific conditions is a frequent task in data wrangling. Rather than working with the entire raw dataset, we often focus on extracting a specific subset of the data that meets our criteria or requirements. For example, suppose we want to select all women on the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_hnzEVxHvtX"
   },
   "outputs": [],
   "source": [
    "# TODO: Load data\n",
    "\n",
    "\n",
    "# TODO: Show top two rows where column 'sex' is 'female'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHIBBFEbMg6p"
   },
   "source": [
    "Select all the rows where the passenger is a female 65 or older:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j20bx3lXMi2Q"
   },
   "outputs": [],
   "source": [
    "dataframe[(dataframe['Sex'] == 'female') & (dataframe['Age'] >= 65)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e0nwXPFNMTi"
   },
   "source": [
    "##2.5 Repalcing values\n",
    "If you want to replace a value in Dataframe use the `replace` function from pandas to find and replace values. For example, you can replace any instance of \"Female\" in the Sec column with \"Woman\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9Okpvb2OFvX"
   },
   "outputs": [],
   "source": [
    "# TODO: Load data\n",
    "dataframe = pd.read_csv(url)\n",
    "\n",
    "# TODO: Replace values, show two rows\n",
    "dataframe['Sex'].replace(\"female\", \"Woman\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoKwtBCiMF4x"
   },
   "source": [
    "We can also replace multiple values at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saGprl4hSQoL"
   },
   "outputs": [],
   "source": [
    "# TODO: Replace \"female\" and \"male with \"Woman\" and \"Man\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw8OkLX_SPIm"
   },
   "source": [
    "We can also find and replace across the entire DataFrame object by specifying the whole data frame instead of a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cscNNwRQSZjO"
   },
   "outputs": [],
   "source": [
    "# TODO: Replace values, show two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DApUGI_ASwry"
   },
   "source": [
    "`replace` also accepts regular expressions. \n",
    "\n",
    "Regular expressions can be used to match and extract specific patterns or sequences of characters within text data. They are widely used in programming, data processing, and text analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVaxgGn7Snox"
   },
   "outputs": [],
   "source": [
    "# TODO: Replace values, show two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp_j8mgwUlI_"
   },
   "source": [
    "##2.6 Finding the Minimum, Maximum, Sum, Average, and Count\n",
    "\n",
    "pandas comes with some built-in methods for commonly used descriptive statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHdD0z_HUvwj"
   },
   "outputs": [],
   "source": [
    "# TODO: Load data\n",
    "\n",
    "\n",
    "# Calculate statistics\n",
    "# TODO: print the max of the age\n",
    "\n",
    "\n",
    "# TODO: print the min of the age\n",
    "\n",
    "\n",
    "# TODO: print the mean of age\n",
    "\n",
    "\n",
    "# TODO: print the sum of ages\n",
    "\n",
    "\n",
    "# TODO: print the number of age items\n",
    "\n",
    "\n",
    "# TODO: Show Dataframe counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgJyZ9q0WG4t"
   },
   "source": [
    "## 2.7 Handling Missing Values\n",
    "\n",
    "`isnull` and `notnull` return booleans indicating whether a value is missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f3MTSeYWiA6"
   },
   "outputs": [],
   "source": [
    "#TODO: select missing values, using isnull(), show two rows\n",
    "dataframe[dataframe['Age'].isnull()].head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbpvC72qY-d1"
   },
   "source": [
    "Dealing with missing values is a common challenge in data wrangling, but it can be more complex than anticipated. In pandas, missing values are represented using NumPy's NaN (\"Not A Number\") value. However, it's important to note that NaN is not fully integrated natively in pandas. For instance, if we attempt to replace all strings containing \"male\" with missing values, it will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaMZNxf2Y_vX"
   },
   "outputs": [],
   "source": [
    "# Attempt to replace values with NaN\n",
    "dataframe['Sex'] = dataframe['Sex'].replace('male', NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUl3c3_PZJcT"
   },
   "source": [
    "To have full functionality with `NaN` we need to import the `NumPy` library first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NH3bzK7TZSAd"
   },
   "outputs": [],
   "source": [
    "#TODO: Load numpy library\n",
    "\n",
    "\n",
    "#TODO: Replace values with NaN\n",
    "\n",
    "\n",
    "# TODO:show dataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQX0H814Z9lg"
   },
   "source": [
    "##2.8 Deleting columns and rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BzKE59jck8U"
   },
   "source": [
    "To delete a column, the most effective approach is to use the \"`drop`\" function with the parameter \"`axis=1`\" (referring to the column axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AftZvMJZcb2e"
   },
   "outputs": [],
   "source": [
    "# TODO: Delete Age column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgt__1Ykc5-v"
   },
   "source": [
    "You can also use a list of column names as the main argument to drop multiple col‐ umns at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkSZMhric6zk"
   },
   "outputs": [],
   "source": [
    "#TODO: Drop Age and Sex columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_icAaURpdC68"
   },
   "source": [
    "If a column does not have a name (which can sometimes happen), you can drop it by its column index using dataframe.columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JKtwcd8dDwx"
   },
   "outputs": [],
   "source": [
    "#TODO:  Drop 1st column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPXaOhUcdqOD"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "\n",
    "The recommended way to delete a column is by using the \"drop\" method. Another approach is to use `del dataframe['Age']`, which is sometimes effective but not recommended due to how it is implemented in pandas which is beyond the scope of this course.\n",
    "\n",
    "One suggestion provided by the [book's](https://www.oreilly.com/library/view/machine-learning-with/9781491989371/) author is to avoid using the \"inplace=True\" argument in pandas. Several pandas methods have an \"inplace\" parameter that, when set to True, modifies the DataFrame directly. However, this approach can create issues in complex data processing pipelines because it treats DataFrames as mutable objects, even though they are technically mutable. It is advisable to treat DataFrames as immutable objects. For example:\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hroZvVqzeCzK"
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame\n",
    "dataframe_name_dropped = dataframe.drop(dataframe.columns[0], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFvK4u3_eGwF"
   },
   "source": [
    "In the above example, we are not modifying the original DataFrame \"dataframe\" directly. Instead, we are creating a new DataFrame called \"dataframe_name_dropped,\" which is a modified version of the original dataframe. By treating DataFrames as immutable objects and avoiding direct modifications, you can prevent potential complications and avoid future difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojqqYC5Vf0dc"
   },
   "source": [
    "###Deleting a row\n",
    "\n",
    "To delete one or more rows from a DataFrame, Use a boolean condition to create a new DataFrame excluding the rows you want to delete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxqe6eubaZ8o"
   },
   "outputs": [],
   "source": [
    "#TODO: Load data\n",
    "\n",
    "\n",
    "#TODO: Delete rows, show first two rows of output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHaw8taccG_Q"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "Technically, you can use the `drop` method (e.g., `df.drop([0, 1], axis=0`) to drop the first two rows), but a more practical approach is to use a boolean condition within `df[]`. This method allows us to leverage the power of conditionals to delete either a single row or multiple rows at once, which is often more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ExPvbJjhMQL"
   },
   "source": [
    "##2.9 Looping Over a Column\n",
    "\n",
    "If you want to iterate over every element in a column and apply some action, you can treat a pandas column like any other sequence in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1bVIaXohd0x"
   },
   "outputs": [],
   "source": [
    "#TODO: Load data\n",
    "\n",
    "\n",
    "#TODO: Print first two names uppercased using for loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4X9HZ_NiH6e"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "In addition to loops (often called for loops), we can also use list comprehensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djuj5--fiKjK"
   },
   "outputs": [],
   "source": [
    "#TODO: Show first two names lowercased using list comprehension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y59bP4E_iXXU"
   },
   "source": [
    "##2.10 Applying a function over all elements in a column\n",
    "If you want to apply some function over all elements in a column, use `apply` to apply a built-in or custom function on every element in a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23uSRLcsiukH"
   },
   "outputs": [],
   "source": [
    "#TODO: Load data\n",
    "\n",
    "\n",
    "#TODO: Create function \n",
    "\n",
    "\n",
    "#TODO: Apply function, show two rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oi-RWUOjAx2"
   },
   "source": [
    "##2.11 Concatenating DataFrames\n",
    "If you want to concatenate two DataFrames, use `concat` with `axis=0` to concatenate along the rows axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fi0RrdjDjd8u"
   },
   "outputs": [],
   "source": [
    "# create DataFrame\n",
    "data_a = {'id': ['1', '2', '3'],\n",
    "          'first': ['Alex', 'Amy', 'Allen'],\n",
    "          'last': ['Anderson', 'Ackerman', 'Ali']}\n",
    "\n",
    "dataframe_a = pd.DataFrame(data_a, columns = ['id', 'first', 'last'])\n",
    "\n",
    "# Create DataFrame\n",
    "data_b = {'id': ['4', '5', '6'],\n",
    "          'first': ['Billy', 'Brian', 'Bran'],\n",
    "          'last': ['Bonder', 'Black', 'Balwner']}\n",
    "\n",
    "dataframe_b = pd.DataFrame(data_b, columns = ['id', 'first', 'last'])\n",
    "\n",
    "#TODO: Concatenate DataFrames by rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opqSAm8nibRK"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "Concatenating is a term commonly used in computer science and programming to describe the act of joining two objects together. In simpler terms, it means to combine or merge two objects. In the provided solution, we merged two smaller DataFrames by specifying the axis parameter, which determines whether the DataFrames are stacked vertically (on top of each other) or horizontally (side by side).\n",
    "\n",
    "Alternatively we can use append to add a new row to a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSuk2m3llFoB"
   },
   "outputs": [],
   "source": [
    "# Create row\n",
    "row = pd.Series([10, 'Chris', 'Chillon'], index=['id', 'first', 'last']) \n",
    "\n",
    "#TODO: Append row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sev7YVOqlVyM"
   },
   "source": [
    "##2.12 Merging DataFrames\n",
    "\n",
    "If you want to merge two DataFrames, to inner join, use `merge` with `on` parameter to specify the column to merge on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpvrMQsilyjG"
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "employee_data = {'employee_id': ['1', '2', '3', '4'],\n",
    "                     'name': ['Amy Jones', 'Allen Keys', 'Alice Bees',\n",
    "                     'Tim Horton']}\n",
    "\n",
    "dataframe_employees = pd.DataFrame(employee_data, columns = ['employee_id',\n",
    "                                                             'name'])\n",
    "\n",
    "dataframe_employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD7SPp-LnPA3"
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "sales_data = {'employee_id': ['3', '4', '5', '6'],\n",
    "              'total_sales': [23456, 2512, 2345, 1455]}\n",
    "\n",
    "dataframe_sales = pd.DataFrame(sales_data, columns = ['employee_id',\n",
    "                                                    'total_sales'])\n",
    "\n",
    "dataframe_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adUrxAr4nHKN"
   },
   "outputs": [],
   "source": [
    "#TODO: Merge DataFrames 'inner'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t685uB4KmfjC"
   },
   "source": [
    "By default, the merge function performs inner joins. However, if we want to perform an outer join, we can specify it using the how parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiaTQpibmgzJ"
   },
   "outputs": [],
   "source": [
    "#TODO: Merge DataFrames 'outer'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB61ICW1mwv1"
   },
   "source": [
    "Left or right join:\n",
    "\n",
    "The same parameter can be used to specify left and right joins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32Qtnmvim2xp"
   },
   "outputs": [],
   "source": [
    "#TODO: Merge DataFrames 'left'\n",
    "pd.merge(dataframe_employees, dataframe_sales, on='employee_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDQx8oAUnm7P"
   },
   "outputs": [],
   "source": [
    "#TODO: Merge DataFrames 'right'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jad6zX_6oLQA"
   },
   "source": [
    "You can also specify the column name in each DataFrame to merge on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "tPOe65OkoNQi",
    "outputId": "231cfa52-76b3-48df-86ab-f7c95f9be959"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-3b2519fd-5876-4e7e-a582-e797e81425bc\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>employee_id</th>\n",
       "      <th>name</th>\n",
       "      <th>total_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Alice Bees</td>\n",
       "      <td>23456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Tim Horton</td>\n",
       "      <td>2512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3b2519fd-5876-4e7e-a582-e797e81425bc')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-3b2519fd-5876-4e7e-a582-e797e81425bc button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-3b2519fd-5876-4e7e-a582-e797e81425bc');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "  employee_id        name  total_sales\n",
       "0           3  Alice Bees        23456\n",
       "1           4  Tim Horton         2512"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge DataFrames\n",
    "pd.merge(dataframe_employees,\n",
    "             dataframe_sales,\n",
    "             left_on='employee_id',\n",
    "             right_on='employee_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f6LxgdGolcY"
   },
   "source": [
    "**Discussion**\n",
    "\n",
    "Frequently, the data we work with is complex and not available as a single entity. Instead, we often encounter various datasets from different sources, such as multiple database queries or files. To consolidate all the data into a unified structure, we can load each query or file into separate DataFrames in pandas and then merge them together to create a single DataFrame.\n",
    "\n",
    "To perform a merge operation, there are three components that need to be specified. \n",
    "\n",
    "Firstly, we need to identify the two DataFrames that we want to merge together. In above exrecises, they were assigned the names \"dataframe_employees\" and \"dataframe_sales\". \n",
    "\n",
    "Secondly, we need to specify the column(s) on which the merge will be based. These columns contain values that are shared between the two DataFrames. Both DataFrames have a column called \"employee_id\". The merge operation will pair up the values in the \"employee_id\" column of each DataFrame. If the column names are the same, the \"`on`\" parameter can be used. However, if the column names differ, we can use \"left_on\" and \"right_on\" to specify the corresponding column names from the left and right DataFrames.\n",
    "\n",
    "What do we mean by the \"left\" and \"right\" DataFrames? In simple terms, the \"left\" DataFrame refers to the first DataFrame that we mention in the merge operation, while the \"right\" DataFrame is the second one. This terminology becomes relevant again when discussing the subsequent parameters.\n",
    "The final aspect, which can be a bit challenging for some, is determining the type of merge operation we want to perform. This is indicated by the \"how\" parameter. The merge function supports the four primary types of joins:\n",
    "\n",
    "Inner\n",
    "\n",
    "Return only the rows that match in both DataFrames (e.g., return any row with an employee_id value appearing in both dataframe_employees and data frame_sales).\n",
    "\n",
    "Outer\n",
    "\n",
    "Return all rows in both DataFrames. If a row exists in one DataFrame but not in the other DataFrame, fill NaN values for the missing values (e.g., return all rows in both employee_id and dataframe_sales).\n",
    "\n",
    "Left\n",
    "\n",
    "Return all rows from the left DataFrame but only rows from the right DataFrame that matched with the left DataFrame. Fill NaN values for the missing values (e.g., return all rows from dataframe_employees but only rows from data frame_sales that have a value for employee_id that appears in data frame_employees).\n",
    "\n",
    "Right\n",
    "\n",
    "Return all rows from the right DataFrame but only rows from the left DataFrame that matched with the right DataFrame. Fill NaN values for the missing values (e.g., return all rows from dataframe_sales but only rows from data frame_employees that have a value for employee_id that appears in data frame_sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G3AniC4EqCaR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCamber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
