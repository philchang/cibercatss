{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3baf5dbd-7118-41b4-b5d4-8f51d616a44d",
   "metadata": {},
   "source": [
    "# Binary Classification on Breast Cancer Dataset\n",
    "\n",
    "In this notebook, we will practice binary classification on the Breast Cancer dataset, which is widely used for testing classification algorithms. Our goal is to predict whether a tumor is malignant or benign based on various features.\n",
    "\n",
    "### Dataset\n",
    "The Breast Cancer dataset is available through `sklearn.datasets`. Each data point represents a tumor, with features such as radius, texture, smoothness, and other measurements. The target variable indicates whether the tumor is malignant (1) or benign (0).\n",
    "\n",
    "Let's start by loading the dataset and exploring it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99620052-7044-4eaa-82bf-f4251d2f3878",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b17c1e9-9f95-4c7c-a245-9a8c5839d065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Convert to DataFrame for easier exploration\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# TODO: Display the first few rows of the DataFrame\n",
    "# Hint: Use the .head() method\n",
    "df.head()\n",
    "\n",
    "# TODO: Check for missing values and data types\n",
    "# Hint: Use .info() on the DataFrame\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c7f19ec-94c5-4acd-84aa-c73b3e4d6709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       worst fractal dimension      target  \n",
       "count               569.000000  569.000000  \n",
       "mean                  0.083946    0.627417  \n",
       "std                   0.018061    0.483918  \n",
       "min                   0.055040    0.000000  \n",
       "25%                   0.071460    0.000000  \n",
       "50%                   0.080040    1.000000  \n",
       "75%                   0.092080    1.000000  \n",
       "max                   0.207500    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7282b6b-a209-4c4d-ad93-c0afbc46be79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean radius                0\n",
       "mean texture               0\n",
       "mean perimeter             0\n",
       "mean area                  0\n",
       "mean smoothness            0\n",
       "mean compactness           0\n",
       "mean concavity             0\n",
       "mean concave points        0\n",
       "mean symmetry              0\n",
       "mean fractal dimension     0\n",
       "radius error               0\n",
       "texture error              0\n",
       "perimeter error            0\n",
       "area error                 0\n",
       "smoothness error           0\n",
       "compactness error          0\n",
       "concavity error            0\n",
       "concave points error       0\n",
       "symmetry error             0\n",
       "fractal dimension error    0\n",
       "worst radius               0\n",
       "worst texture              0\n",
       "worst perimeter            0\n",
       "worst area                 0\n",
       "worst smoothness           0\n",
       "worst compactness          0\n",
       "worst concavity            0\n",
       "worst concave points       0\n",
       "worst symmetry             0\n",
       "worst fractal dimension    0\n",
       "target                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Check for missing values\n",
    "# Hint: Use .isnull().sum() to verify if there are any missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a6179-bd02-4e45-8c04-63e8f6fcb0db",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2758845-3dcc-47fd-80b0-d54cd3a5b426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Since there are no missing values, we can proceed to feature scaling.\n",
    "\n",
    "# Standardize the feature values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO: Initialize the scaler and transform features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# TODO: Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 12)\n",
    "\n",
    "# TODO: Fit and transform features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9b3a2-156c-4f08-a2a4-8f456a80857a",
   "metadata": {},
   "source": [
    "## Implementing Classification Algorithms\n",
    "\n",
    "We'll use cross-validation to evaluate each model. For cross-validation, we will use the cross_val_score function with 5 folds.\n",
    "\n",
    "Using cross-validation will give us more insight into the stability and robustness of each model by evaluating them on multiple subsets of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab4c49-5153-4f0a-aeb9-b48ba2338fd6",
   "metadata": {},
   "source": [
    "### 1 - Logistic Regression with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9dcb22e-0c6b-4802-83a2-a707ea77102f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross-Validation Scores: [0.97674419 0.98823529 0.96470588 0.98823529 0.97647059]\n",
      "Average Cross-Validation Score: 0.9788782489740082\n",
      "Logistic Regression Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        53\n",
      "           1       0.97      0.98      0.97        90\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.96      0.97      0.96       143\n",
      "\n",
      "[[50  3]\n",
      " [ 2 88]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import necessary classes for Logistic Regression, evaluation, and cross-validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# TODO: Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# TODO: Perform cross-validation on Logistic Regression model\n",
    "log_reg_cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "# Print cross-validation scores and average accuracy\n",
    "print(\"Logistic Regression Cross-Validation Scores:\", log_reg_cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", log_reg_cv_scores.mean())\n",
    "\n",
    "# Train and evaluate Logistic Regression on the test set\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Print Logistic Regression performance metrics\n",
    "print(\"Logistic Regression Performance on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "print(confusion_matrix(y_test, y_pred_log))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a61766-7f61-4d4e-ae9c-4de946fc7f6a",
   "metadata": {},
   "source": [
    "## 2 - Support Vector Machine (SVM) with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b8192e-7ab0-4899-9ba6-ba9c02b76d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Cross-Validation Scores: [0.98837209 0.98823529 0.97647059 0.98823529 0.97647059]\n",
      "Average Cross-Validation Score: 0.9835567715458277\n",
      "SVM Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        53\n",
      "           1       0.97      0.97      0.97        90\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n",
      "[[50  3]\n",
      " [ 3 87]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import SVC from sklearn.svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: Initialize the SVM model with kernel='linear'\n",
    "svm = SVC(kernel= 'linear')\n",
    "\n",
    "# TODO: Perform cross-validation on the SVM model\n",
    "svm_cv_scores = cross_val_score(svm, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "# Print cross-validation scores and average accuracy\n",
    "print(\"SVM Cross-Validation Scores:\", svm_cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", svm_cv_scores.mean())\n",
    "\n",
    "# Train and evaluate SVM on the test set\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm.predict(X_test_scaled)\n",
    "\n",
    "# Print SVM performance metrics\n",
    "print(\"SVM Performance on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(confusion_matrix(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bff85-28c1-429e-af37-60c2ed4973dd",
   "metadata": {},
   "source": [
    "## 3 - K-Nearest Neighbors (KNN) with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90dc99dc-7cd6-4b52-8c6b-fb60eefb389f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross-Validation Scores: [0.96511628 0.97647059 0.96470588 0.98823529 0.96470588]\n",
      "Average Cross-Validation Score: 0.9718467852257182\n",
      "KNN Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92        53\n",
      "           1       0.95      0.97      0.96        90\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.94      0.94       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n",
      "[[48  5]\n",
      " [ 3 87]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# TODO: Initialize the KNN model with n_neighbors=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# TODO: Perform cross-validation on the KNN model\n",
    "knn_cv_scores = cross_val_score(knn, X_train_scaled, y_train, cv =5)\n",
    "\n",
    "# Print cross-validation scores and average accuracy\n",
    "print(\"KNN Cross-Validation Scores:\", knn_cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", knn_cv_scores.mean())\n",
    "\n",
    "# Train and evaluate KNN on the test set\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn.predict(X_test_scaled)\n",
    "\n",
    "# Print KNN performance metrics\n",
    "print(\"KNN Performance on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe4db42-6a13-4c91-b523-f86767869314",
   "metadata": {},
   "source": [
    "## 4 - Decision Tree with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6493a40d-5206-43da-a382-9ee87bceb9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross-Validation Scores: [0.93023256 0.95294118 0.96470588 0.96470588 0.91764706]\n",
      "Average Cross-Validation Score: 0.9460465116279071\n",
      "Decision Tree Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87        53\n",
      "           1       0.91      0.94      0.93        90\n",
      "\n",
      "    accuracy                           0.91       143\n",
      "   macro avg       0.91      0.90      0.90       143\n",
      "weighted avg       0.91      0.91      0.91       143\n",
      "\n",
      "[[45  8]\n",
      " [ 5 85]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import DecisionTreeClassifier from sklearn.tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Initialize the Decision Tree model\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Perform cross-validation on the Decision Tree model\n",
    "dt_cv_scores = cross_val_score(dt, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "# Print cross-validation scores and average accuracy\n",
    "print(\"Decision Tree Cross-Validation Scores:\", dt_cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", dt_cv_scores.mean())\n",
    "\n",
    "# Train and evaluate Decision Tree on the test set\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt.predict(X_test_scaled)\n",
    "\n",
    "# Print Decision Tree performance metrics\n",
    "print(\"Decision Tree Performance on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(confusion_matrix(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d3ed1-7771-405e-aa20-fb55be26bbfd",
   "metadata": {},
   "source": [
    "## 5 - Random Forest with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e008ee1b-06ed-4d6f-8f1f-c09818f0bc9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Cross-Validation Scores: [0.94186047 0.97647059 0.97647059 0.98823529 0.96470588]\n",
      "Average Cross-Validation Score: 0.9695485636114911\n",
      "Random Forest Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89        53\n",
      "           1       0.94      0.92      0.93        90\n",
      "\n",
      "    accuracy                           0.92       143\n",
      "   macro avg       0.91      0.91      0.91       143\n",
      "weighted avg       0.92      0.92      0.92       143\n",
      "\n",
      "[[48  5]\n",
      " [ 7 83]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import RandomForestClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# TODO: Initialize the Random Forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# TODO: Perform cross-validation on the Random Forest model\n",
    "rf_cv_scores = cross_val_score(rf, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "# Print cross-validation scores and average accuracy\n",
    "print(\"Random Forest Cross-Validation Scores:\", rf_cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", rf_cv_scores.mean())\n",
    "\n",
    "# Train and evaluate Random Forest on the test set\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "\n",
    "# Print Random Forest performance metrics\n",
    "print(\"Random Forest Performance on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff67a20-9d6e-4908-842a-3ae3aa4707ef",
   "metadata": {},
   "source": [
    "## 6 - Gradient Boosting with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f900613-8fb5-4915-94a9-faf05c171ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Cross-Validation Scores: [0.95348837 0.97647059 0.97647059 0.98823529 0.92941176]\n",
      "Average Cross-Validation Score: 0.9648153214774282\n",
      "Gradient Boosting Performance on Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        53\n",
      "           1       0.94      0.94      0.94        90\n",
      "\n",
      "    accuracy                           0.93       143\n",
      "   macro avg       0.93      0.93      0.93       143\n",
      "weighted avg       0.93      0.93      0.93       143\n",
      "\n",
      "[[48  5]\n",
      " [ 5 85]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import GradientBoostingClassifier from sklearn.ensemble\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# TODO: Initialize the Gradient Boosting model\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "# TODO: Perform cross-validation on the Gradient Boosting model\n",
    "gb_cv_scores = cross_val_score(gb, X_train_scaled, y_train, cv = 5)\n",
    "\n",
    "# Print cross-validation scores and average accuracy\n",
    "print(\"Gradient Boosting Cross-Validation Scores:\", gb_cv_scores)\n",
    "print(\"Average Cross-Validation Score:\", gb_cv_scores.mean())\n",
    "\n",
    "# Train and evaluate Gradient Boosting on the test set\n",
    "gb.fit(X_train_scaled, y_train)\n",
    "y_pred_gb = gb.predict(X_test_scaled)\n",
    "\n",
    "# Print Gradient Boosting performance metrics\n",
    "print(\"Gradient Boosting Performance on Test Set:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(confusion_matrix(y_test, y_pred_gb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd20dc71-1cdb-45b2-9696-574ac0461841",
   "metadata": {},
   "source": [
    "\n",
    "## Model Comparison and Conclusion\n",
    "\n",
    "Now that we have explored six classification algorithms, let’s compare their performance using the cross-validation scores as well as the accuracy, precision, recall, and F1-score on the test set.\n",
    "\n",
    "Consider the stability and reliability of each model based on their cross-validation scores. Reflect on which models might be most suitable for predicting tumor malignancy and why, based on your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc55a153-bf6b-4623-8e1f-82d4f36f76e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Avg. Cross-Validation Score</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.978878</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.967033</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.972376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.983557</td>\n",
       "      <td>0.958042</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.971847</td>\n",
       "      <td>0.944056</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.956044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.946047</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.913978</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.928962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.969549</td>\n",
       "      <td>0.916084</td>\n",
       "      <td>0.943182</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>0.932584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.964815</td>\n",
       "      <td>0.930070</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Avg. Cross-Validation Score  Test Accuracy  Precision  \\\n",
       "0  Logistic Regression                     0.978878       0.965035   0.967033   \n",
       "1                  SVM                     0.983557       0.958042   0.966667   \n",
       "2                  KNN                     0.971847       0.944056   0.945652   \n",
       "3        Decision Tree                     0.946047       0.909091   0.913978   \n",
       "4        Random Forest                     0.969549       0.916084   0.943182   \n",
       "5    Gradient Boosting                     0.964815       0.930070   0.944444   \n",
       "\n",
       "     Recall  F1-Score  \n",
       "0  0.977778  0.972376  \n",
       "1  0.966667  0.966667  \n",
       "2  0.966667  0.956044  \n",
       "3  0.944444  0.928962  \n",
       "4  0.922222  0.932584  \n",
       "5  0.944444  0.944444  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary library for DataFrame\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Create a dictionary to hold the model names and their performance metrics\n",
    "model_results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"SVM\", \"KNN\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"],\n",
    "    \"Avg. Cross-Validation Score\": [\n",
    "        log_reg_cv_scores.mean(), \n",
    "        svm_cv_scores.mean(), \n",
    "        knn_cv_scores.mean(), \n",
    "        dt_cv_scores.mean(), \n",
    "        rf_cv_scores.mean(), \n",
    "        gb_cv_scores.mean()\n",
    "    ],\n",
    "\"Test Accuracy\": [\n",
    "    accuracy_score(y_test, y_pred_log),\n",
    "    accuracy_score(y_test, y_pred_svm),\n",
    "    accuracy_score(y_test, y_pred_knn),\n",
    "    accuracy_score(y_test, y_pred_dt),\n",
    "    accuracy_score(y_test, y_pred_rf),\n",
    "    accuracy_score(y_test, y_pred_gb)\n",
    "],\n",
    "    \"Precision\": [\n",
    "        classification_report(y_test, y_pred_log, output_dict=True)['1']['precision'],\n",
    "        classification_report(y_test, y_pred_svm, output_dict=True)['1']['precision'],\n",
    "        classification_report(y_test, y_pred_knn, output_dict=True)['1']['precision'],\n",
    "        classification_report(y_test, y_pred_dt, output_dict=True)['1']['precision'],\n",
    "        classification_report(y_test, y_pred_rf, output_dict=True)['1']['precision'],\n",
    "        classification_report(y_test, y_pred_gb, output_dict=True)['1']['precision']\n",
    "    ],\n",
    "    \"Recall\": [\n",
    "        classification_report(y_test, y_pred_log, output_dict=True)['1']['recall'],\n",
    "        classification_report(y_test, y_pred_svm, output_dict=True)['1']['recall'],\n",
    "        classification_report(y_test, y_pred_knn, output_dict=True)['1']['recall'],\n",
    "        classification_report(y_test, y_pred_dt, output_dict=True)['1']['recall'],\n",
    "        classification_report(y_test, y_pred_rf, output_dict=True)['1']['recall'],\n",
    "        classification_report(y_test, y_pred_gb, output_dict=True)['1']['recall']\n",
    "    ],\n",
    "    \"F1-Score\": [\n",
    "        classification_report(y_test, y_pred_log, output_dict=True)['1']['f1-score'],\n",
    "        classification_report(y_test, y_pred_svm, output_dict=True)['1']['f1-score'],\n",
    "        classification_report(y_test, y_pred_knn, output_dict=True)['1']['f1-score'],\n",
    "        classification_report(y_test, y_pred_dt, output_dict=True)['1']['f1-score'],\n",
    "        classification_report(y_test, y_pred_rf, output_dict=True)['1']['f1-score'],\n",
    "        classification_report(y_test, y_pred_gb, output_dict=True)['1']['f1-score']\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame for easy visualization\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "# Display the table\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f8b84-9c46-4e49-8d8a-e6d5a6e9c265",
   "metadata": {},
   "source": [
    "### Are the Results Robust?\n",
    "\n",
    "Consistency: If both cross-validation and test scores are high, this generally suggests a robust model. Consistency between cross-validation scores and test set performance indicates the model is not overfitting or underfitting, which is a positive sign for robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3e163-578d-4c07-90b9-3fb7759bb787",
   "metadata": {},
   "source": [
    "In medical contexts, a slight preference might be given to models with higher recall (measures how well a model finds all the true positive cases.), even if it means a slight trade-off in precision. Missing a malignant tumor is often more serious than incorrectly identifying a benign tumor as malignant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
