{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c319af5-350c-4fa0-800c-78731650bbd4",
   "metadata": {},
   "source": [
    "## Project: Fashion MNIST Classification with CNN\n",
    "### Project Description\n",
    "\n",
    "In this project, you will build Convolutional Neural Networks (CNNs) to classify images of clothing from the Fashion MNIST dataset. The Fashion MNIST dataset consists of 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28x28 pixels):\n",
    "\n",
    "60,000 training images\n",
    "10,000 test images\n",
    "Each image is labeled with one of the following classes:\n",
    "\n",
    "T-shirt/top\n",
    "Trouser\n",
    "Pullover\n",
    "Dress\n",
    "Coat\n",
    "Sandal\n",
    "Shirt\n",
    "Sneaker\n",
    "Bag\n",
    "Ankle boot\n",
    "Project Goal\n",
    "\n",
    "The goal of this project is to build and train three different CNN models that can accurately classify images of clothing into their respective categories. Your models will take as input a 28x28 pixel grayscale image and predict which one of the 10 categories the image belongs to. After training, you will compare and evaluate the performance of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abfe1d-33cc-4f0e-b8d8-265f962de992",
   "metadata": {},
   "source": [
    "### Instructions and Starter Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2a338-cc5a-4101-8e15-c9abad5411e7",
   "metadata": {},
   "source": [
    "1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93b8a7-3e6e-4a28-ba7f-3a9e8bfb84a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary librarieimport tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1534ae9-aa05-48d0-83dd-c17ab12099d1",
   "metadata": {},
   "source": [
    "2. Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5581f8-e8b5-44be-a6bc-fe0497d6f09f",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Load the Fashion MNIST dataset using `tf.keras.datasets.fashion_mnist.load_data()`.\n",
    "2. Normalize the pixel values of the training and test images to be between 0 and 1.\n",
    "3. Display the first 25 images from the training set along with their labels to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b75819-a322-411b-ad8b-14ac63384dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Fashion MNIST dataset\n",
    "fashion_mnist = datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Display the first 25 images from the training set to understand the data\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24d4a10-9d61-4cff-93e3-d25e436cc374",
   "metadata": {},
   "source": [
    "3. Build and Train Three Different CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c89d0b-2849-4d1c-a487-1d6c9879f7aa",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Build three different CNN models using `tf.keras.Sequential()` and layers such as `Conv2D`, `MaxPooling2D`, `Flatten`, and `Dense`.\n",
    "2. Compile each model using the 'adam' optimizer and `SparseCategoricalCrossentropy` loss.\n",
    "3. Train each model for 10 epochs using the training data and validate it on the test data.\n",
    "4. Save the training history of each model for later comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134f2b4-8f7c-44dd-9d91-82a590baf281",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Build three different CNN models using `tf.keras.Sequential()` and layers such as `Conv2D`, `MaxPooling2D`, `Flatten`, and `Dense`.\n",
    "2. Compile each model using the 'adam' optimizer and `SparseCategoricalCrossentropy` loss.\n",
    "3. Train each model for 10 epochs using the training data and validate it on the test data.\n",
    "4. Save the training history of each model for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93bf59a-5e8f-444a-8a13-d042476c7a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model 1\n",
    "model_1 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebbcc9-63b4-4509-b520-ca9ca8ca0e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model 2\n",
    "model_2 = models.Sequential([\n",
    "    layers.Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (5, 5), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff23c95-9a90-4d14-aee3-9c6fa5d05b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model 3\n",
    "model_3 = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ad325-12d8-4441-9cea-46e1c9c2ddb0",
   "metadata": {},
   "source": [
    " find check points to save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd5800c-4aca-423c-b77a-26c2f4b93d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the best model based on the optimized epoch results during training\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback_1 = ModelCheckpoint(\n",
    "    filepath='best_model_1.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "checkpoint_callback_2 = ModelCheckpoint(\n",
    "    filepath='best_model_2.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "checkpoint_callback_3 = ModelCheckpoint(\n",
    "    filepath='best_model_3.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6950e7e7-c4e7-4b71-9f21-cc3ba4836504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Compile the models\n",
    "model_1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model_2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model_3.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the models\n",
    "history_1 = model_1.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels), \n",
    "                       callbacks=[checkpoint_callback_1])\n",
    "\n",
    "history_2 = model_2.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                       callbacks=[checkpoint_callback_2])\n",
    "\n",
    "history_3 = model_3.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels),\n",
    "                       callbacks=[checkpoint_callback_3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e193e-9135-4c6d-b916-9a449d1e804a",
   "metadata": {},
   "source": [
    "4. Evaluate the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcd98d3-5e97-4900-8e7e-35978f3daab0",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "1. Evaluate each model using the test data.\n",
    "2. Print the test accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d0ea5-709d-4a1f-96ff-e642405aedac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Best model for Model 1 saved at: {checkpoint_callback_1.filepath}\")\n",
    "print(f\"Best model for Model 2 saved at: {checkpoint_callback_2.filepath}\")\n",
    "print(f\"Best model for Model 3 saved at: {checkpoint_callback_3.filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02670ff-a4ed-43e5-a3e6-f929c181dfb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  best models metrics in a table \n",
    "# first load the best models\n",
    "from tensorflow.keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "best_model_1 = load_model('best_model_1.h5')\n",
    "best_model_2 = load_model('best_model_2.h5')\n",
    "best_model_3 = load_model('best_model_3.h5')\n",
    "\n",
    "# second evaluate the best models on the test data\n",
    "eval_1 = best_model_1.evaluate(test_images, test_labels, verbose=0)\n",
    "eval_2 = best_model_2.evaluate(test_images, test_labels, verbose=0)\n",
    "eval_3 = best_model_3.evaluate(test_images, test_labels, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45535d14-fb43-4af7-ab32-34c8daf22073",
   "metadata": {},
   "source": [
    "5. Compare and Visualize Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cfdac4-d7c3-4f4e-a030-cd27c85211ea",
   "metadata": {},
   "source": [
    "### calculating AUC and plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd012d9-ce3e-44d5-a59d-9e089d0bcb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# custom function to do the job\n",
    "def evaluate_model(model, test_images, test_labels):\n",
    "    # Get the probabilities for the positive class\n",
    "    y_pred_prob = model.predict(test_images)\n",
    "    \n",
    "    # Convert test_labels to one-hot encoding for AUC calculation\n",
    "    test_labels_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = roc_curve(test_labels_one_hot.ravel(), y_pred_prob.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5cc5f9-96b2-47af-8974-0d52c8d41323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2a085a-76c8-41d7-8196-054afb465abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate for the best models\n",
    "roc_auc_1, fpr_1, tpr_1 = evaluate_model(best_model_1, test_images, test_labels)\n",
    "roc_auc_2, fpr_2, tpr_2 = evaluate_model(best_model_2, test_images, test_labels)\n",
    "roc_auc_3, fpr_3, tpr_3 = evaluate_model(best_model_3, test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6594888-0bd2-4ba7-b1fd-4bc6492cf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then create a DataFrame to display the results\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Model 1\", \"Model 2\", \"Model 3\"],\n",
    "    \"Loss\": [eval_1[0], eval_2[0], eval_3[0]],\n",
    "    \"Accuracy\": [eval_1[1], eval_2[1], eval_3[1]],\n",
    "    \"Validation Loss\": [min(history_1.history['val_loss']), min(history_2.history['val_loss']), min(history_3.history['val_loss'])],\n",
    "    \"Validation Accuracy\": [max(history_1.history['val_accuracy']), max(history_2.history['val_accuracy']), max(history_3.history['val_accuracy'])]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4b24d-bb4c-432b-984a-c3aa17dfd27c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# extracting metrics from history\n",
    "def extract_metrics(history):\n",
    "    return (min(history.history['loss']), max(history.history['accuracy']),\n",
    "            min(history.history['val_loss']), max(history.history['val_accuracy']))\n",
    "\n",
    "\n",
    "loss_1, acc_1, val_loss_1, val_acc_1 = extract_metrics(history_1)\n",
    "loss_2, acc_2, val_loss_2, val_acc_2 = extract_metrics(history_2)\n",
    "loss_3, acc_3, val_loss_3, val_acc_3 = extract_metrics(history_3)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Model 1\", \"Model 2\", \"Model 3\"],\n",
    "    \"Loss\": [f\"{loss_1:.4f}\", f\"{loss_2:.4f}\", f\"{loss_3:.4f}\"],\n",
    "    \"Accuracy\": [f\"{acc_1:.4f}\", f\"{acc_2:.4f}\", f\"{acc_3:.4f}\"],\n",
    "    \"Validation Loss\": [f\"{val_loss_1:.4f}\", f\"{val_loss_2:.4f}\", f\"{val_loss_3:.4f}\"],\n",
    "    \"Validation Accuracy\": [f\"{val_acc_1:.4f}\", f\"{val_acc_2:.4f}\", f\"{val_acc_3:.4f}\"],\n",
    "    \"AUC\": [f\"{roc_auc_1:.4f}\", f\"{roc_auc_2:.4f}\", f\"{roc_auc_3:.4f}\"]\n",
    "})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07eb943-c813-4141-87bb-873fd278ab05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ploting ROC curve for the best model\n",
    "plt.figure()\n",
    "plt.plot(fpr_1, tpr_1, label=f'Model 1 (AUC = {roc_auc_1:.4f})')\n",
    "plt.plot(fpr_2, tpr_2, label=f'Model 2 (AUC = {roc_auc_2:.4f})')\n",
    "plt.plot(fpr_3, tpr_3, label=f'Model 3 (AUC = {roc_auc_3:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a61f58-7abe-4799-b385-91d56ccf0ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the best model based on validation accuracy\n",
    "best_model_index = np.argmax([eval_1[1], eval_2[1], eval_3[1]])\n",
    "best_model_name = f\"Model {best_model_index + 1}\"\n",
    "best_model_auc = [roc_auc_1, roc_auc_2, roc_auc_3][best_model_index]\n",
    "best_model_val_loss = [eval_1[0], eval_2[0], eval_3[0]][best_model_index]\n",
    "\n",
    "print(f\"\\nConclusion: The best model is {best_model_name} with an AUC of {best_model_auc:.4f} and a validation loss of {best_model_val_loss:.4f}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
