{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Day 2 : Unsupervised Learning Techniques"
      ],
      "metadata": {
        "id": "aW4ST-6hG2ot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Intro:*** Unsupervised learning is a type of machine learning where the algorithm learns patterns or structures from unlabeled data. \n",
        "\n",
        "***The key difference between supervised and unsupervised learning:*** Unlike supervised learning, which requires labeled data with input-output pairs, unsupervised learning algorithms work on data without any predefined output labels or target variables.\n",
        "\n",
        "***The objective of unsupervised learning*** is to discover inherent patterns, relationships, or structures within the data. It aims to uncover hidden insights, group similar data points together, or identify meaningful representations of the data without any prior knowledge or guidance.\n",
        "\n",
        "***Common tasks in unsupervised learning include:***\n",
        "\n",
        "1. Clustering techniques: Grouping similar data points together based on their characteristics or proximity.\n",
        "\n",
        "2. Dimensionality Reduction: Reducing the number of features or variables in the data while preserving its essential information.\n",
        "\n",
        "3. Anomaly Detection: Identifying unusual or rare instances that deviate significantly from the norm.\n",
        "\n",
        "4. Association Rule Mining: Discovering interesting associations or relationships between different variables in the data.\n",
        "\n",
        "Unsupervised learning algorithms rely on mathematical techniques such as clustering algorithms (e.g., K-means, DBSCAN), dimensionality reduction methods (e.g., PCA, t-SNE), and density estimation techniques (e.g., Gaussian Mixture Models) to learn patterns and extract valuable insights from the data.\n",
        "\n",
        "Unsupervised learning has various applications, including customer segmentation, recommendation systems, anomaly detection, image and text analysis, and data exploration. It plays a crucial role in exploratory data analysis and can provide valuable insights when dealing with large, unstructured, or unlabeled datasets."
      ],
      "metadata": {
        "id": "oTEZVfjLNV5x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6opRgKuNVVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Unsupervised Learning Example \n",
        "####Project: Clustering Analysis of COVID-19 Tweets\n",
        "\n",
        "Description:\n",
        "You have been provided with a dataset containing a collection of tweets related to the COVID-19 pandemic. The objective of this project is to apply unsupervised learning techniques to cluster the tweets based on their content and identify common themes or topics discussed on Twitter during the pandemic.\n",
        "\n"
      ],
      "metadata": {
        "id": "Oucx-kKMMkzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "1. Data Preprocessing:\n",
        "   - Load the COVID-19 Twitter dataset.\n",
        "   - Text Cleaning: Removal of special characters, URLs, and unnecessary white spaces.\n",
        "   - Tokenization: Splitting the tweet text into individual words or tokens.\n",
        "   - Stopword Removal: Removing common words that do not carry significant meaning.\n",
        "   - Stemming or Lemmatization: Reducing words to their base form (e.g., running -> run) for normalization.\n",
        "\n",
        "2. Feature Extraction:\n",
        "   \n",
        "   Convert the preprocessed text data into numerical representations suitable for clustering.\n",
        "   - Utilize techniques such as TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe) to represent the tweets as feature vectors.\n",
        "\n",
        "3. Unsupervised Learning Techniques:\n",
        "  - Clustering: Grouping similar tweets together based on their content or topics.\n",
        "  - Topic Modeling: Extracting latent topics from the tweets using techniques like Latent Dirichlet Allocation (LDA).\n",
        "  - Word Embeddings: Representing words or phrases as dense vectors to capture semantic relationships.\n",
        "  - Dimensionality Reduction: Reducing the dimensionality of the dataset using techniques like Principal Component Analysis (PCA) or t-SNE.\n",
        "\n",
        "4. Cluster Analysis and Visualization:\n",
        "   - Analyze the content and themes of each cluster to gain insights into the different topics discussed in COVID-19 tweets.\n",
        "   - Use visualization techniques (e.g., word clouds, bar charts) to visualize the most frequent words or phrases within each cluster.\n",
        "   - Identify and label the clusters based on the dominant themes present in the tweets.\n",
        "\n",
        "5. Interpretation and Insights:\n",
        "   - Interpret the clustering results and identify the major topics or discussions surrounding COVID-19 on Twitter.\n",
        "   - Explore the temporal aspects of the tweets to observe any changes in the topics over time.\n",
        "   - Discuss any interesting findings or patterns discovered through the analysis."
      ],
      "metadata": {
        "id": "Tn0jpMx4PkMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://www.kaggle.com/datasets/gpreda/all-covid19-vaccines-tweets"
      ],
      "metadata": {
        "id": "7lolZonFPwFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start the project.\n",
        "\n",
        "1. Data Preprocessing: \n",
        "\n",
        "  - Load the COVID-19 Twitter dataset.\n",
        "     You could download the COVID-19 Twitter dataset from this [github](https://github.com/Sammyjoon/COVID-19_Twitter) repositiory."
      ],
      "metadata": {
        "id": "2fLtNsblyYX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset in the form of `CSV` file. To work with csv in Python you need `pandas` package. Using the pandas library makes working with CSV files in Python easier and more efficient. `pandas` provides high-level data structures and data analysis tools, making it a popular choice for working with structured data, including `CSV` files. "
      ],
      "metadata": {
        "id": "uCJdiR897YIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "rQcYp11E0p4M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Reading data from a `CSV` file and save it in a dataframe available in \n",
        "`pandas`. The DataFrame is a two-dimensional tabular data structure that provides easy indexing, slicing, and manipulation of data. "
      ],
      "metadata": {
        "id": "alyTqV7C-LHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('specify the file path or URL as the argument')\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "XimiqEMn9YWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing data in a DataFrame:\n"
      ],
      "metadata": {
        "id": "FfPUdDDI_WK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_column = df['Name'] #Column-wise access\n",
        "first_row = df.loc[0]  # Access row by label (index)\n",
        "second_row = df.iloc[1]  # Access row by integer location"
      ],
      "metadata": {
        "id": "J3zF1pPr_x2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Text Cleaning\n",
        "\n",
        "   To demonstrate common text cleaning techniques you need to use the nltk library i. Python. NLTK provides various functionalities and resources for tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, parsing, and more."
      ],
      "metadata": {
        "id": "y1ocRbGGBAJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Text to be cleaned\n",
        "text = \"This is an example sentence! It contains punctuation marks and stopwords.\"\n",
        "\n",
        "# Lowercasing\n",
        "text = text.lower()\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Removing Punctuation\n",
        "tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "# Removing Stop Words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Print the cleaned tokens\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "vHfTxxwYCWUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Feature Extraction:\n",
        "     \n",
        "     Feature extraction is a process in machine learning and data analysis that involves transforming raw data into a set of meaningful and informative features. It aims to capture the most relevant information from the data and represent it in a way that is suitable for the learning algorithms.\n",
        "   - Utilize techniques such as TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe) to represent the tweets as feature vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "Oe8_ymhVPoDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do feature extraction we use `Scikit-learn` library. `Scikit-learn` is a popular machine learning library in Python that provides a wide range of tools for various tasks, such as data preprocessing, feature selection, model training, and evaluation. \n",
        "\n",
        "To use `Scikit-learn` we need to install it first."
      ],
      "metadata": {
        "id": "XSu7KL4YGakQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "id": "IHH5mo8-Pwv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17e5669-5f1a-4aed-f8c1-a25c0d0d7ad9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction using the `TF-IDF` technique in Python using the `Scikit-learn` library"
      ],
      "metadata": {
        "id": "FQF4zgtvHnKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text data\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the corpus and transform the corpus into TF-IDF features\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# Print the TF-IDF features\n",
        "for i, doc in enumerate(corpus):\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for j, word in enumerate(feature_names):\n",
        "        print(f\"{word}: {X[i, j]}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "KlonPqPtHsK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Unsupervised Learning Techniques:\n",
        "  - Clustering: Grouping similar tweets together based on their content or topics.\n",
        "  - Topic Modeling: Extracting latent topics from the tweets using techniques like Latent Dirichlet Allocation (LDA).\n",
        "  - Word Embeddings: Representing words or phrases as dense vectors to capture semantic relationships.\n",
        "  - Dimensionality Reduction: Reducing the dimensionality of the dataset using techniques like Principal Component Analysis (PCA) or t-SNE.\n",
        "\n"
      ],
      "metadata": {
        "id": "-x8_sfrQPqIX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATYqdGprPxby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Cluster Analysis and Visualization:\n",
        "   - Analyze the content and themes of each cluster to gain insights into the different topics discussed in COVID-19 tweets.\n",
        "   - Use visualization techniques (e.g., word clouds, bar charts) to visualize the most frequent words or phrases within each cluster.\n",
        "   - Identify and label the clusters based on the dominant themes present in the tweets.\n",
        "\n"
      ],
      "metadata": {
        "id": "zSyTlOafPsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DiTXJ3IBPyD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Interpretation and Insights:\n",
        "   - Interpret the clustering results and identify the major topics or discussions surrounding COVID-19 on Twitter.\n",
        "   - Explore the temporal aspects of the tweets to observe any changes in the topics over time.\n",
        "   - Discuss any interesting findings or patterns discovered through the analysis."
      ],
      "metadata": {
        "id": "aT3G-mEgPt6W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlgbM3JqPyln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
