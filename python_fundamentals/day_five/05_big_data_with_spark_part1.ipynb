{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FEqoGwI2722A",
    "outputId": "175760f7-f96a-4bdb-8683-f30df3320164",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import camber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "em0G3zHh8DBy",
    "outputId": "6eff7521-2f92-4c63-b014-2eb7f21ccc4e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531d0cd3502f4f1dbc43f7c85a31eb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = camber.spark.connect(worker_size=\"XSMALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_lXyn73l78z_"
   },
   "source": [
    "**Resilient Distributed Dataset (RDD)**: RDD is the fundamental data structure of Spark. It is fault-tolerant (resilient) and immutable distributed collections of any type of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "Q5SujEC2-OT8",
    "outputId": "c407bfa7-8cc1-4f79-91c8-9bb6cede63c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing Spark\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-ZDN4Fo-Hi2"
   },
   "source": [
    "# Part 1: Create RDDs and Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwP9DOU87__n",
    "outputId": "2aeab194-4e68-427b-d6b1-2aaa2a43882c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 22, 7, 20, 38, 11, 2, 8, 16, 24]\n"
     ]
    }
   ],
   "source": [
    "# Generate random data:\n",
    "import random\n",
    "# Generate 10 random numbers between 0 and 40\n",
    "randomlist = random.sample(range(0, 40), 10)\n",
    "print(randomlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "diNJOFur-K6T",
    "outputId": "f9bc73c0-dfcf-4110-87e7-8b3abc186775",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 22, 7, 20, 38, 11, 2, 8, 16, 24]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create RDD:\n",
    "rdd1 = sc.parallelize(randomlist, 4)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK4ETrZL-MX2",
    "outputId": "75f48add-465b-49cc-8573-56c6458091ef",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[14, 22], [7, 20], [38, 11], [2, 8, 16, 24]]\n",
      "Two partitions:  [[14, 22], [7, 20]]\n"
     ]
    }
   ],
   "source": [
    "# Data distribution in partitions:\n",
    "print(rdd1.getNumPartitions())\n",
    "print(rdd1.glom().collect())\n",
    "print(\"Two partitions: \", rdd1.glom().take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeSymVTG-Tnc",
    "outputId": "0cd9a7c7-6f18-4c4a-df96-3981fa18c450",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "8\n",
      "16\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Print last partition\n",
    "for item in rdd1.glom().collect()[3]:\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGMn2Dxq-YBd",
    "outputId": "b5e3e7bb-4c12-46dc-927e-46e7de6cd81f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count():\n",
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkqsdkUB-bfl",
    "outputId": "39975b76-56c6-4498-95d4-4d6161520b1d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first():\n",
    "rdd1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZGMZFON-6h6",
    "outputId": "dfde297c-79d8-4a50-fc48-680efa1e2ee1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 22], [7, 20], [38, 11], [2, 8, 16, 24]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "umIa1V0z-k6T",
    "outputId": "a22aaaa1-1fdd-44de-a949-6f27472456ba",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 69, 24, 63, 117, 36, 9, 27, 51, 75]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map():\n",
    "rdd_map = rdd1.map(lambda x:(x+1)*3)\n",
    "rdd_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmpTiEyX-pr7",
    "outputId": "df71c0b2-9409-4b95-e819-1d67a72336d3",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter(): \n",
    "rdd_filter = rdd1.filter(lambda x : x%3==0)\n",
    "rdd_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v0NL-1nV-9fP",
    "outputId": "23c9b815-844e-4566-9716-5eb0a66c8076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 19, 24, 27, 9, 12, 22, 25, 40, 43, 13, 16, 4, 7, 10, 13, 18, 21, 26, 29]\n",
      "The summation of elements = 394\n"
     ]
    }
   ],
   "source": [
    "# flatMap():\n",
    "rdd_flatmap=rdd1.flatMap(lambda x: [x+2,x+5])\n",
    "print(rdd_flatmap.collect())\n",
    "print(\"The summation of elements =\", rdd_flatmap.reduce(lambda a,b : a + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oLH1PfOn-_o4",
    "outputId": "ff0633dd-2463-4c00-9068-473db00169bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 2, 16.2, 162, 9.85]\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics:\n",
    "print([rdd1.max(), rdd1.min(), rdd1.mean(), rdd1.sum(), round(rdd1.stdev(),2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ior5zOXN_Fec",
    "outputId": "6b18402f-3811-44e6-beb2-68b2690038dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14, 22], [7, 20], [38, 11], [2, 8, 16, 24]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6mdMFoO_NMv",
    "outputId": "a82dc908-570e-4653-9acf-7488a70ff20e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[36, 27, 49, 50]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapPartitions():\n",
    "\n",
    "def myfunc(partition):\n",
    "  sum = 0\n",
    "  for item in partition:\n",
    "    sum = sum + item\n",
    "  yield sum  # \"return\" causes a function to exit; \"yield\" is used to define generator and returns an intermediate results.\n",
    "\n",
    "rdd_mapPartition = rdd1.mapPartitions(myfunc)\n",
    "rdd_mapPartition.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJZFglPB_R7J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeicaWjyBPTy"
   },
   "source": [
    "# Code Demo 1: Filter and Transform Words\n",
    "\n",
    "Write a Python program that takes a list of sentences as input and performs the following operations:\n",
    "\n",
    "* Filter out sentences that contain the word \"spam\".\n",
    "* Split each remaining sentence into a list of words.\n",
    "* Transform each word to uppercase.\n",
    "* Flatten the list of words into a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YzrPUk-BTSl",
    "outputId": "be321c5a-e9cb-467b-eef3-8f7dd152ec22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This is not spam.'], ['I love spam and eggs.'], ['No spam here.'], ['Spam, spam, spam!']]\n",
      "['This', 'is', 'not', 'I', 'love', 'and', 'eggs', 'No', 'here', 'Spam']\n"
     ]
    }
   ],
   "source": [
    "# Input sentences\n",
    "sentences = [\n",
    "    \"This is not spam.\",\n",
    "    \"I love spam and eggs.\",\n",
    "    \"No spam here.\",\n",
    "    \"Spam, spam, spam!\"\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "rdd1 = sc.parallelize(sentences, 4)\n",
    "\n",
    "print(rdd1.glom().collect())\n",
    "\n",
    "# Implement the filter and transform operations\n",
    "filtered_sentences = rdd1.map(lambda s: s.replace(\"spam\", \"\").\\\n",
    "                              replace('\"', '').replace('.', '').\\\n",
    "                              replace(',', '').replace('!',''))\n",
    "words = filtered_sentences.flatMap(lambda s: s.split())\n",
    "\n",
    "# Print the result\n",
    "print(words.collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vALL-6GC7Yy"
   },
   "source": [
    "In Apache Spark, `flatMap()` is a transformation operation on an RDD (Resilient Distributed Dataset) that allows you to apply a function to each element of the RDD and flatten the results. It creates a new RDD by first applying the function to each element and then flattening the results into a single list.\n",
    "\n",
    "Here's how `flatMap()` works:\n",
    "\n",
    "1. The input function is applied to each element of the RDD.\n",
    "2. The function returns an iterator or sequence of elements for each input element.\n",
    "3. The results from the function are flattened into a single list by merging all the generated iterators/sequences.\n",
    "4. The flattened list is used to create a new RDD.\n",
    "\n",
    "The key difference between `flatMap()` and `map()` is that `flatMap()` allows each input element to map to zero or more output elements, while `map()` maps each input element to a single output element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icxZaZBiEJ-u"
   },
   "source": [
    "# Code Demo 2: Medical Dataset Analysis and Storytelling\n",
    "\n",
    "Assume you have a custom medical dataset that contains records of patients and their medical conditions. Each record is a string with the following format: \"<patient_id>:<condition>\". The exercise involves analyzing the dataset, filtering specific conditions, transforming data, and generating a story based on the results.\n",
    "\n",
    "Here are the steps to follow:\n",
    "\n",
    "1. Create an RDD from a list of medical records.\n",
    "2. Implement a filter operation to select records of patients with a specific condition (e.g., \"diabetes\").\n",
    "3. Transform the filtered records by mapping each record to the patient ID only.\n",
    "Implement a flatMap operation to obtain a single list of patient IDs from the transformed records.\n",
    "4. Generate a story by using the patient IDs to gather additional information (e.g., demographic data) from another dataset.\n",
    "Note: For this exercise, assume you have a separate dataset or API that provides additional patient information based on their ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhFoilLYEJf_",
    "outputId": "c9325a0c-446c-433c-d07c-13d1470197c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following patients have been diagnosed with diabetes: 1, 3, 5, 7, 9\n"
     ]
    }
   ],
   "source": [
    "# Input medical records\n",
    "medical_records = [\n",
    "    \"1:diabetes\",\n",
    "    \"2:asthma\",\n",
    "    \"3:diabetes\",\n",
    "    \"4:hypertension\",\n",
    "    \"5:diabetes\",\n",
    "    \"6:arthritis\",\n",
    "    \"7:diabetes\",\n",
    "    \"8:asthma\",\n",
    "    \"9:diabetes\"\n",
    "]\n",
    "\n",
    "# Create RDD from medical records\n",
    "rdd_medical = sc.parallelize(medical_records)\n",
    "\n",
    "# Filter for patients with diabetes\n",
    "filtered_diabetes = rdd_medical.filter(lambda record: record.split(\":\")[1] == \"diabetes\")\n",
    "\n",
    "# Transform to patient IDs\n",
    "patient_ids = filtered_diabetes.map(lambda record: record.split(\":\")[0])\n",
    "\n",
    "# Obtain a single list of patient IDs\n",
    "flat_patient_ids = patient_ids.flatMap(lambda x: x)\n",
    "\n",
    "# Generate a story based on patient IDs\n",
    "story = \"The following patients have been diagnosed with diabetes: \" + \", \".join(flat_patient_ids.collect())\n",
    "\n",
    "# Print the story\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOe05jOm_YnG"
   },
   "source": [
    "# Part 2: Advanced RDD Transformations and Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmISSRU__clA",
    "outputId": "cd237097-8ab1-4b75-8178-cda60b729ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 36, 1, 5, 19, 33, 25, 28, 10, 12]\n",
      "[1, 14, 20, 20, 29, 10, 13, 3]\n",
      "6\n",
      "[21, 36, 1, 5, 19, 33, 25, 28, 10, 12, 1, 14, 20, 20, 29, 10, 13, 3]\n"
     ]
    }
   ],
   "source": [
    "# union():\n",
    "rdd1 = sc.parallelize(random.sample(range(0, 40), 10), 4)\n",
    "print(rdd1.collect())\n",
    "rdd2 = sc.parallelize([1, 14, 20, 20, 29, 10, 13, 3],2)\n",
    "print(rdd2.collect())\n",
    "\n",
    "rdd_union = rdd1.union(rdd2)\n",
    "print(rdd_union.getNumPartitions())\n",
    "print(rdd_union.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9S8jZEtD_79N",
    "outputId": "ab989c3b-559a-49f6-c4ec-38de3b5fc5fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 36],\n",
       " [1, 5],\n",
       " [19, 33],\n",
       " [25, 28, 10, 12],\n",
       " [1, 14, 20, 20],\n",
       " [29, 10, 13, 3]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_union.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccNljlf3ARH-",
    "outputId": "1a259923-25ce-4c83-fdb9-d5d779980aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 36, 1, 5, 19, 33, 25, 28, 10, 12]\n",
      "[1, 5, 10, 12, 19]\n",
      "[36, 33, 28, 25, 21]\n"
     ]
    }
   ],
   "source": [
    "# takeOrdered(n, [ordering])\n",
    "# This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n",
    "print(rdd1.collect())\n",
    "print(rdd1.takeOrdered(5))\n",
    "print(rdd1.takeOrdered(5, key=lambda x: -x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3A8r0sYJm-w"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13xURDHeAT-w",
    "outputId": "8087d09a-81a5-48e3-d5cd-f2cf8f410757"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce():\n",
    "# A commutative and associative binary operator.\n",
    "rdd1.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m14w-sSfAYW5",
    "outputId": "fe921516-4fb5-4418-b004-a99f4b3168cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (7, 10), (5, 7), (1, 12), (7, 12), (7, 1), (9, 1), (7, 4)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey():\n",
    "rdd_Rbk = sc.parallelize([(1,4),(7,10),(5,7),(1,12),(7,12),(7,1),(9,1),(7,4)], 2)\n",
    "print(rdd_Rbk.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ-dV9zHNnxF",
    "outputId": "4288a5d1-5145-4f14-bed7-88076c733ff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 16), (5, 7), (7, 27), (9, 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_Rbk.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGfDB9fUAatf",
    "outputId": "58e892ad-7df9-4512-c85d-d2234005b0f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 16), (5, 7), (7, 27), (9, 1)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey():\n",
    "rdd_Rbk.reduceByKey(lambda x,y: x+y).sortByKey().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-pVMKRJCAclL",
    "outputId": "010c2921-45fc-4cee-d372-80b84aee3b5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (5, 1), (7, 4), (9, 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# countByKey()\n",
    "rdd_Rbk.countByKey()\n",
    "rdd_Rbk.countByKey().items()\n",
    "sorted(rdd_Rbk.countByKey())\n",
    "sorted(rdd_Rbk.countByKey().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iUWA5q0Aeby",
    "outputId": "2c898e69-5537-4f90-cd39-5807eea7a096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [4, 12]\n",
      "5 [7]\n",
      "7 [10, 12, 1, 4]\n",
      "9 [1]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey():\n",
    "rdd_group = rdd_Rbk.groupByKey() \n",
    "rdd_group.getNumPartitions()\n",
    "\n",
    "rdd_group.collect() # it executes at driver node, not recommended\n",
    "\n",
    "for item in rdd_group.collect():\n",
    "  print(item[0], [value for value in item[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukugOxc7AgVe",
    "outputId": "f1291c81-7ba8-4da6-ac73-a64915f81337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 12, 1, 4]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup(key):\n",
    "rdd_Rbk.lookup(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wG7_72GUPmir"
   },
   "source": [
    "# Code Demo 3: Books collection\n",
    "\n",
    "Assume you have a dataset representing a collection of books. Each record in the dataset contains information about a book, such as its title, author, publication year, and genre. Your task is to perform various analyses on the books using RDD methods in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "D_YNhdFbAjX7"
   },
   "outputs": [],
   "source": [
    "bookRecords = [\n",
    "    (\"Book A\", \"Author 1\", 2005, \"Fiction\"),\n",
    "    (\"Book B\", \"Author 2\", 2010, \"Mystery\"),\n",
    "    (\"Book C\", \"Author 1\", 2015, \"Science Fiction\"),\n",
    "    (\"Book D\", \"Author 3\", 2018, \"Fantasy\"),\n",
    "    (\"Book E\", \"Author 3\", 2018, \"Fantasy\"),\n",
    "    # Add more book records as needed\n",
    "]\n",
    "\n",
    "booksRDD = sc.parallelize(bookRecords, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hruGkSBPPwkC",
    "outputId": "c04fbbe3-0c4e-46b5-e81f-2effddebb6ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fiction', 1), ('Mystery', 1), ('Fantasy', 2), ('Science Fiction', 1)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of books in each genre.\n",
    "booksByGenre = booksRDD.map(lambda book: (book[3], 1)) \\\n",
    "                      .reduceByKey(lambda a, b: a + b)\n",
    "booksByGenre.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iJUhw1sP1zA",
    "outputId": "93096ec8-65f5-479b-fdaf-ca3679dd7ce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Author 3', 2), ('Author 1', 2), ('Author 2', 1)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the top 5 authors who have written the most books.\n",
    "topAuthors = booksRDD.map(lambda book: (book[1], 1)) \\\n",
    "                    .reduceByKey(lambda a, b: a + b) \\\n",
    "                    .takeOrdered(5, key=lambda x: -x[1])\n",
    "topAuthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdNVX4-hQA1U",
    "outputId": "ef66cf15-c984-43cb-82d2-1c24a43de0ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Book A', 'Author 1', 2005, 'Fiction')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the book with the earliest publication year.\n",
    "oldestBook = booksRDD.min(key=lambda book: book[2])\n",
    "oldestBook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "KqWjY77WQWly"
   },
   "outputs": [],
   "source": [
    "# Count the number of books written by each author.\n",
    "booksByAuthor = booksRDD.map(lambda book: (book[1], 1)) \\\n",
    "                        .reduceByKey(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0mOR4YDQehm",
    "outputId": "de621696-515d-4982-eb2d-ba677ee7fd25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Author 2', 1), ('Author 3', 2), ('Author 1', 2)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booksByAuthor.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "3ez60MhfQu7t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "authorYearRDD = booksRDD.map(lambda x: (x[1], x[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [('Author 1', 2005)],\n",
       " [('Author 2', 2010)],\n",
       " [('Author 1', 2015)],\n",
       " [('Author 3', 2018)],\n",
       " [('Author 3', 2018)]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorYearRDD.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sumCountRDD = authorYearRDD.combineByKey(\n",
    "    lambda value: (value, 1),\n",
    "    lambda x, value: (x[0] + value, x[1] + 1),\n",
    "    lambda x, y: (x[0] + y[0], x[1] + y[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Author 2', (2010, 1)), ('Author 3', (4036, 2)), ('Author 1', (4020, 2))]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumCountRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meanYearsRDD = sumCountRDD.mapValues(lambda x: x[0] / x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Author 2', 2010.0), ('Author 3', 2018.0), ('Author 1', 2010.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanYearsRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. `lambda value: (value, 1)`: This is the `createCombiner` function. It is executed when encountering a new key for the first time. In this case, it takes the `value` as an argument and returns a tuple `(value, 1)`. It creates the initial intermediate value by using the `value` itself and setting the count to 1.\n",
    "\n",
    "2. `lambda x, value: (x[0] + value, x[1] + 1)`: This is the `mergeValue` function. It is called for each subsequent value associated with the same key. The function takes two arguments: `x`, the current intermediate value, and `value`, the new value to be merged. In this case, it adds the `value` to the existing sum (`x[0] + value`) and increments the count by 1 (`x[1] + 1`).\n",
    "\n",
    "3. `lambda x, y: (x[0] + y[0], x[1] + y[1])`: This is the `mergeCombiners` function. It is used to merge intermediate values within the same partition when there are multiple partitions for the same key. The function takes two arguments: `x` and `y`, representing two intermediate values. In this case, it adds the sums (`x[0] + y[0]`) and counts (`x[1] + y[1]`) to combine the intermediate results across partitions.\n",
    "\n",
    "Now, let's see how these functions are applied in the `combineByKey()` transformation:\n",
    "\n",
    "1. When encountering a new key for the first time, the `createCombiner` function is called. It creates the initial intermediate value `(value, 1)`.\n",
    "\n",
    "2. For subsequent values with the same key, the `mergeValue` function is called. It merges the new value with the existing intermediate value. It updates the sum by adding the new value to the existing sum and increments the count by 1.\n",
    "\n",
    "3. If there are multiple partitions, the `mergeCombiners` function is used to merge intermediate values across partitions. It combines the sums and counts from different partitions to create a final intermediate result.\n",
    "\n",
    "The resulting RDD `sumCountRDD` will contain key-value pairs, where the key is the unique author, and the value is a tuple representing the sum of the years and the count of books for that author.\n",
    "\n",
    "By using `combineByKey()`, you can perform custom aggregations on each key in a distributed and parallel manner, making it a powerful tool for data aggregation in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCamber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
