{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7f8d9d",
   "metadata": {},
   "source": [
    "# Python Optimization\n",
    "\n",
    "In this section we will discuss python optimization.  The key fact about python optimization is several-fold.\n",
    "1. Is your code correct?\n",
    "2. Do you need to optimize?\n",
    "3. Do you really need to optimize?\n",
    "4. Optimize is not parallelization -- usually do this last.\n",
    "5. Optimization involves tradeoffs.  Be careful what you wish for.\n",
    "\n",
    "There are a few steps to optimization:\n",
    "1. profile\n",
    "2. profile again.\n",
    "3. check the hotspots.\n",
    "4. payoff in optimization: modify your use case, use better algorithms, use builtin functions, use numba, pre-compiled code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66692f2",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "The first and most important aspect of optimization is to figure out what is the slow part.  For this you need to profile your code.  Fortunately python offers some excellent profilers and jupyter has step this part up even further.  For this, we will use the magic function %prun.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9c299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "N_massive = 20\n",
    "N_bodies = N_massive\n",
    "M = np.ones(N_bodies)\n",
    "\n",
    "def Nbody_derivatives(pos,vel) :\n",
    "    dpdt = vel\n",
    "    dvdt = np.zeros(vel.shape)\n",
    "    for i in range(N_bodies) :\n",
    "        for j in range(N_bodies) :\n",
    "            if i == j : \n",
    "                continue\n",
    "            r = np.linalg.norm( pos[j]-pos[i])\n",
    "            mass = M[j]\n",
    "            rhat =(pos[j] - pos[i])/r\n",
    "            dvdt[i] -= mass/(r*r)*rhat\n",
    "        \n",
    "    return dpdt, dvdt\n",
    "\n",
    "def initial_conditions() : \n",
    "    pos = np.random.random([N_bodies,3])\n",
    "    vel = np.random.random([N_bodies,3])\n",
    "\n",
    "    return pos, vel\n",
    "\n",
    "def run_Nbody_rk2(tend,tframe,dt,derivatives=Nbody_derivatives) :\n",
    "    p,v = initial_conditions()\n",
    "    t = 0\n",
    "    tnext = tframe\n",
    "    positions = []\n",
    "    while t<tend :\n",
    "        while t < tnext :\n",
    "            delta_t = min(tnext-t,dt)\n",
    "            dpdt, dvdt = derivatives(p,v) \n",
    "            phalf, vhalf = p+dpdt*0.5*delta_t, v+dvdt*0.5*delta_t\n",
    "            dpdt, dvdt = derivatives(phalf, vhalf)\n",
    "            p, v = p + dpdt*delta_t, v + dvdt*delta_t\n",
    "            t += delta_t\n",
    "        positions.append(p.copy())\n",
    "        tnext += tframe\n",
    "    return positions\n",
    "\n",
    "tframe = 0.01\n",
    "dt = 0.001\n",
    "frames = 100\n",
    "%prun positions = run_Nbody_rk2(frames*tframe, tframe, dt)\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def animate(i, positions):\n",
    "    ax.clear()\n",
    "    # Get the point from the points list at index i\n",
    "    pos = positions[i]\n",
    "    ax.scatter(pos[:,0], pos[:,1], color='green', marker='o')\n",
    "    # Set the x and y axis to display a fixed range\n",
    "    ax.set_xlim([-10,10])\n",
    "    ax.set_ylim([-10,10])\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=len(positions), interval=50, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d149db",
   "metadata": {},
   "source": [
    "As you can see the %prun reveals what is slow.  Nearly the entire code is spent in Nbody_derivatives, which computes the accelerations.  So it goes to show what profiling can do for you.  Pretty much nothing else is spent anywhere else. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7548cc6",
   "metadata": {},
   "source": [
    "## Optimal python\n",
    "\n",
    "The first thing we want to do is rewrite the code so that it is more correct or more pythonic -- this means that we want to write it so that is more numpy like.  Lets look at the following.  Starting with the above code, I challenge you to write it so that it is significantly cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0289fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Nbody_derivatives2(pos,vel) :\n",
    "    dpdt = vel\n",
    "    dvdt = np.zeros(vel.shape)\n",
    "    rvec = pos[np.newaxis,:,:] - pos[:,np.newaxis,:]\n",
    "    r = np.maximum(np.linalg.norm(rvec,axis=-1),1e-30)\n",
    "    dvdt = -((M[np.newaxis,:]/(r*r*r))[:,:,np.newaxis]*rvec).sum(axis=1)\n",
    "        \n",
    "    return dpdt, dvdt\n",
    "\n",
    "%prun positions = run_Nbody_rk2(frames*tframe, tframe, dt, derivatives=Nbody_derivatives2)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=len(positions), interval=50, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d3b10",
   "metadata": {},
   "source": [
    "This was a significant speedup -- about a factor of 20.   This is really excellent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1667a4f",
   "metadata": {},
   "source": [
    "## Hot spot optimization with numba\n",
    "\n",
    "Recently python compilation has started to become a thing.  One noteworthy example is numba, which is a jit compiler that works well with numpy. Lets try this one, but using the code originally.  \n",
    "\n",
    "It is extremely easy to use.  You can just use the decorator @jit before the function to optimize.\n",
    "\n",
    "There are two mode of operation nopython=True or False.  When nopython=True produces much faster code, but there are limitations that forces it to fall back to False mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e25c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import jit,njit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def Nbody_derivatives3(pos,vel) :\n",
    "    dpdt = vel\n",
    "    dvdt = np.zeros(vel.shape)\n",
    "    for i in range(N_bodies) :\n",
    "        for j in range(N_bodies) :\n",
    "            if i == j : \n",
    "                continue\n",
    "            r = np.linalg.norm( pos[j]-pos[i])\n",
    "            mass = M[j]\n",
    "            rhat =(pos[j] - pos[i])/r\n",
    "            dvdt[i] += -mass/(r*r)*rhat\n",
    "        \n",
    "    return dpdt, dvdt\n",
    "\n",
    "%prun positions = run_Nbody_rk2(frames*tframe, tframe, dt, derivatives=Nbody_derivatives3)\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=frames, interval=25, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c887cd0b",
   "metadata": {},
   "source": [
    "This is faster than idiomic python, but it can be marginally so. Still much faster than the original version.  Not bad for a simple @jit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd64de",
   "metadata": {},
   "source": [
    "## Hot spot optimization with cython\n",
    "\n",
    "Cython is a superset of the python language the \"converts python to c\" and then compiles the code to generate a fast runtime.  \n",
    "1. This means that any python program is a cython program/code.\n",
    "2. This also means that you can give cython directives to help do the conversion faster.  \n",
    "\n",
    "There are a number of directives, but the most important are data directives.  So you can define variables as \n",
    "1. cdef int -> int in c\n",
    "2. cdef float or cdef double -> float or double in c\n",
    "3. cdef int/float/double [:] or [:,:] as arrays -> int, float, double * or **\n",
    "This will python numerical data which are objects to fast native data types.  \n",
    "\n",
    "First we load cython in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341269d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d23845-3352-4b73-81ee-31d368dced7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we compile a cython program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3a62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cython -a\n",
    "import numpy as np\n",
    "from cpython cimport array\n",
    "import array\n",
    "import math \n",
    "def cython_Nbody_derivatives(pos,vel,M) :\n",
    "    cdef int N_bodies = M.size\n",
    "    cdef int i = 0, j = 0 \n",
    "    cdef double [:,:] p = pos\n",
    "    cdef double [:] mass = M\n",
    "    rhat = np.zeros(3)\n",
    "    cdef double [:] rh = rhat\n",
    "    cdef double r = 0., r2 = 0.\n",
    "    dpdt = vel\n",
    "    dvdt_arr = np.zeros(vel.shape)\n",
    "    cdef double [:,:] dvdt = dvdt_arr\n",
    "    for i in range(N_bodies) :\n",
    "        for j in range(N_bodies) :\n",
    "            if i == j : \n",
    "                continue\n",
    "            r2 = 0.\n",
    "            for k in range(3) : \n",
    "                r2 += p[j,k]*p[j,k] + p[i,k]*p[i,k]\n",
    "            r = math.sqrt(r2)\n",
    "            for k in range(3) : \n",
    "                rh[k] =(p[j,k] - p[i,k])/r\n",
    "                dvdt[i,k] += -mass[j]/(r*r)*rh[k]\n",
    "        \n",
    "    return dpdt, dvdt_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1513feae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cython_derivs(pos,vel) :\n",
    "    return cython_Nbody_derivatives(pos,vel,M)\n",
    "%prun positions = run_Nbody_rk2(frames*tframe, tframe, dt, derivatives=cython_derivs)\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=frames, interval=25, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c54d9f-31a1-4f48-a184-54b8917f6be7",
   "metadata": {},
   "source": [
    "As is, you get no speedup.  But if you judiciously use cdef int, cdef double, and cdef double [:,:], you get huge speedups. \n",
    "\n",
    "The speedup that I got was about 100x.  Can you match it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c26819",
   "metadata": {},
   "source": [
    "## Hot spot optimization with f2py\n",
    "\n",
    "The final example is using fortran to optimize the slowest bits.  Why fortran?  Because fortran 90 plays extremely well with python.\n",
    "\n",
    "First we generate the .f90 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb4356",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%file nbody_derivatives.f90\n",
    "SUBROUTINE derivs(pos,vel,mass,dpdt,dvdt,n)\n",
    "    implicit none\n",
    "    integer, intent(IN) :: n\n",
    "    double precision, intent(IN), dimension(n,3):: pos, vel\n",
    "    double precision, intent(IN), dimension(n) :: mass\n",
    "    double precision, intent(OUT), dimension(n,3):: dpdt, dvdt\n",
    "!f2py intent(in) n\n",
    "!f2py intent(in) pos, vel, mass\n",
    "!f2py intent(out) dpdt, dvdt\n",
    "!f2py depend(n) mass\n",
    "    integer :: i, j\n",
    "    double precision, dimension(3) :: rhat,r\n",
    "    double precision :: r2\n",
    "    \n",
    "    dpdt(:,:) = vel(:,:)\n",
    "    dvdt(:,:) = 0.\n",
    "    do i = 1,n\n",
    "        do j = 1,n\n",
    "            if( i .eq. j) then\n",
    "                cycle\n",
    "            endif\n",
    "            r(:) = pos(j,:) - pos(i,:)\n",
    "            r2 = sum(r*r)\n",
    "            rhat = r/sqrt(r2)\n",
    "            dvdt(i,:) = dvdt(i,:)- mass(j)/(r2)*rhat(:)\n",
    "        enddo\n",
    "    enddo\n",
    "    \n",
    "    return\n",
    "end subroutine derivs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043d682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!f2py -m nbody_derivatives -c nbody_derivatives.f90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe791a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nbody_derivatives as nbd\n",
    "import importlib\n",
    "importlib.reload(nbd)\n",
    "\n",
    "def fortran_derivs(pos,vel) :\n",
    "    return nbd.derivs(pos,vel,M)\n",
    "%prun positions = run_Nbody_rk2(frames*tframe, tframe, dt, derivatives=fortran_derivs)\n",
    "#print(positions)\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=frames, interval=50, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d99f7f",
   "metadata": {},
   "source": [
    "This is extremely fast like 600x faster than the original code.  It is so fast that python becomes the limiting factor. Compiled code in a highly optimized language is extremely powerful.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba95ea3",
   "metadata": {},
   "source": [
    "# Running in Parallel\n",
    "\n",
    "Students usually think the running in parallel is the way you optimize your code.  This is not true.  As you can see above, the speedups are fortran, cython, numba, and optimized python.  These speedups that way to do it.  You should almost always try optimized python and numba first as they are easiest.  But then switch over to cython and fortran.  Fortran is explicitly the fastest, but it is the most work and requires you to learn a new language. \n",
    "\n",
    "There are many ways of doing parallelization.  One way we discussed already is using apache spark, which allows parallel processing of large distributed data sets, but this is overkill for most purposes, but extremely valuable in technology companies.  Instead we will focus on a simple way to do this.  This is one node, but with nodes containing upwards of 128 cpu cores, this is generally plenty for most problems. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177d7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing \n",
    "count = 4\n",
    "pool = multiprocessing.Pool(processes=count)\n",
    "\n",
    "def onebody_derivatives(i, pos, vel) :\n",
    "    dvdt = np.zeros(3)\n",
    "    for j in range(N_bodies) :\n",
    "        if i == j : \n",
    "            continue\n",
    "        r = np.linalg.norm( pos[j]-pos[i])\n",
    "        mass = M[j]\n",
    "        rhat =(pos[j] - pos[i])/r\n",
    "        dvdt += mass/(r*r)*rhat\n",
    "        \n",
    "    return dvdt\n",
    "\n",
    "def parallel_Nbody_derivatives(pos,vel) :\n",
    "    dpdt = vel\n",
    "    res = pool.starmap(onebody_derivatives, zip(range(N_bodies), [pos] * N_bodies, [vel]*N_bodies))\n",
    "    dvdt = np.array(res)\n",
    "    return dpdt, dvdt\n",
    "\n",
    "%prun positions = run_Nbody_rk2(frames*tframe, tframe, dt, derivatives=parallel_Nbody_derivatives)\n",
    "fig, ax = plt.subplots()\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=frames, interval=50, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8dc2f",
   "metadata": {},
   "source": [
    "This is the problem with parallelization.  In this case, we got worse performance.  You can see why, it has to lock down memory to make sure that people don't clobber each other.  This is because the amount of work per thing is too small.  So parallel is good if there is no contention *and* the amount of work is large per core.  This is not always true *cough, gpu*.  But generally for most of the problems you deal with it is the case. It doesn't usually give you the best performance.  Even this one where the operation is relatively parallel, the gains are not so great as being more careful about how you approach things.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e7a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCamber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
