{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb81279",
   "metadata": {},
   "source": [
    "## maximum likelihood\n",
    "\n",
    "$$\n",
    "L = \\exp\\left(-\\sum_i \\left(\\frac{x_i - \\mu_i}{\\sigma_i}\\right)^2\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\log(L) - \\Gamma =\\frac {1}{N-k}\\sum_i \\left(\\frac{x_i - \\mu_i}{\\sigma_i}\\right)^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19768a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0,2*np.pi,0.01*np.pi)\n",
    "y = np.sin(x) + (0.5*np.random.random(x.size)-0.25)\n",
    "\n",
    "plt.scatter(x,y,s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22a704",
   "metadata": {},
   "source": [
    "### Fitting a polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c5b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndeg = 3\n",
    "\n",
    "x_test = x.copy()\n",
    "# create a polynomial fit of degree 3 and plot it as x_test, y_test\n",
    "\n",
    "plt.scatter(x,y,s=1)\n",
    "plt.plot(x_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as so\n",
    "\n",
    "def fit_func(x,parameters):\n",
    "    pass\n",
    "\n",
    "def compute_residuals(parameters, xdata, ydata,sigma=0.25) : \n",
    "    pass\n",
    "\n",
    "lstsq = so.least_squares(compute_residuals, x0=[4,1], args=[x,y])\n",
    "\n",
    "\n",
    "#compute the chi squared\n",
    "\n",
    "print( chi_sq)\n",
    "\n",
    "plt.plot(x,fit_func(x, lstsq.x))\n",
    "plt.scatter(x,y,s=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cbaac",
   "metadata": {},
   "source": [
    "The biggest use of MCMC is its application to model fitting especially in the context of Bayesian modeling.  Usually in this case, we are concerned about the computation of the posterior distribution function $\\pi(\\vec{\\theta};\\vec{x})$ of parameters $\\vec{\\theta}$ given some data $\\vec{x}$ and a prior $\\pi(\\vec{\\theta})$.  We do this via Bayes' theorem\n",
    "$$\n",
    "\\pi(\\vec{\\theta};\\vec{x}) = \\frac{\\pi(\\vec{\\theta})p(\\vec{x};\\vec{\\theta})}{p(\\vec{x})}\n",
    "$$\n",
    "where $p(\\vec{x};\\vec{\\theta})$ is a likelihood function and $p(\\vec{x}) = \\int \\pi(\\vec{\\theta})p(\\vec{x};\\vec{\\theta})d\\vec{\\theta}$ is essentially the normalization.  This is the exact same problem that we encountered earlier where we needed to compute the expectation value of the energy in the Ising model.  However, in some ways this is simplier as the special substitutions are avoided in this case.  In essence, we want to compute\n",
    "$$\n",
    "p(\\vec{x}) = \\int \\pi(\\vec{\\theta})p(\\vec{x};\\vec{\\theta})d\\vec{\\theta}\n",
    "$$\n",
    "The key is that we only want to compute over the values of $\\theta$ that really contribute.  So we can define a Metropolis algorithm that goes for this as\n",
    "\\begin{enumerate}\n",
    "    \\item Begin with some parameter set $\\vec{\\theta}_1$ \n",
    "    \\item Compute the joint likelihood $L_1 = \\pi(\\vec{\\theta}_1)p(\\vec{x};\\vec{\\theta}_1)$\n",
    "    \\item Pick a random proposal $\\vec{\\theta}_2$ from a distribution based on the priors.\n",
    "    \\item Compute the likelihood $L_2 = \\pi(\\vec{\\theta}_2)p(\\vec{x};\\vec{\\theta}_2)$\n",
    "    \\item Compute the transition probability $R = L_2/L_1$\n",
    "    \\item Accept the proposal with probability $R$\n",
    "\\end{enumerate}\n",
    "\n",
    "Now to make contact with the problem in the Ising model where we computed the expectation value of the energy, let me consider the computation of the expectation values of the parameters $\\vec{\\theta}$ over the distribution given by the posterior distribution, $\\pi(\\vec{\\theta};\\vec{x})$ or\n",
    "$$\n",
    "\\left<\\vec{\\theta}\\right>_{\\pi} = \\int \\vec{\\theta}\\pi(\\vec{\\theta};\\vec{x}) d\\vec{\\theta} \n",
    "$$\n",
    "Now the analogue with the ising model is much clearer now.  If I compute this value, I get the expected value of the parameters.  Just as in the Ising model I get the expected value of the energy when integrating over a Maxwell-Boltzmann distribution.  Here I am integrate over the posterior distribution of the parameters.  \n",
    "\n",
    "Thus I can use MCMC for this.  A subtle, but important point is that when I take the average over the entire Markov chain, then the values of the parameters should reduce to the expected value.  However, if I take the distribution of the parameters, i.e., take a histogram of the values of the parameters in the chain, the distribution of the parameters is the posterior distribution.  This is a subtle, but important point and in case you ever wondered how these parameters distributions for various models where ever generated, now you know. \n",
    "\n",
    "Lets solidify our understanding with a ``simple'' example.  Let us consider our noisy sin wave as before, which we reproduce for clarity\n",
    "\n",
    "Now we will define a log-likelihood function\n",
    "$$\n",
    "\\log p(\\vec{y}; \\vec{\\theta}) = -\\sum_i\\left(\\frac{y_i - \\theta_0\\sin(t_i + \\theta_1)}{\\sigma}\\right)^2,\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(func, t, y, x, sigma=1) :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a89312",
   "metadata": {},
   "source": [
    "We need to set some priors. Lets set $\\theta_0$ to be a flat distribution between 1 and 10 and $\\theta_1$ to be a flat distribution between 0 and $2\\pi$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c80939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priors( x) : \n",
    "    if(x[0] < 1 or x[0] > 10) : \n",
    "        return 0\n",
    "    if(x[1] < 0 or x[1] > 2*np.pi) : \n",
    "        return 0\n",
    "    return 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d55666",
   "metadata": {},
   "source": [
    "We need to define a move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(func, t, y, x, h = 0.1, TINY=1e-30) :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc(func, t, y, x, burn_in=1000, MAX_CHAIN=500000) :\n",
    "    xchain = [] \n",
    "    i = 0\n",
    "\n",
    "    for j in range(MAX_CHAIN) : \n",
    "        x, accepted_move = move(func, t, y, x)\n",
    "    if( accepted_move) :\n",
    "        i += 1 \n",
    "        if( i > burn_in) : \n",
    "            xchain.append(x)\n",
    "    return np.array(xchain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
