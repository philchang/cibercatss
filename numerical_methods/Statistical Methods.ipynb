{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb81279",
   "metadata": {},
   "source": [
    "## maximum likelihood\n",
    "\n",
    "Lets first generate some data.  The data is a sine function with some noise -- just in the y-direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19768a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_data() : \n",
    "    x = np.arange(0,2*np.pi,0.01*np.pi)\n",
    "    y = np.sin(x) + (0.5*np.random.random(x.size)-0.25)\n",
    "    return x,y\n",
    "\n",
    "x,y = generate_data()\n",
    "plt.scatter(x,y,s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22a704",
   "metadata": {},
   "source": [
    "### Fitting a polynomial\n",
    "\n",
    "Lets try to fit a polynomial to it.  As an exercise in figuring out how to do it.  Lets look up numpy polyfit and see if you can figure out how to \n",
    "1. create a polynomial fit degree, ndeg -- currently set to 3\n",
    "2. plot it. \n",
    "3. increase the upper bound beyond $2\\pi$ and see what this produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c5b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ndeg = 3\n",
    "\n",
    "x_test = np.arange(0,2*np.pi,0.01*np.pi)\n",
    "# create a polynomial fit of degree 3 and plot it as x_test, y_test\n",
    "# Look up numpy.polyfit\n",
    "\n",
    "plt.scatter(x,y,s=1)\n",
    "plt.plot(x_test,y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636867d8",
   "metadata": {},
   "source": [
    "The thing is that we know the underlying function is a sine wave, so lets try to fit a sine wave to it and gets the estimated/best-fit parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as so\n",
    "\n",
    "def fit_func(x,parameters):\n",
    "    pass\n",
    "\n",
    "def compute_residuals(parameters, xdata, ydata,sigma=0.25) : \n",
    "    pass\n",
    "\n",
    "lstsq = so.least_squares(compute_residuals, x0=[4,1], args=[x,y])\n",
    "\n",
    "\n",
    "#compute the chi squared\n",
    "\n",
    "print( chi_sq)\n",
    "\n",
    "plt.plot(x,fit_func(x, lstsq.x))\n",
    "plt.scatter(x,y,s=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cbaac",
   "metadata": {},
   "source": [
    "## Bayesian statistics\n",
    "\n",
    "The essense of Bayesian statistical inference is encoded in Bayes' theorem\n",
    "$$\n",
    "\\pi(\\vec{\\theta};\\vec{x}) = \\frac{\\pi(\\vec{\\theta})p(\\vec{x};\\vec{\\theta})}{p(\\vec{x})}\n",
    "$$\n",
    "where $p(\\vec{x};\\vec{\\theta})$ is a likelihood function and $p(\\vec{x}) = \\int \\pi(\\vec{\\theta})p(\\vec{x};\\vec{\\theta})d\\vec{\\theta}$ is essentially the normalization.  Other way of intepreting this is the likelihood of a model with paramters $\\vec{\\theta}$ is the product of the distribution of the model parameters (what we think are good parameters) and the likelihood of the parameters given the data.  This is the exact same problem that we encountered earlier where we needed to compute the expectation value of the energy in the Ising model.  However, in some ways this is simplier as the special substitutions are avoided in this case.  In essence, we want to compute\n",
    "$$\n",
    "p(\\vec{x}) = \\int \\pi(\\vec{\\theta})p(\\vec{x};\\vec{\\theta})d\\vec{\\theta}\n",
    "$$\n",
    "The key is that we only want to compute over the values of $\\theta$ that really contribute.  So we can define a Metropolis algorithm that goes for this as\n",
    "\n",
    "1. Begin with some parameter set $\\vec{\\theta}_1$ \n",
    "2. Compute the joint likelihood $L_1 = \\pi(\\vec{\\theta}_1)p(\\vec{x};\\vec{\\theta}_1)$\n",
    "3. Pick a random proposal $\\vec{\\theta}_2$ from a distribution based on the priors.\n",
    "4. Compute the likelihood $L_2 = \\pi(\\vec{\\theta}_2)p(\\vec{x};\\vec{\\theta}_2)$\n",
    "5. Compute the transition probability $R = L_2/L_1$\n",
    "6. Accept the proposal with probability $R$\n",
    "    \n",
    "Now to make contact with the problem in the Ising model where we computed the expectation value of the energy, let me consider the computation of the expectation values of the parameters $\\vec{\\theta}$ over the distribution given by the posterior distribution, $\\pi(\\vec{\\theta};\\vec{x})$ or\n",
    "$$\n",
    "\\left<\\vec{\\theta}\\right>_{\\pi} = \\int \\vec{\\theta}\\pi(\\vec{\\theta};\\vec{x}) d\\vec{\\theta} \n",
    "$$\n",
    "Now the analogue with the ising model is much clearer now.  If I compute this value, I get the expected value of the parameters.  Just as in the Ising model I get the expected value of the energy when integrating over a Maxwell-Boltzmann distribution.  Here I am integrate over the posterior distribution of the parameters.  \n",
    "\n",
    "Thus I can use MCMC for this.  A subtle, but important point is that when I take the average over the entire Markov chain, then the values of the parameters should reduce to the expected value.  However, if I take the distribution of the parameters, i.e., take a histogram of the values of the parameters in the chain, the distribution of the parameters is the posterior distribution.  This is a subtle, but important point and in case you ever wondered how these parameters distributions for various models where ever generated, now you know. \n",
    "\n",
    "Lets solidify our understanding with a ``simple'' example.  Let us consider our noisy sin wave as before, which we reproduce for clarity\n",
    "\n",
    "Now we will define a log-likelihood function\n",
    "$$\n",
    "\\log p(\\vec{y}; \\vec{\\theta}) = -\\sum_i\\left(\\frac{y_i - \\theta_0\\sin(t_i + \\theta_1)}{\\sigma}\\right)^2,\n",
    "$$\n",
    "Note that this is the log-likelihood function and we need the likelihood function in MCMC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(func, x, y, theta, sigma=1) :\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a89312",
   "metadata": {},
   "source": [
    "We need to set some priors. Lets set $\\theta_0$ to be a flat distribution between 1 and 10 and $\\theta_1$ to be a flat distribution between 0 and $2\\pi$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c80939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def priors( theta) : \n",
    "    if(theta[0] < 1 or theta[0] > 10) : \n",
    "        return 0\n",
    "    if(theta[1] < 0 or theta[1] > 2*np.pi) : \n",
    "        return 0\n",
    "    return 1  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d55666",
   "metadata": {},
   "source": [
    "We need to define a move.  The move starts with with a initial position in parameter space $\\theta$ and you make a random move with a small step of h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(func, x, y, theta, h = 0.1, TINY=1e-30) :\n",
    "    #\n",
    "    # returns None, False if a move is rejected\n",
    "    # returns theta_new, True if the move is accepted\n",
    "    dtheta = (2*np.random.rand(theta.size)-1.)*h\n",
    "    theta_move = theta + dtheta\n",
    "    \n",
    "    # 1. compute the prior on theta_move and if it fails \n",
    "    # return None, False <-- so that the move is rejected \n",
    "    \n",
    "    # 2. if the prior is ok, then compute the log_likelihood \n",
    "    # and compute the posterior for this which is likelihood times prior\n",
    "    # for both theta and theta_new, call this posterior and posterior_new\n",
    "    #\n",
    "    \n",
    "    #\n",
    "    # 3. if posterior_new/posterior >= 1, accept the move.  If not,\n",
    "    # only accept if np.random.rand() < posterior_new/posterior.  \n",
    "    #\n",
    " \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d93f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc(func, x, y, theta, burn_in=1000, MAX_CHAIN=500000) :\n",
    "    theta_chain = [] \n",
    "    i = 0\n",
    "\n",
    "    for j in range(MAX_CHAIN) : \n",
    "        theta, accepted_move = move(func, x, y, theta)\n",
    "    if( accepted_move) :\n",
    "        i += 1 \n",
    "        if( i > burn_in) : \n",
    "            theta_chain.append(theta)\n",
    "    return np.array(theta_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cffad80",
   "metadata": {},
   "source": [
    "And finally lets run it.  Now, we can compute statistics using the distribution of parameters, i.e., theta_chain.  Go ahead and compute the 25th, 50th, and 75th percentile of the parameters using numpy.percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b3ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_guess = np.zeros(2)\n",
    "theta_guess[0] = 2\n",
    "guess[1] = 0\n",
    "x,y = generate_data()\n",
    "theta_chain = mcmc(test_function, x, y, theta_guess, MAX_CHAIN=10000)\n",
    "plt.hist(theta_chain[:,0],bins=100)\n",
    "plt.show()\n",
    "\n",
    "# now compute some parameters from the chain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
