{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ce1688f",
   "metadata": {},
   "source": [
    "# ODE Integration\n",
    "$\\newcommand{\\vec}[1]{{\\bf #1}}$\n",
    "$\\newcommand{\\ee}{\\end{eqnarray}}$\n",
    "\n",
    "\n",
    "## Euler's Method\n",
    "\n",
    "Consider the following first order ordinary differential equation.\n",
    "$$\n",
    "\\frac {d\\vec{y}}{dt} = \\vec{f}(\\vec{y}, t),\n",
    "$$\n",
    "where $\\vec{y}$ is a vector of variables, $t$ is the independent variable, and $\\vec{f}$ is some arbitrary vector function of $\\vec{y}$ and $t$.  We can use our definition of derivative to write: \n",
    "$$\n",
    "\\frac {d\\vec{y}}{dt} \\approx \\frac {\\vec{y}_{i+1} - \\vec{y}_i}{\\Delta t} = \\vec{f}(\\vec{y}_i, t_i),\\label{eq:explicit}\n",
    "$$\n",
    "where $\\Delta t = t_{i+1} - t_i$.  This is not the only choice that could have been made, it is also possible to write it as \n",
    "$$\n",
    "\\frac {\\vec{y}_{i+1} - \\vec{y}_i}{\\Delta t} = \\vec{f}(\\vec{y}_{i+1}, t_{i+1}). \\label{eq:implicit}\n",
    "$$\n",
    "The difference between these two is the choice of either $t_{i}$ or $t_{i+1}$ on the right hand side.  Equation (\\ref{eq:implicit}) gives rise to implicit methods which are harder to code up, but offers potentially greater stability and speed.  Instead, we will focus on equation (\\ref{eq:explicit}).  \n",
    "\n",
    "If we know the value at $\\vec{y}(t_i)$, we can solve for $\\vec{y}(t_{i+1})$ to be\n",
    "$$\n",
    "\\vec{y}_{i+1} = \\vec{y}_i + \\vec{f}(\\vec{y}_i, t_i)\\Delta t.\\label{eq:euler method}\n",
    "$$\n",
    "This method is known as Euler's method.  As an aside, it we take $\\vec{f}$ to be a scalar function of just $t$, then it just becomes an ordinary integral.  Hence, the methodology we describe here is directly relevant for numerical integration so we don't need a separate discussion for it.\n",
    "\n",
    "Equation (\\ref{eq:euler method}) gives the correct answer for $\\Delta t \\rightarrow 0$, but it should never be used solving any equations you code up on a computer as superior methods abound\\footnote{Of course in the movie Hidden Figures, the lead protagonist uses Euler's method to ensure that John Glenn lands safely. Then again the computer was a human being.}.  To see why Euler's method is not recommended, let consider the error that this algorithm introduces for finite $\\Delta t$.  We do a Taylor expansion of the true solution, $\\vec{y}(t)$:\n",
    "$$\n",
    "\\vec{y}_{i+1} \\approx \\vec{y}_i + \\frac{d\\vec{y}}{d t}(t_i)\\Delta t + \\frac 1 2 \\frac{d^2\\vec{y}}{d t^2}(t_i)\\Delta t^2 + \\mathcal{O}(\\Delta t^3)\n",
    "$$\n",
    "The first two terms constitute Euler's method, so for each time step, we will accumulate an error that is proportional to $\\Delta t^2$.  Since the number of steps over an interval $T$ goes like $T/\\Delta t$, then the total error scales like $1/\\Delta t$.  So for sufficiently small $\\Delta t$, then the error is reduced, but it comes at the cost of lots of steps.  Because of the scaling with error, Euler's method is known as a first-order method.  To get better results we want scaling that reduces the error as much as possible. \n",
    "\n",
    "As an example, lets consider the equation \n",
    "$$\n",
    "\\frac {dy}{dt} = \\cos(t).\n",
    "$$\n",
    "We can see that we can solve this analytically by noting\n",
    "$$\n",
    "y(t) = \\int\\cos(t) dt = \\sin(t) + C,\n",
    "$$\n",
    "where $C$ is a constant.  Let see how we can do this numerically. First we define a function for the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d90923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# implement the integrand above.\n",
    "def derivatives( t, y) : \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c1625",
   "metadata": {},
   "source": [
    "Now you might wonder about the second variable in the derivatives function. We shall return to this shortly. Now let us define the Euler method.  \n",
    "$$\n",
    "y(t+\\Delta t) = y(t) + \\frac{dy}{dt}(t,y(t))\\Delta t\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12cb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the euler's method as using dy/dt as defined by derivatives.  derivatives has the same signature as above.\n",
    "\n",
    "def euler(derivatives, t, delta_t, y) : \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48fcaa",
   "metadata": {},
   "source": [
    "Note that the euler function above follows the exact definition of euler's method as defined above.  Now lets drive this between $t=0$ and $t=100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_euler(N=1000, t0=0, t1=100) :\n",
    "    delta_t = (t1-t0)/N\n",
    "    y = 0\n",
    "\n",
    "    t_array = np.arange(t0,t1+dt,delta_t)\n",
    "    y_array = np.zeros(t_array.size)\n",
    "\n",
    "    y_array[0] = y\n",
    "    for i in range(t_array.size-1):\n",
    "        y_array[i+1] = euler(derivatives, t_array[i], delta_t, y_array[i])\n",
    "    return t_array, y_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5817681",
   "metadata": {},
   "source": [
    "Great.  Now lets plot this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479c38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_array, y_array = run_euler()\n",
    "plt.plot(t_array, y_array)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b8d9d",
   "metadata": {},
   "source": [
    "## Runge-Kutta 2 step (RK2)\n",
    "\n",
    "Euler's method is beautifully simple, elegant, and not that accurate.  There are much better methods abound, mainly ones that are much more accurate.  There is a slightly more complicated method that can give much better answers.\n",
    "\n",
    "Runge-Kutta methods are an example of predictor-corrector methods.  That is, it \"predicts\" the value at $\\vec{y}_{i+1}$ from the current solution at $\\vec{y}_i$.  Using this predicted value, it performs a \"corrector\" step to increase the accuracy of the solution.  The generic two-step Runge-Kutta method is as follows:\n",
    "$$\n",
    "\\vec{k}_1 = \\Delta t \\vec{f}(\\vec{y}_i, t_i) \n",
    "$$\n",
    "$$\n",
    "\\vec{k}_2 = \\Delta t \\vec{f}(\\vec{y}_i + \\beta \\vec{k}_1, t_i + \\alpha \\Delta t) \n",
    "$$\n",
    "$$\n",
    "\\vec{y}_{i+1} = \\vec{y}_i + a \\vec{k}_1 + b \\vec{k}_2\n",
    "$$\n",
    "where $\\vec{k}_1$ is the \"predictor\" and is the same as an Euler step, $\\vec{k}_2$ is the \"corrector\", and the $i+1$ step is some linear combination of the two.  The constants, $\\alpha$, $\\beta$, $a$, and $b$ are chosen to make the entire algoritm accurate to $\\mathcal{O}(\\Delta t^3)$.  To determine these unknown constants, let perform a Taylor expansion of $\\vec{y}_{i+1}$\n",
    "$$\n",
    "\\vec{y}_{i+1} = \\vec{y}_i + \\frac{d\\vec{y}}{d t}(t_i)\\Delta t + \\frac 1 2 \\frac{d^2\\vec{y}}{dt^2}(t_i)\\Delta t^2\n",
    "$$\n",
    "Now \n",
    "$$\n",
    "\\frac{d^2\\vec{y}}{dt^2}(t_i) = \\frac {d\\vec{f}(\\vec{y},t)}{dt} = \\frac {\\partial \\vec{f}(\\vec{y},t)}{\\partial t} + \\frac{d\\vec{y}}{dt}\\cdot\\vec{\\nabla}_{\\vec{y}}\\vec{f} \n",
    "$$\n",
    "$$\n",
    "= \\frac {\\partial \\vec{f}(\\vec{y},t)}{\\partial t} + \\vec{f}\\cdot\\vec{\\nabla}_{\\vec{y}}\\vec{f}\n",
    "$$  \n",
    "Thus we have \n",
    "$$\n",
    "\\vec{y}_{i+1} = \\vec{y}_i + \\vec{f}(\\vec{y}_i, t_i)\\Delta t + \\frac 1 2 \\left(\\frac {\\partial \\vec{f}(\\vec{y}_i,t_i)}{\\partial t} + \\vec{f}(\\vec{y}_i,t_i)\\cdot\\vec{\\nabla}_{\\vec{y}}\\vec{f}(\\vec{y}_i,t_i)\\right)\\Delta t^2 +\\mathcal{O}(\\Delta t^3)\\label{eq:2nd order}\n",
    "$$\n",
    "Now we Taylor expand out $\\vec{k}_2$ to find\n",
    "$$\n",
    "\\vec{k}_2 = \\Delta t \\vec{f}(\\vec{y}_i + \\beta \\vec{k}_1, t_i + \\alpha \\Delta t) \n",
    "$$\n",
    "$$\n",
    "= \\Delta t\\left( \\vec{f}(\\vec{y}_i, t_i) + \\alpha\\Delta t\\frac {\\partial \\vec{f}(\\vec{y}_i,t_i)}{\\partial t} + \\beta\\Delta t \\vec{f}\\cdot\\vec{\\nabla}_{\\vec{y}}\\vec{f}(\\vec{y}_i,t_i)\\right)\n",
    "$$\n",
    "Putting this all together, we have \n",
    "$$\n",
    "\\vec{y}_{i+1} = \\vec{y}_i + (a+b)\\Delta t \\vec{f}(\\vec{y}_i, t_i) + b\\Delta t^2\\left(\\alpha\\frac {\\partial \\vec{f}(\\vec{y}_i,t_i)}{\\partial t} + \\beta\\vec{f}\\cdot\\vec{\\nabla}_{\\vec{y}}\\vec{f}(\\vec{y}_i,t_i) \\right)+\\mathcal{O}(\\Delta t^3)\\label{eq:rk2 expansion}\n",
    "$$\n",
    "Comparing Equations (\\ref{eq:2nd order}) and (\\ref{eq:rk2 expansion}), we get the following conditions:\n",
    "$$\n",
    "a+b = 1 \\qquad b\\alpha = \\frac 1 2 \\qquad b\\beta = \\frac 1 2,\n",
    "$$\n",
    "or 3 equation for 4 unknowns.  So that mean there exist a infinite number of second order schemes that are possible, e.g., error per step that goes likes $\\Delta t^3$, so the total error over an interval goes like $\\Delta t^2$.  So using $\\alpha$ as a parameter, we have \n",
    "$$\n",
    "\\beta = \\alpha \\qquad b = \\frac 1 {2\\alpha} \\qquad a = 1 - \\frac 1 {2\\alpha}\n",
    "$$\n",
    "So a generic second order Runge-Kutta scheme is then\n",
    "$$\n",
    "\\vec{k}_1 = \\Delta t \\vec{f}(\\vec{y}_i, t_i)\n",
    "$$\n",
    "$$\n",
    "\\vec{k}_2 = \\Delta t \\vec{f}(\\vec{y}_i + \\alpha \\vec{k}_1, t_i + \\alpha \\Delta t) \n",
    "$$\n",
    "$$\n",
    "\\vec{y}_{i+1} = \\vec{y}_i + \\left(1 - \\frac 1 {2\\alpha}\\right) \\vec{k}_1 + \\frac {\\vec{k}_2} {2\\alpha} \n",
    "$$\n",
    "\n",
    "A few famous examples are \n",
    "1. Midpoint method: $\\alpha = 1/2$ Estimate the values of y at the midpoint and solve for the derivative at the midpoint.  Use this midpoint derivative to complete the integration.  Note that prefactor in front of $\\vec{k}_1$ in this case is zero.\n",
    "2. Heun's Method: $\\alpha = 1$ Estimate the values of y at the endpoint and give equal weight to both starting and endpoints to compute the derivative.\n",
    "\n",
    "Lets implement the midpoint method.  We will keep it more general\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab82e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement rk2.  derivatives have the signature derivatives(t,y) as above\n",
    "def rk2(derivatives, t, y, dt) : \n",
    "    y_new = y\n",
    "    pass\n",
    "    return t+dt, y_new\n",
    "\n",
    "def run_rk2(N=1000, t0=0, t1=100):\n",
    "    dt = (t1-t0)/N\n",
    "    y = 0\n",
    "    t_array = np.arange(t0,t1+dt,dt)\n",
    "    y_array = np.zeros(t_array.size)\n",
    "\n",
    "    y_array[0] = y\n",
    "    for i in range(t_array.size-1):\n",
    "        y_array[i+1] = rk2(derivatives, t_array[i], dt, y_array[i])\n",
    "    return t_array, y_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t_array, y_array = run_rk2()\n",
    "plt.plot(t_array, y_array)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef40e7d",
   "metadata": {},
   "source": [
    "## Using Scipy integrate\n",
    "\n",
    "Now it seems quaint to roll your own routines, but they are relatively simple to implement.  However for the most part, you want to use scipy.integrate.ode for this stuff.\n",
    "\n",
    "Let use the same example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b0983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as si\n",
    "\n",
    "# set up RK5\n",
    "integrator = si.ode(derivatives).set_integrator(\"dopri5\")\n",
    "N = 1000\n",
    "#set up initial conditions\n",
    "y0 = np.zeros(1)\n",
    "t0 = 0\n",
    "t1 = 100\n",
    "dt = (t1-t0)/N\n",
    "integrator.set_initial_value(y0, t0)\n",
    "\n",
    "\n",
    "y_array = []\n",
    "t_array = []\n",
    "while integrator.successful() and integrator.t < t1 : \n",
    "    integrator.integrate(integrator.t+dt)\n",
    "    t_array.append(integrator.t)\n",
    "    y_array.append(integrator.y)\n",
    "    \n",
    "plt.plot(t_array, y_array)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6891ecd",
   "metadata": {},
   "source": [
    "## Timestepping\n",
    "\n",
    "One thing that we have not discussed is the choice for $\\Delta t$.  For an interval between $t_0$ and $t_1$, a larger $\\Delta t$ results in fewer computational steps, which makes things faster.  However, a smaller $\\Delta t$ results in greater accuracy.  There is a limit with higher order methods on how accurate you can make a solution.  \n",
    "\n",
    "But there is another subtle issue that can happen.  In many instances the right hand side of an ODE can take on large (positive or negative) values for a limited set of circumstances.  In these cases, it is useful to have a variable $\\Delta t$ -- small when things change quickly and large when things change slowly.  How can we estimate when these occurs.  \n",
    "\n",
    "Suppose you have an ODE of the form\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial t} = f(y,t)\n",
    "$$\n",
    "Then according to Euler's method we have\n",
    "$$\n",
    "y_{n+1} - y_n = f(y_n,t_n)\\Delta t\n",
    "$$\n",
    "Now suppose we want the change in $\\Delta y = |y_{n+1} - y_n| < \\alpha |y_n|$.  This means that\n",
    "$$\n",
    "\\alpha |y_n| = |f(y_n,t_n)|\\Delta t_{\\rm max}\n",
    "$$\n",
    "This allows us to solve for $\\Delta t_{\\rm max}$ to be\n",
    "$$\n",
    "\\Delta t_{\\rm max} = \\alpha \\left|\\frac{y_n}{f(y_n,t_n)}\\right|\n",
    "$$\n",
    "Typically, you don't want $\\alpha$ to be too large nor too small.  I have found values between 0.01 and 0.1 to work well.  \n",
    "\n",
    "At the same time, we don't want to miss something if $\\Delta t_{\\rm max}$ is too large that it totally misses a change.  In this case, I like to pick a $\\Delta t_{\\rm max,0} = (t_1 - t_0)/N_0$, where $N_0$ is a number between 10 or 100, but this can change as well.\n",
    "\n",
    "So a selection for $\\Delta t$ at a time $t$ would be\n",
    "$$\n",
    "\\Delta t = \\min( t_1 - t, \\min(\\Delta t_{\\rm max,0}, \\Delta t_{\\rm max}))\n",
    "$$\n",
    "\n",
    "Now lets try an example of this. Consider the ODE\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial t} = \\frac 1 {\\sigma} \\exp\\left(-\\frac{t^2}{\\sigma^2}\\right)\n",
    "$$\n",
    "You probably recognize the answer to this one.  Integrated from $t=-\\infty$ to $\\infty$, it is $\\sqrt{\\pi}$ or $\\sqrt{2\\pi}$. I forget\n",
    "\n",
    "Now lets try to do this numerically.  First we define the derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_gaussian( t, y, sigma=0.1) :\n",
    "    if( np.abs(t/sigma) > 10 ) : # this is to prevent overflows that can happen for large exp(-t^2/sigma^2)\n",
    "         return 0.\n",
    "    return np.exp(-t**2/sigma**2)\n",
    "\n",
    "def timestep(derivatives, t1, t, y, dt0, al) : # compute the timestep -- implement this\n",
    "    pass\n",
    "\n",
    "def run_gaussian(t0=-10, t1=10, N0 = 100) : \n",
    "    y = 0\n",
    "    t = t0\n",
    "    dt0 = (t1 - t0)/N\n",
    "    while t < t1 :\n",
    "        dt = timestep(derivatives_gaussian, t1, t, y, dt0)\n",
    "        y = rk2(derivatives, t_array[i], dt, y)\n",
    "        t += dt\n",
    "        \n",
    "    print( f\"End result is {y}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3889a",
   "metadata": {},
   "source": [
    "## Higher Order ODEs\n",
    "\n",
    "Thus far we have discussed the case of first order odes.  What about higher order ODEs.  It turns out that there is a very simple extension to arbitrary high order ODEs.  The trick is it identify higher order derivatives as variables in themselves.  Consider the ODE\n",
    "$$\n",
    "\\sum_n^N\\frac{d^n f}{dx^n} = 0\n",
    "$$\n",
    "We can write this as a sum first order ODEs by the identification of \n",
    "$$\n",
    "f_i = \\frac{df_{i-1}}{dx} \\qquad\\textrm{and}\\qquad f_0 = f\n",
    "$$\n",
    "Thus we have \n",
    "$$\n",
    "\\frac{df_{N-1}}{dx} + \\sum_i^{N-1} f_i = 0,\n",
    "$$\n",
    "$$\n",
    "\\frac{df}{dx} = f_1\n",
    "$$\n",
    "$$\n",
    "\\frac{df_1}{dx} = f_2\n",
    "$$\n",
    "$$\n",
    " . . . .\n",
    "$$\n",
    "$$\n",
    "\\frac{df_{N-2}}{dx} = f_{N-1}.\n",
    "$$\n",
    "So this converts a Nth order ODE to N first order ODEs, which we can solve.\n",
    "\n",
    "As an example lets try a simple harmonic oscillator \n",
    "$$\n",
    "\\frac{d^2 x}{dt^2} = -x\n",
    "$$\n",
    "We of course know that the solution is \n",
    "$$\n",
    "x(t) = A\\cos(t+\\phi)\n",
    "$$\n",
    "\n",
    "This is second order so we break this up into 2 first order equations\n",
    "$$\n",
    "\\frac{dx}{dt} = v\n",
    "$$\n",
    "$$\n",
    "\\frac{dv}{dt} = -x\n",
    "$$\n",
    "Now lets implement this numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement the SHO derivative as 2 first order equations\n",
    "def SHO_derivatives(t,y) :\n",
    "    x = y[0]\n",
    "    v = y[1]\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da2f27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_SHO_rk2(t,delta_t) :\n",
    "    y = np.zeros(2)\n",
    "    y[0] = 1 # set initial condition\n",
    "    t1 = 0\n",
    "    while t1 < t : \n",
    "        delta_t = min(t-t1,delta_t)\n",
    "        # implement the rk2 step function\n",
    "        pass\n",
    "    return y\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def animate(i):\n",
    "    ax.clear()\n",
    "    # Get the point from the points list at index i\n",
    "    delta_t = 0.05\n",
    "    y = run_SHO_rk2(i*delta_t, delta_t)\n",
    "    ax.plot(y[0], 0., color='green', marker='o')\n",
    "    # Set the x and y axis to display a fixed range\n",
    "    ax.set_xlim([-1.2, 1.2])\n",
    "    ax.set_ylim([-1,1])\n",
    "ani = FuncAnimation(fig, animate, frames=250, interval=20, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f862bb",
   "metadata": {},
   "source": [
    "Now challenge problem.  Let's consider a very classic problem from astronomy.  Two massive bodies interacting gravitationally.  The force is a 3-d force which is defined by\n",
    "$$\n",
    "F = -\\frac{M_1 M_2}{r^2}\n",
    "$$\n",
    "we will scale out newton's constant, $G$, here. Suppose you have two bodies.  Go ahead and implement the derivatives and run it.  It should look like two bodies orbiting each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265adc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_bodies = 2\n",
    "M = np.array([1.0, 0.25])\n",
    "\n",
    "# given N bodies, implement the force equations as stated above.\n",
    "def Nbody_derivatives(t,y) :\n",
    "    pass\n",
    "    \n",
    "# some simple initial conditions\n",
    "def initial_conditions() : \n",
    "    pos_and_vel = np.zeros([N_bodies,6])\n",
    "\n",
    "    pos_and_vel[0,0] = -1\n",
    "    pos_and_vel[0,4] = -0.25\n",
    "    pos_and_vel[1,0] = 1\n",
    "    pos_and_vel[1,4] = 0.25\n",
    "    return pos_and_vel\n",
    "\n",
    "def run_Nbody_rk2(tend,tframe,dt) :\n",
    "    pos_and_vel = initial_conditions()\n",
    "    y = pos_and_vel\n",
    "    t = 0\n",
    "    tnext = tframe\n",
    "    positions = []\n",
    "    while t<tend :\n",
    "        while t < tnext :\n",
    "            delta_t = min(tnext-t,dt)\n",
    "            # compute using rk2\n",
    "        positions.append(pos_and_vel[:,0:3].copy())\n",
    "        tnext += tframe\n",
    "    return positions\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "frames =100\n",
    "tframe = 0.25\n",
    "dt = 0.025\n",
    "\n",
    "positions = run_Nbody_rk2(frames*tframe, tframe, dt)\n",
    "\n",
    "def animate(i, positions):\n",
    "    ax.clear()\n",
    "    # Get the point from the points list at index i\n",
    "    pos = positions[i]\n",
    "    ax.scatter(pos[:,0], pos[:,1], color='green', marker='o')\n",
    "    # Set the x and y axis to display a fixed range\n",
    "    ax.set_xlim([-5, 5])\n",
    "    ax.set_ylim([-5,5])\n",
    "ani = FuncAnimation(fig, lambda i : animate(i, positions), frames=len(positions), interval=50, repeat=False)\n",
    "video = ani.to_html5_video()\n",
    "html = display.HTML(video)\n",
    "display.display(html)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c5ee0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
